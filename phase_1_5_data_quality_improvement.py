#!/usr/bin/env python3
"""
ç¬¬1.5é˜¶æ®µï¼šæ•°æ®è´¨é‡æå‡å’Œè¯­ä¹‰åˆ†ç±»æ”¹è¿›
ç›®æ ‡ï¼šä¸ºç¬¬äºŒé˜¶æ®µçš„æ”»å‡»å®éªŒåšå¥½æ•°æ®å‡†å¤‡
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
import re
from typing import Dict, List, Tuple, Any

class DataQualityImprover:
    """æ•°æ®è´¨é‡æå‡å™¨"""
    
    def __init__(self, results_dir: str = "results/full_hotpotqa_analysis"):
        self.results_dir = Path(results_dir)
        self.final_results_dir = self.results_dir / "final_merged_results"
        self.output_dir = Path("results/quality_improved_analysis")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸ”§ æ•°æ®è´¨é‡æå‡å™¨åˆå§‹åŒ–")
        
    def step1_clean_invalid_answers(self):
        """æ­¥éª¤1ï¼šæ¸…ç†æ— æ•ˆç­”æ¡ˆ"""
        print("\nğŸ§¹ æ­¥éª¤1ï¼šæ¸…ç†æ— æ•ˆç­”æ¡ˆ...")
        
        # åŠ è½½æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µ
        with open(self.final_results_dir / "all_knowledge_pieces.json", 'r') as f:
            all_pieces = json.load(f)
        
        print(f"åŸå§‹çŸ¥è¯†ç‰‡æ®µæ•°é‡: {len(all_pieces)}")
        
        # å®šä¹‰æ— æ•ˆç­”æ¡ˆæ¨¡å¼
        invalid_patterns = [
            r'^___$',  # çº¯å ä½ç¬¦
            r'what is the answer\?',  # é—®é¢˜æ ¼å¼é”™è¯¯
            r'^Based on the fact.*what is the answer\?.*$',  # æ ¼å¼é”™è¯¯
        ]
        
        valid_pieces = []
        invalid_count = 0
        
        for piece in all_pieces:
            answer = piece.get('answer', '').strip()
            question = piece.get('question', '').strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆç­”æ¡ˆ
            is_invalid = False
            for pattern in invalid_patterns:
                if re.search(pattern, answer, re.IGNORECASE) or re.search(pattern, question, re.IGNORECASE):
                    is_invalid = True
                    break
            
            # æ£€æŸ¥å¡«ç©ºé¢˜ä½†ç­”æ¡ˆä¸åŒ¹é…çš„æƒ…å†µ
            if 'Fill in the blank:' in question and '___' in question:
                # è¿™ç§æƒ…å†µä¸‹ç­”æ¡ˆåº”è¯¥æ˜¯å¡«ç©ºçš„å†…å®¹
                if not self._validate_fill_blank_answer(question, answer):
                    is_invalid = True
            
            if not is_invalid:
                valid_pieces.append(piece)
            else:
                invalid_count += 1
        
        print(f"æ¸…ç†åæœ‰æ•ˆçŸ¥è¯†ç‰‡æ®µæ•°é‡: {len(valid_pieces)}")
        print(f"æ¸…ç†æ‰çš„æ— æ•ˆç‰‡æ®µæ•°é‡: {invalid_count}")
        
        # ä¿å­˜æ¸…ç†åçš„æ•°æ®
        with open(self.output_dir / "cleaned_knowledge_pieces.json", 'w') as f:
            json.dump(valid_pieces, f, indent=2, ensure_ascii=False)
        
        return valid_pieces
    
    def _validate_fill_blank_answer(self, question: str, answer: str) -> bool:
        """éªŒè¯å¡«ç©ºé¢˜ç­”æ¡ˆçš„åˆç†æ€§"""
        # ç®€å•éªŒè¯ï¼šç­”æ¡ˆä¸åº”è¯¥æ˜¯å ä½ç¬¦
        if answer.strip() in ['___', 'yes', 'no'] and 'Fill in the blank:' in question:
            return False
        return True
    
    def step2_semantic_question_classification(self, valid_pieces: List[Dict]):
        """æ­¥éª¤2ï¼šåŸºäºè¯­ä¹‰çš„é—®é¢˜åˆ†ç±»"""
        print("\nğŸ§  æ­¥éª¤2ï¼šåŸºäºè¯­ä¹‰çš„é—®é¢˜åˆ†ç±»...")
        
        # å®šä¹‰è¯­ä¹‰åˆ†ç±»è§„åˆ™
        semantic_categories = {
            'Temporal_Questions': {
                'patterns': [r'when', r'what year', r'which year', r'\d{4}', r'founded', r'established', r'born'],
                'description': 'æ—¶é—´ç›¸å…³é—®é¢˜'
            },
            'Entity_Relation_Questions': {
                'patterns': [r'who is', r'what is', r'which.*is', r'name.*person', r'founder', r'director'],
                'description': 'å®ä½“å…³ç³»é—®é¢˜'
            },
            'Boolean_Logic_Questions': {
                'patterns': [r'is.*same', r'are.*both', r'do.*both', r'true or false', r'whether'],
                'description': 'å¸ƒå°”é€»è¾‘é—®é¢˜'
            },
            'Location_Questions': {
                'patterns': [r'where', r'located', r'country', r'city', r'state'],
                'description': 'åœ°ç†ä½ç½®é—®é¢˜'
            },
            'Quantity_Questions': {
                'patterns': [r'how many', r'how much', r'number of', r'count'],
                'description': 'æ•°é‡é—®é¢˜'
            }
        }
        
        categorized_pieces = defaultdict(list)
        
        for piece in valid_pieces:
            question = piece.get('question', '').lower()
            answer = piece.get('answer', '').lower()
            
            # åˆ†ç±»é€»è¾‘
            category = self._classify_question_semantically(question, answer, semantic_categories)
            piece['semantic_category'] = category
            categorized_pieces[category].append(piece)
        
        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        print("\nğŸ“Š è¯­ä¹‰åˆ†ç±»ç»“æœ:")
        for category, pieces in categorized_pieces.items():
            print(f"   {category}: {len(pieces)} ä¸ªç‰‡æ®µ")
        
        # ä¿å­˜åˆ†ç±»ç»“æœ
        with open(self.output_dir / "semantically_categorized_pieces.json", 'w') as f:
            json.dump(dict(categorized_pieces), f, indent=2, ensure_ascii=False)
        
        return categorized_pieces
    
    def _classify_question_semantically(self, question: str, answer: str, categories: Dict) -> str:
        """åŸºäºè¯­ä¹‰å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        # æ£€æŸ¥ç­”æ¡ˆç±»å‹
        if answer in ['yes', 'no'] and any(pattern in question for pattern in ['is', 'are', 'do', 'does']):
            return 'Boolean_Logic_Questions'
        
        if re.match(r'^\d{4}$', answer.strip()):
            return 'Temporal_Questions'
        
        # åŸºäºé—®é¢˜æ¨¡å¼åˆ†ç±»
        for category, info in categories.items():
            for pattern in info['patterns']:
                if re.search(pattern, question, re.IGNORECASE):
                    return category
        
        return 'Other_Questions'
    
    def step3_rebuild_coupling_analysis(self, categorized_pieces: Dict):
        """æ­¥éª¤3ï¼šåŸºäºè¯­ä¹‰åˆ†ç±»é‡å»ºè€¦åˆåˆ†æ"""
        print("\nğŸ”— æ­¥éª¤3ï¼šåŸºäºè¯­ä¹‰åˆ†ç±»é‡å»ºè€¦åˆåˆ†æ...")
        
        # åŠ è½½é«˜è€¦åˆå¯¹
        batch_0_file = self.results_dir / "batch_0000" / "high_coupling_pairs.json"
        with open(batch_0_file, 'r') as f:
            batch_data = json.load(f)
            pairs = batch_data['pairs']
        
        # åˆ›å»ºpiece_idåˆ°è¯­ä¹‰ç±»åˆ«çš„æ˜ å°„
        piece_id_to_category = {}
        all_pieces_flat = []
        for category, pieces in categorized_pieces.items():
            for piece in pieces:
                piece_id_to_category[piece['piece_id']] = category
                all_pieces_flat.append(piece)
        
        # é‡æ–°åˆ†æè€¦åˆå¯¹
        semantic_coupling_analysis = {
            'same_semantic_category': {'pairs': [], 'strengths': []},
            'different_semantic_category': {'pairs': [], 'strengths': []},
            'same_semantic_and_answer': {'pairs': [], 'strengths': []},
            'same_semantic_different_answer': {'pairs': [], 'strengths': []}
        }
        
        for pair in pairs:
            piece_1_id = pair['piece_1_id']
            piece_2_id = pair['piece_2_id']
            
            category_1 = piece_id_to_category.get(piece_1_id)
            category_2 = piece_id_to_category.get(piece_2_id)
            
            if category_1 and category_2:
                if category_1 == category_2:
                    semantic_coupling_analysis['same_semantic_category']['pairs'].append(pair)
                    semantic_coupling_analysis['same_semantic_category']['strengths'].append(pair['coupling_strength'])
                    
                    # è¿›ä¸€æ­¥æ£€æŸ¥ç›¸åŒè¯­ä¹‰ç±»åˆ«ä¸­ç­”æ¡ˆæ˜¯å¦ç›¸åŒ
                    if pair['piece_1_answer'] == pair['piece_2_answer']:
                        semantic_coupling_analysis['same_semantic_and_answer']['pairs'].append(pair)
                        semantic_coupling_analysis['same_semantic_and_answer']['strengths'].append(pair['coupling_strength'])
                    else:
                        semantic_coupling_analysis['same_semantic_different_answer']['pairs'].append(pair)
                        semantic_coupling_analysis['same_semantic_different_answer']['strengths'].append(pair['coupling_strength'])
                else:
                    semantic_coupling_analysis['different_semantic_category']['pairs'].append(pair)
                    semantic_coupling_analysis['different_semantic_category']['strengths'].append(pair['coupling_strength'])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š åŸºäºè¯­ä¹‰çš„è€¦åˆåˆ†æç»“æœ:")
        for analysis_type, data in semantic_coupling_analysis.items():
            if data['strengths']:
                avg_strength = np.mean(data['strengths'])
                count = len(data['pairs'])
                print(f"   {analysis_type}: {count} å¯¹, å¹³å‡å¼ºåº¦: {avg_strength:.4f}")
        
        # ä¿å­˜åˆ†æç»“æœ
        results_summary = {
            'semantic_coupling_statistics': {
                analysis_type: {
                    'count': len(data['pairs']),
                    'average_strength': float(np.mean(data['strengths'])) if data['strengths'] else 0,
                    'std_strength': float(np.std(data['strengths'])) if data['strengths'] else 0
                }
                for analysis_type, data in semantic_coupling_analysis.items()
            }
        }
        
        with open(self.output_dir / "semantic_coupling_analysis.json", 'w') as f:
            json.dump(results_summary, f, indent=2, ensure_ascii=False)
        
        return semantic_coupling_analysis
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®è´¨é‡æå‡æµç¨‹"""
        print("ğŸš€ å¼€å§‹ç¬¬1.5é˜¶æ®µï¼šæ•°æ®è´¨é‡æå‡å’Œè¯­ä¹‰åˆ†ç±»æ”¹è¿›")
        print("=" * 60)
        
        # æ­¥éª¤1ï¼šæ¸…ç†æ— æ•ˆç­”æ¡ˆ
        valid_pieces = self.step1_clean_invalid_answers()
        
        # æ­¥éª¤2ï¼šè¯­ä¹‰åˆ†ç±»
        categorized_pieces = self.step2_semantic_question_classification(valid_pieces)
        
        # æ­¥éª¤3ï¼šé‡å»ºè€¦åˆåˆ†æ
        semantic_coupling_analysis = self.step3_rebuild_coupling_analysis(categorized_pieces)
        
        print(f"\nâœ… ç¬¬1.5é˜¶æ®µå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        
        return {
            'valid_pieces': valid_pieces,
            'categorized_pieces': categorized_pieces,
            'semantic_coupling_analysis': semantic_coupling_analysis
        }

def main():
    """ä¸»å‡½æ•°"""
    improver = DataQualityImprover()
    results = improver.run_full_pipeline()
    
    print("\nğŸ‰ ç¬¬1.5é˜¶æ®µå®Œæˆï¼Œå‡†å¤‡è¿›å…¥ç¬¬äºŒé˜¶æ®µæ”»å‡»å®éªŒï¼")

if __name__ == "__main__":
    main() 