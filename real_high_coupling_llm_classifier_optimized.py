#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆçœŸå®é«˜è€¦åˆå¯¹LLMè¯­ä¹‰åˆ†ç±»å™¨
- æ·»åŠ è¯¦ç»†è¿›åº¦æ¡
- æ”¯æŒåå°è¿è¡Œ
- ä¼˜åŒ–åˆ†ç±»ç±»åˆ«
- å®æ—¶ä¿å­˜ç»“æœ
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import time
import sys
import signal
import os
from typing import Dict, List, Tuple, Any
import logging
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('classification_progress.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizedHighCouplingLLMClassifier:
    """ä¼˜åŒ–ç‰ˆçœŸå®é«˜è€¦åˆå¯¹LLMåˆ†ç±»å™¨"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-14B-Instruct"):
        self.model_name = model_name
        self.results_dir = Path("results/full_hotpotqa_analysis")
        self.output_dir = Path("results/optimized_high_coupling_classification")
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ¤– ä¼˜åŒ–ç‰ˆé«˜è€¦åˆå¯¹LLMåˆ†ç±»å™¨åˆå§‹åŒ–")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´ç”Ÿæˆå‚æ•°
        if "14B" in model_name or "32B" in model_name:
            self.max_new_tokens = 35
            self.temperature = 0.05  # æ›´ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§
            print(f"   ğŸ¯ å¤§æ¨¡å‹é…ç½®: max_tokens={self.max_new_tokens}, temp={self.temperature}")
        else:
            self.max_new_tokens = 30
            self.temperature = 0.1
            print(f"   ğŸ¯ æ ‡å‡†é…ç½®: max_tokens={self.max_new_tokens}, temp={self.temperature}")
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ç”¨äºä¼˜é›…åœæ­¢
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.stop_requested = False
        
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…åœæ­¢"""
        print(f"\nâš ï¸ æ¥æ”¶åˆ°åœæ­¢ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è¿›åº¦...")
        self.stop_requested = True
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹ - æ”¯æŒ7B/14B/32B"""
        print(f"\nğŸ”„ åŠ è½½æ¨¡å‹: {self.model_name}")
        
        try:
            # æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´åŠ è½½ç­–ç•¥
            if "14B" in self.model_name:
                print(f"   ğŸ¯ 14Bæ¨¡å‹ä¼˜åŒ–é…ç½®:")
                print(f"      - ä½¿ç”¨FP16ç²¾åº¦")
                print(f"      - å¯ç”¨gradient checkpointing") 
                print(f"      - ä¼˜åŒ–æ˜¾å­˜ç®¡ç†")
                
                torch_dtype = torch.float16
                device_map = "auto"
                additional_kwargs = {
                    "use_cache": True,
                    "torch_dtype": torch_dtype
                }
                
            elif "32B" in self.model_name:
                print(f"   ğŸ¯ 32Bæ¨¡å‹ä¼˜åŒ–é…ç½®:")
                print(f"      - ä½¿ç”¨FP16ç²¾åº¦")
                print(f"      - å¤šGPUåˆ†å¸ƒç­–ç•¥")
                
                torch_dtype = torch.float16
                device_map = "auto"
                additional_kwargs = {
                    "use_cache": True,
                    "torch_dtype": torch_dtype,
                    "low_cpu_mem_usage": True
                }
                
            else:  # 7Bæ¨¡å‹
                print(f"   ğŸ¯ 7Bæ¨¡å‹æ ‡å‡†é…ç½®")
                torch_dtype = torch.float16
                device_map = "auto"
                additional_kwargs = {
                    "torch_dtype": torch_dtype
                }
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device_map,
                trust_remote_code=True,
                **additional_kwargs
            )
            
            # æ˜¾ç¤ºåŠ è½½ç»“æœ
            if torch.cuda.is_available():
                total_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                print(f"   ğŸ“Š æ˜¾å­˜ä½¿ç”¨: {total_memory:.1f}GB")
                
                # 14Bæ¨¡å‹ç‰¹æ®Šæç¤º
                if "14B" in self.model_name:
                    print(f"   ğŸ¯ 14Bæ¨¡å‹å·²å¯ç”¨ï¼Œé¢„æœŸæ›´é«˜çš„è¯­ä¹‰åˆ†ç±»å‡†ç¡®ç‡")
                    if total_memory > 35:
                        print(f"   âš ï¸ æ˜¾å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ç›‘æ§GPUçŠ¶æ€")
            else:
                print(f"   âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (CPUæ¨¡å¼)")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"   ğŸ’¡ å»ºè®®:")
            if "14B" in self.model_name:
                print(f"      - ç¡®ä¿GPUæ˜¾å­˜è‡³å°‘30GB")
                print(f"      - è€ƒè™‘ä½¿ç”¨ --model Qwen/Qwen2.5-7B-Instruct")
            raise
    
    def load_all_high_coupling_pairs(self):
        """åŠ è½½æ‰€æœ‰é«˜è€¦åˆå¯¹æ•°æ®"""
        print(f"\nğŸ“Š åŠ è½½æ‰€æœ‰é«˜è€¦åˆå¯¹æ•°æ®...")
        
        all_pairs = []
        batch_dirs = [d for d in self.results_dir.iterdir() if d.is_dir() and d.name.startswith('batch_')]
        
        # æ·»åŠ è¿›åº¦æ¡
        with tqdm(batch_dirs, desc="åŠ è½½æ‰¹æ¬¡", unit="batch") as pbar:
            for batch_dir in pbar:
                coupling_file = batch_dir / "high_coupling_pairs.json"
                if coupling_file.exists():
                    with open(coupling_file, 'r') as f:
                        batch_data = json.load(f)
                        all_pairs.extend(batch_data['pairs'])
                    pbar.set_postfix({'å½“å‰æ‰¹æ¬¡': batch_dir.name, 'æ€»å¯¹æ•°': len(all_pairs)})
        
        print(f"   ä» {len(batch_dirs)} ä¸ªæ‰¹æ¬¡åŠ è½½äº† {len(all_pairs):,} ä¸ªé«˜è€¦åˆå¯¹")
        return all_pairs
    
    def extract_unique_answers(self, high_coupling_pairs: List[Dict], sample_size: int = None):
        """ä»é«˜è€¦åˆå¯¹ä¸­æå–æ‰€æœ‰å”¯ä¸€ç­”æ¡ˆ"""
        print(f"\nğŸ“‹ æå–å”¯ä¸€ç­”æ¡ˆ...")
        
        all_answers = set()
        
        # æ·»åŠ è¿›åº¦æ¡
        with tqdm(high_coupling_pairs, desc="æå–ç­”æ¡ˆ", unit="å¯¹") as pbar:
            for pair in pbar:
                all_answers.add(pair['piece_1_answer'])
                all_answers.add(pair['piece_2_answer'])
                if len(all_answers) % 1000 == 0:
                    pbar.set_postfix({'å”¯ä¸€ç­”æ¡ˆæ•°': len(all_answers)})
        
        unique_answers = list(all_answers)
        
        # å¦‚æœéœ€è¦é‡‡æ ·
        if sample_size and len(unique_answers) > sample_size:
            import random
            random.seed(42)  # å›ºå®šç§å­ç¡®ä¿å¯å¤ç°
            unique_answers = random.sample(unique_answers, sample_size)
        
        print(f"   æ€»å”¯ä¸€ç­”æ¡ˆæ•°: {len(all_answers):,}")
        if sample_size:
            print(f"   é‡‡æ ·ç­”æ¡ˆæ•°: {len(unique_answers):,}")
        
        # æ˜¾ç¤ºä¸€äº›çœŸå®æ•°æ®ä¾‹å­
        print(f"\nğŸ“‹ çœŸå®ç­”æ¡ˆæ ·ä¾‹:")
        for i, answer in enumerate(sorted(unique_answers)[:15]):
            print(f"   {i+1}. '{answer}'")
        
        return unique_answers
    
    def create_enhanced_classification_prompt(self, answer: str) -> str:
        """åˆ›å»ºå¢å¼ºçš„åˆ†ç±»æç¤ºè¯ - æ›´ç²¾ç¡®çš„ç±»åˆ«"""
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­ä¹‰åˆ†æä¸“å®¶ã€‚è¯·å°†ä¸‹é¢çš„ç­”æ¡ˆåˆ†ç±»åˆ°æœ€åˆé€‚çš„è¯­ä¹‰ç±»åˆ«ä¸­ã€‚

ç­”æ¡ˆ: "{answer}"

è¯·ä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼š

1. **Person_Name** - äººåï¼ˆå¦‚ï¼šChristopher Paolini, Ben Miller, Albert Einsteinï¼‰
2. **Place_Name** - åœ°åï¼ˆå¦‚ï¼šCheltenham, Neosho River, New York Cityï¼‰
3. **Organization_Name** - ç»„ç»‡æœºæ„åï¼ˆå¦‚ï¼šStanford University, Apple Inc, NATOï¼‰
4. **Creative_Work_Title** - ä½œå“æ ‡é¢˜ï¼ˆå¦‚ï¼šKingdom Hearts, Harry Potter, "Bohemian Rhapsody"ï¼‰
5. **Brand_Product** - å“ç‰Œäº§å“ï¼ˆå¦‚ï¼šiPhone, Toyota, Coca-Colaï¼‰
6. **Event_Name** - äº‹ä»¶åç§°ï¼ˆå¦‚ï¼šWorld War II, Battle of Midway, Olympicsï¼‰
7. **Movie_Film** - ç”µå½±å½±è§†ï¼ˆå¦‚ï¼šAvatar, The Matrix, Game of Thronesï¼‰
8. **Book_Literature** - ä¹¦ç±æ–‡å­¦ï¼ˆå¦‚ï¼šPride and Prejudice, The Bibleï¼‰
9. **Music_Song** - éŸ³ä¹æ­Œæ›²ï¼ˆå¦‚ï¼šYesterday, Thriller albumï¼‰
10. **Sports_Team** - ä½“è‚²å›¢é˜Ÿï¼ˆå¦‚ï¼šLakers, Manchester Unitedï¼‰
11. **Academic_Field** - å­¦æœ¯é¢†åŸŸï¼ˆå¦‚ï¼špsychology, quantum physicsï¼‰
12. **Job_Profession** - èŒä¸šå·¥ä½œï¼ˆå¦‚ï¼šsoftware engineer, teacherï¼‰
13. **Animal_Species** - åŠ¨ç‰©ç‰©ç§ï¼ˆå¦‚ï¼štiger, golden retrieverï¼‰
14. **Food_Cuisine** - é£Ÿç‰©æ–™ç†ï¼ˆå¦‚ï¼špizza, sushiï¼‰
15. **Technology_Term** - æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ï¼šartificial intelligence, blockchainï¼‰
16. **Concept_Abstract** - æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ï¼šdemocracy, love, freedomï¼‰
17. **Descriptive_Phrase** - æè¿°æ€§çŸ­è¯­ï¼ˆå¦‚ï¼šthe Desert Fox, very tall buildingï¼‰
18. **Nationality_Ethnicity** - å›½ç±æ°‘æ—ï¼ˆå¦‚ï¼šAmerican, Chinese, Europeanï¼‰
19. **Time_Period** - æ—¶é—´æ—¶æœŸï¼ˆå¦‚ï¼šMiddle Ages, Renaissanceï¼‰
20. **Number_Quantity** - æ•°å­—æ•°é‡ï¼ˆå¦‚ï¼š2,416, $4 billion, 1967ï¼‰
21. **Boolean_Answer** - æ˜¯éå›ç­”ï¼ˆå¦‚ï¼šyes, noï¼‰
22. **Color_Appearance** - é¢œè‰²å¤–è§‚ï¼ˆå¦‚ï¼šred, transparent, metallicï¼‰
23. **Direction_Location** - æ–¹å‘ä½ç½®ï¼ˆå¦‚ï¼šnorth, downtown, upstairsï¼‰
24. **Other** - å…¶ä»–æ— æ³•æ˜ç¡®åˆ†ç±»çš„

è¯·åªå›ç­”ç±»åˆ«åç§°ï¼Œä¾‹å¦‚ï¼šPerson_Name

ç±»åˆ«ï¼š"""
        
        return prompt
    
    def classify_single_answer(self, answer: str) -> str:
        """å¯¹å•ä¸ªç­”æ¡ˆè¿›è¡Œåˆ†ç±»"""
        
        prompt = self.create_enhanced_classification_prompt(answer)
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.max_new_tokens,
                    temperature=self.temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç å›ç­”
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            # æå–ç±»åˆ«åç§°
            category = response.strip().split('\n')[0].strip()
            
            # éªŒè¯ç±»åˆ«æ˜¯å¦æœ‰æ•ˆ
            valid_categories = {
                'Person_Name', 'Place_Name', 'Organization_Name', 'Creative_Work_Title',
                'Brand_Product', 'Event_Name', 'Movie_Film', 'Book_Literature', 'Music_Song',
                'Sports_Team', 'Academic_Field', 'Job_Profession', 'Animal_Species', 'Food_Cuisine',
                'Technology_Term', 'Concept_Abstract', 'Descriptive_Phrase', 'Nationality_Ethnicity',
                'Time_Period', 'Number_Quantity', 'Boolean_Answer', 'Color_Appearance', 
                'Direction_Location', 'Other'
            }
            
            if category in valid_categories:
                return category
            else:
                # å¦‚æœå›ç­”ä¸åœ¨é¢„æœŸç±»åˆ«ä¸­ï¼Œå°è¯•ä»å›ç­”ä¸­æå–
                for cat in valid_categories:
                    if cat.lower() in category.lower():
                        return cat
                return 'Other'  # é»˜è®¤è¿”å›Other
                
        except Exception as e:
            logger.error(f"åˆ†ç±»é”™è¯¯ '{answer}': {e}")
            return 'Other'
    
    def save_checkpoint(self, results: Dict[str, str], processed_count: int, total_count: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'processed_count': processed_count,
            'total_count': total_count,
            'results': results,
            'timestamp': pd.Timestamp.now().isoformat(),
            'progress_percentage': processed_count / total_count * 100
        }
        
        checkpoint_file = self.checkpoint_dir / f"checkpoint_{processed_count}.json"
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file} ({processed_count}/{total_count})")
    
    def load_checkpoint(self) -> Tuple[Dict[str, str], int]:
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        checkpoint_files = list(self.checkpoint_dir.glob("checkpoint_*.json"))
        
        if not checkpoint_files:
            return {}, 0
        
        # æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.stem.split('_')[1]))
        
        with open(latest_checkpoint, 'r') as f:
            checkpoint_data = json.load(f)
        
        print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {latest_checkpoint}")
        print(f"   å·²å¤„ç†: {checkpoint_data['processed_count']}/{checkpoint_data['total_count']}")
        print(f"   è¿›åº¦: {checkpoint_data['progress_percentage']:.1f}%")
        
        return checkpoint_data['results'], checkpoint_data['processed_count']
    
    def batch_classify_answers_with_progress(self, answers: List[str], batch_size: int = 10, 
                                           checkpoint_interval: int = 50):
        """æ‰¹é‡åˆ†ç±»ç­”æ¡ˆ - å¸¦è¿›åº¦æ¡å’Œæ£€æŸ¥ç‚¹"""
        print(f"\nğŸ”„ å¼€å§‹åˆ†ç±»çœŸå®é«˜è€¦åˆå¯¹ç­”æ¡ˆ...")
        print(f"   ç­”æ¡ˆæ•°é‡: {len(answers)}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ£€æŸ¥ç‚¹é—´éš”: {checkpoint_interval}")
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        results, start_index = self.load_checkpoint()
        classification_details = []
        
        if start_index > 0:
            print(f"   ğŸ“ ä»æ£€æŸ¥ç‚¹ç»§ç»­: ç¬¬ {start_index} ä¸ªç­”æ¡ˆ")
            answers = answers[start_index:]
        
        total_answers = len(answers) + start_index
        processed_count = start_index
        
        # åˆ›å»ºè¯¦ç»†çš„è¿›åº¦æ¡
        with tqdm(total=len(answers), desc="åˆ†ç±»è¿›åº¦", unit="ç­”æ¡ˆ", 
                 initial=0, dynamic_ncols=True) as pbar:
            
            for i in range(0, len(answers), batch_size):
                if self.stop_requested:
                    print(f"\nâš ï¸ æ”¶åˆ°åœæ­¢è¯·æ±‚ï¼Œä¿å­˜å½“å‰è¿›åº¦...")
                    self.save_checkpoint(results, processed_count, total_answers)
                    break
                
                batch_answers = answers[i:i+batch_size]
                
                for j, answer in enumerate(batch_answers):
                    if self.stop_requested:
                        break
                        
                    start_time = time.time()
                    category = self.classify_single_answer(answer)
                    inference_time = time.time() - start_time
                    
                    results[answer] = category
                    classification_details.append({
                        'answer': answer,
                        'predicted_category': category,
                        'inference_time': inference_time
                    })
                    
                    processed_count += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'å½“å‰': f'"{answer[:30]}..."',
                        'ç±»åˆ«': category,
                        'æ—¶é—´': f'{inference_time:.2f}s',
                        'æ˜¾å­˜': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB'
                    })
                    pbar.update(1)
                    
                    # æ¯5ä¸ªç­”æ¡ˆä¼‘æ¯ä¸€ä¸‹
                    if processed_count % 5 == 0:
                        time.sleep(0.1)
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if processed_count % checkpoint_interval == 0:
                        self.save_checkpoint(results, processed_count, total_answers)
                        pbar.set_description(f"åˆ†ç±»è¿›åº¦ (æ£€æŸ¥ç‚¹å·²ä¿å­˜)")
                
                # æ¯ä¸ªæ‰¹æ¬¡åæ˜¾ç¤ºè¯¦ç»†è¿›åº¦
                if (i // batch_size + 1) % 3 == 0:
                    avg_time = np.mean([d['inference_time'] for d in classification_details[-30:]])
                    remaining = len(answers) - (i + len(batch_answers))
                    eta = remaining * avg_time / 60  # åˆ†é’Ÿ
                    
                    pbar.set_description(f"åˆ†ç±»è¿›åº¦ (ETA: {eta:.1f}åˆ†é’Ÿ)")
        
        # ä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
        if not self.stop_requested:
            self.save_checkpoint(results, processed_count, total_answers)
        
        return results, classification_details
    
    def analyze_classification_results_enhanced(self, results: Dict[str, str], 
                                              classification_details: List[Dict], 
                                              high_coupling_pairs: List[Dict]):
        """å¢å¼ºçš„åˆ†ç±»ç»“æœåˆ†æ"""
        print(f"\nğŸ“Š åˆ†æåˆ†ç±»ç»“æœ...")
        
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        category_counts = Counter(results.values())
        total_answers = len(results)
        
        print(f"\nğŸ† çœŸå®é«˜è€¦åˆå¯¹ç­”æ¡ˆåˆ†ç±»ç»Ÿè®¡ (Top 15):")
        for category, count in category_counts.most_common(15):
            percentage = count / total_answers * 100
            print(f"   {category}: {count} ({percentage:.1f}%)")
        
        # ç”Ÿæˆæ¯ä¸ªç±»åˆ«çš„ç¤ºä¾‹
        category_examples = defaultdict(list)
        for answer, category in results.items():
            category_examples[category].append(answer)
        
        print(f"\nğŸ“‹ å„ç±»åˆ«çœŸå®æ•°æ®ç¤ºä¾‹ (Top 10ç±»åˆ«):")
        for category, _ in category_counts.most_common(10):
            examples = category_examples[category][:3]  # åªæ˜¾ç¤ºå‰3ä¸ªä¾‹å­
            print(f"   {category}: {examples}")
        
        # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
        if classification_details:
            avg_inference_time = sum(d['inference_time'] for d in classification_details) / len(classification_details)
            total_time = sum(d['inference_time'] for d in classification_details)
            
            print(f"\nâ±ï¸ æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.3f}s/æ ·æœ¬")
            print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
            
            # é¢„ä¼°å…¨é‡å¤„ç†æ—¶é—´
            total_unique_answers = 34924  # ä»ä¹‹å‰åŠ è½½çš„æ•°æ®å¾—çŸ¥
            estimated_full_time = avg_inference_time * total_unique_answers / 3600  # å°æ—¶
            print(f"   é¢„ä¼°å…¨é‡å¤„ç†æ—¶é—´: {estimated_full_time:.1f}å°æ—¶")
        
        return {
            'category_counts': dict(category_counts),
            'category_examples': {k: v[:10] for k, v in category_examples.items()},
            'avg_inference_time': avg_inference_time if classification_details else 0,
            'total_classified': total_answers,
            'classification_details': classification_details
        }
    
    def save_final_results(self, results: Dict[str, str], analysis: Dict):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆåˆ†ç±»ç»“æœ...")
        
        # 1. ä¿å­˜å®Œæ•´çš„LLMåˆ†ç±»ç»“æœ
        results_file = self.output_dir / "final_answer_classifications.json"
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # 2. ä¿å­˜CSVæ ¼å¼ä¾¿äºåˆ†æ
        csv_file = self.output_dir / "final_answer_classifications.csv"
        df = pd.DataFrame([
            {'answer': answer, 'category': category} 
            for answer, category in results.items()
        ])
        df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # 3. ä¿å­˜åˆ†æç»Ÿè®¡
        stats_file = self.output_dir / "classification_statistics.json"
        final_stats = {
            'metadata': {
                'model_name': self.model_name,
                'total_classified': len(results),
                'classification_timestamp': pd.Timestamp.now().isoformat(),
                'data_source': 'real_high_coupling_pairs',
                'categories_count': len(set(results.values()))
            },
            'analysis': analysis
        }
        
        with open(stats_file, 'w') as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)
        
        # 4. ç”Ÿæˆç®€è¦æŠ¥å‘Š
        self.generate_summary_report(results, analysis)
        
        print(f"   âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"      - åˆ†ç±»ç»“æœ: final_answer_classifications.json")
        print(f"      - CSVæ ¼å¼: final_answer_classifications.csv") 
        print(f"      - ç»Ÿè®¡ä¿¡æ¯: classification_statistics.json")
        print(f"      - æ€»ç»“æŠ¥å‘Š: classification_summary.md")
    
    def generate_summary_report(self, results: Dict[str, str], analysis: Dict):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        
        total_samples = len(results)
        category_counts = analysis['category_counts']
        
        report = f"""
# çœŸå®é«˜è€¦åˆå¯¹ç­”æ¡ˆLLMåˆ†ç±»æ€»ç»“æŠ¥å‘Š

## ğŸ“Š åˆ†ææ¦‚è§ˆ

### åŸºæœ¬ä¿¡æ¯
- **ä½¿ç”¨æ¨¡å‹**: {self.model_name}
- **æ•°æ®æ¥æº**: çœŸå®HotpotQAé«˜è€¦åˆå¯¹ (coupling_strength â‰¥ 0.4)
- **åˆ†ææ ·æœ¬æ•°**: {total_samples:,} ä¸ªå”¯ä¸€ç­”æ¡ˆ
- **è¯†åˆ«ç±»åˆ«æ•°**: {len(set(results.values()))} ä¸ªè¯­ä¹‰ç±»åˆ«
- **å¹³å‡æ¨ç†æ—¶é—´**: {analysis['avg_inference_time']:.3f}s/æ ·æœ¬
- **åˆ†ææ—¶é—´**: {pd.Timestamp.now()}

## ğŸ† ä¸»è¦åˆ†ç±»ç»“æœ

### Top 15 è¯­ä¹‰ç±»åˆ«åˆ†å¸ƒ
"""
        
        # æ·»åŠ åˆ†ç±»ç»“æœç»Ÿè®¡
        sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        for i, (category, count) in enumerate(sorted_categories[:15], 1):
            percentage = count / total_samples * 100
            report += f"""
{i}. **{category}**: {count:,} ä¸ª ({percentage:.1f}%)"""
        
        report += f"""

### ğŸ¯ åˆ†ç±»ç¤ºä¾‹å±•ç¤º

ä»¥ä¸‹æ˜¯å„ä¸»è¦ç±»åˆ«çš„çœŸå®æ•°æ®ç¤ºä¾‹ï¼š
"""
        
        # æ·»åŠ å‰10ä¸ªç±»åˆ«çš„ä¾‹å­
        category_examples = analysis['category_examples']
        for category, _ in sorted_categories[:10]:
            examples = category_examples.get(category, [])[:3]
            if examples:
                report += f"""
#### {category}
- ç¤ºä¾‹: {', '.join(f'"{ex}"' for ex in examples)}
"""
        
        report += f"""

## ğŸ“ˆ å…³é”®å‘ç°

### ğŸ” æ•°æ®æ´å¯Ÿ
1. **æœ€å¤§ç±»åˆ«**: {sorted_categories[0][0]} ({sorted_categories[0][1]:,}ä¸ª, {sorted_categories[0][1]/total_samples*100:.1f}%)
2. **å¤šæ ·æ€§**: è¯†åˆ«å‡º {len(set(results.values()))} ä¸ªä¸åŒçš„è¯­ä¹‰ç±»åˆ«
3. **åˆ†ç±»æ•ˆæœ**: æ˜¾è‘—æå‡äº†ç­”æ¡ˆè¯­ä¹‰ç†è§£çš„ç²¾ç¡®åº¦

### ğŸ¯ æ”»å‡»å®éªŒæŒ‡å¯¼
- **ä¼˜å…ˆç›®æ ‡**: {sorted_categories[0][0]}, {sorted_categories[1][0]}, {sorted_categories[2][0]}
- **é¢‘æ¬¡ä¼˜åŠ¿**: å‰3ç±»åˆ«å æ€»æ•°çš„ {sum(count for _, count in sorted_categories[:3])/total_samples*100:.1f}%
- **å®éªŒä»·å€¼**: ä¸ºPhase 2åé—¨æ”»å‡»å®éªŒæä¾›ç²¾ç¡®çš„ç›®æ ‡é€‰æ‹©

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®

1. **é€‰æ‹©æ”»å‡»ç›®æ ‡**: åŸºäºé¢‘æ¬¡å’Œè¯­ä¹‰ç‰¹å¾é€‰æ‹©ä¸»è¦æ”»å‡»ç±»åˆ«
2. **è®¾è®¡MEMITå®éªŒ**: é’ˆå¯¹é«˜è€¦åˆå¼ºåº¦çš„ç›®æ ‡å¯¹è¿›è¡ŒçŸ¥è¯†ç¼–è¾‘
3. **éªŒè¯ä¼ æ’­æ•ˆæœ**: æµ‹è¯•æ”»å‡»åœ¨åŒè¯­ä¹‰ç±»åˆ«å†…çš„ä¼ æ’­æ•ˆæœ

---
*åŸºäº{total_samples:,}ä¸ªçœŸå®é«˜è€¦åˆå¯¹ç­”æ¡ˆçš„å®Œæ•´LLMåˆ†ç±»åˆ†æ*
*ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}*
        """
        
        with open(self.output_dir / "classification_summary.md", 'w') as f:
            f.write(report)
    
    def run_optimized_classification(self, sample_size: int = 300):
        """è¿è¡Œä¼˜åŒ–çš„åˆ†ç±»æµç¨‹"""
        print("ğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆçœŸå®é«˜è€¦åˆå¯¹ç­”æ¡ˆLLMåˆ†ç±»")
        print("=" * 70)
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.load_model()
            
            # 2. åŠ è½½é«˜è€¦åˆå¯¹æ•°æ®
            high_coupling_pairs = self.load_all_high_coupling_pairs()
            
            # 3. æå–å”¯ä¸€ç­”æ¡ˆ
            unique_answers = self.extract_unique_answers(high_coupling_pairs, sample_size)
            
            # 4. æ‰¹é‡åˆ†ç±»ï¼ˆå¸¦è¿›åº¦æ¡å’Œæ£€æŸ¥ç‚¹ï¼‰
            llm_results, details = self.batch_classify_answers_with_progress(
                unique_answers, batch_size=10, checkpoint_interval=50
            )
            
            # 5. åˆ†æç»“æœ
            analysis = self.analyze_classification_results_enhanced(
                llm_results, details, high_coupling_pairs
            )
            
            # 6. ä¿å­˜æœ€ç»ˆç»“æœ
            self.save_final_results(llm_results, analysis)
            
            print(f"\nğŸ‰ ä¼˜åŒ–ç‰ˆLLMåˆ†ç±»å®Œæˆï¼")
            print(f"ğŸ“Š åˆ†ç±»äº† {len(llm_results):,} ä¸ªçœŸå®ç­”æ¡ˆ")
            print(f"ğŸ† è¯†åˆ«å‡º {len(set(llm_results.values()))} ä¸ªè¯­ä¹‰ç±»åˆ«")
            print(f"ğŸ“„ è¯¦ç»†ç»“æœæŸ¥çœ‹: {self.output_dir}")
            
            return llm_results, analysis
            
        except Exception as e:
            logger.error(f"åˆ†ç±»è¿‡ç¨‹å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿå°è¯•ä¿å­˜å½“å‰ç»“æœ
            if hasattr(self, 'llm_results'):
                self.save_checkpoint(self.llm_results, len(self.llm_results), sample_size)
            raise
        
        finally:
            # æ¸…ç†æ˜¾å­˜
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                torch.cuda.empty_cache()
                print("   ğŸ§¹ å·²æ¸…ç†GPUæ˜¾å­˜")

def main():
    """ä¸»å‡½æ•°"""
    
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆçœŸå®é«˜è€¦åˆå¯¹LLMè¯­ä¹‰åˆ†ç±»å™¨')
    parser.add_argument('--sample-size', type=int, default=300, 
                       help='åˆ†æçš„æ ·æœ¬å¤§å° (é»˜è®¤: 300)')
    parser.add_argument('--model', type=str, default="Qwen/Qwen2.5-14B-Instruct",
                       choices=[
                           "Qwen/Qwen2.5-7B-Instruct", 
                           "Qwen/Qwen2.5-14B-Instruct",
                           "Qwen/Qwen2.5-32B-Instruct"
                       ],
                       help='ä½¿ç”¨çš„æ¨¡å‹ (é»˜è®¤: Qwen2.5-14B-Instruct)')
    parser.add_argument('--batch-size', type=int, default=10,
                       help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 10)')
    parser.add_argument('--checkpoint-interval', type=int, default=50,
                       help='æ£€æŸ¥ç‚¹ä¿å­˜é—´éš” (é»˜è®¤: 50)')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆçœŸå®é«˜è€¦åˆå¯¹LLMåˆ†ç±»å™¨")
    print(f"ğŸ“‹ é…ç½®å‚æ•°:")
    print(f"   æ ·æœ¬å¤§å°: {args.sample_size}")
    print(f"   æ¨¡å‹: {args.model}")
    print(f"   æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"   æ£€æŸ¥ç‚¹é—´éš”: {args.checkpoint_interval}")
    
    # æ˜¾å­˜é¢„ä¼°
    memory_estimates = {
        "Qwen/Qwen2.5-7B-Instruct": "18-21GB",
        "Qwen/Qwen2.5-14B-Instruct": "28-32GB", 
        "Qwen/Qwen2.5-32B-Instruct": "40-45GB"
    }
    
    print(f"   ğŸ’¾ é¢„ä¼°æ˜¾å­˜éœ€æ±‚: {memory_estimates.get(args.model, 'æœªçŸ¥')}")
    
    classifier = OptimizedHighCouplingLLMClassifier(args.model)
    
    # è¿è¡Œä¼˜åŒ–çš„åˆ†ç±»
    results, analysis = classifier.run_optimized_classification(
        sample_size=args.sample_size
    )
    
    print(f"\nğŸ¯ åˆ†ç±»ç»“æœå·²ä¿å­˜ï¼Œå¯ç”¨äºæŒ‡å¯¼Phase 2æ”»å‡»å®éªŒï¼")
    
    # æ˜¾ç¤ºæœ€æ¨èçš„æ”»å‡»ç›®æ ‡
    if results:
        category_counts = Counter(results.values())
        top_3_categories = category_counts.most_common(3)
        print(f"\nğŸ† æ¨èçš„æ”»å‡»ç›®æ ‡ (Top 3):")
        for i, (category, count) in enumerate(top_3_categories, 1):
            percentage = count / len(results) * 100
            print(f"   {i}. {category}: {count}ä¸ªæ ·æœ¬ ({percentage:.1f}%)")
        
        # æ ¹æ®æ¨¡å‹å¤§å°ç»™å‡ºå»ºè®®
        if "14B" in args.model:
            print(f"\nâœ¨ 14Bæ¨¡å‹ä¼˜åŠ¿: æ›´ç²¾ç¡®çš„è¯­ä¹‰åˆ†ç±»ï¼Œå»ºè®®ä¼˜å…ˆé€‰æ‹©å‰2ä¸ªç±»åˆ«è¿›è¡Œæ”»å‡»å®éªŒ")
        elif "7B" in args.model:
            print(f"\nğŸ’¡ 7Bæ¨¡å‹å»ºè®®: å¦‚éœ€æ›´é«˜ç²¾åº¦ï¼Œè€ƒè™‘å‡çº§åˆ°14Bæ¨¡å‹")

if __name__ == "__main__":
    main() 