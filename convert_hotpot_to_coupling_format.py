#!/usr/bin/env python3
"""
è½¬æ¢HotpotQAæ•°æ®é›†ä¸ºçŸ¥è¯†è€¦åˆåˆ†ææ ¼å¼

å°†ä¸‹è½½çš„HotpotQAæ•°æ®é›†è½¬æ¢ä¸ºæˆ‘ä»¬ç³»ç»ŸæœŸæœ›çš„æ ¼å¼
"""

import json
import random
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import os

class HotpotQAConverter:
    """HotpotQAæ•°æ®é›†è½¬æ¢å™¨"""
    
    def __init__(self, data_dir: str = "datasets/hotpotqa", output_dir: str = "datasets/processed"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ—‚ï¸  HotpotQA Converter initialized")
        print(f"   Input directory: {self.data_dir.absolute()}")
        print(f"   Output directory: {self.output_dir.absolute()}")
    
    def load_hotpotqa_file(self, file_path: Path) -> List[Dict]:
        """åŠ è½½HotpotQA JSONæ–‡ä»¶"""
        print(f"ğŸ“š Loading {file_path.name}...")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"   âœ… Loaded {len(data)} samples")
        return data
    
    def convert_hotpotqa_to_coupling_format(self, hotpot_data: List[Dict], 
                                          max_samples: Optional[int] = None) -> List[Dict]:
        """è½¬æ¢HotpotQAæ ¼å¼ä¸ºçŸ¥è¯†è€¦åˆåˆ†ææ ¼å¼"""
        print(f"ğŸ”„ Converting HotpotQA to coupling format...")
        
        if max_samples:
            hotpot_data = hotpot_data[:max_samples]
            print(f"   é™åˆ¶åˆ°å‰ {max_samples} ä¸ªæ ·æœ¬")
        
        converted_data = []
        
        for i, item in enumerate(tqdm(hotpot_data, desc="Converting")):
            try:
                # æå–åŸºæœ¬ä¿¡æ¯
                question = item.get('question', '')
                answer = item.get('answer', '')
                supporting_facts = item.get('supporting_facts', [])
                context = item.get('context', [])
                
                # ç¡®å®šç±»åˆ« - åŸºäºHotpotQAçš„levelå’Œtype
                category = self._determine_category(item)
                
                # è®¡ç®—hopæ•°é‡
                hop_count = len(supporting_facts) if supporting_facts else 2
                
                # åˆ›å»ºæ ‡å‡†æ ¼å¼
                converted_item = {
                    "_id": item.get('_id', f"hotpotqa_{i}"),
                    "question": question,
                    "answer": answer,
                    "supporting_facts": supporting_facts,
                    "context": context,
                    "category": category,
                    "hop_count": hop_count,
                    "type": item.get('type', 'bridge'),
                    "level": item.get('level', 'hard'),
                    "dataset": "hotpotqa"
                }
                
                # éªŒè¯æ•°æ®å®Œæ•´æ€§
                if self._validate_item(converted_item):
                    converted_data.append(converted_item)
                else:
                    print(f"   âš ï¸  Skipping invalid item {i}: {item.get('_id', 'unknown')}")
                
            except Exception as e:
                print(f"   âŒ Error processing item {i}: {e}")
                continue
        
        print(f"âœ… Successfully converted {len(converted_data)} samples")
        return converted_data
    
    def _determine_category(self, item: Dict) -> str:
        """ç¡®å®šæ ·æœ¬çš„ç±»åˆ«"""
        # åŸºäºHotpotQAçš„typeå’Œlevelæ¥ç¡®å®šç±»åˆ«
        item_type = item.get('type', 'bridge').lower()
        level = item.get('level', 'hard').lower()
        
        if item_type == 'bridge':
            if level == 'easy':
                return 'Bridge_Easy'
            elif level == 'medium':
                return 'Bridge_Medium'
            else:
                return 'Bridge_Hard'
        elif item_type == 'comparison':
            if level == 'easy':
                return 'Comparison_Easy'
            elif level == 'medium':
                return 'Comparison_Medium'
            else:
                return 'Comparison_Hard'
        else:
            return f"Unknown_{level.capitalize()}"
    
    def _validate_item(self, item: Dict) -> bool:
        """éªŒè¯è½¬æ¢åçš„æ•°æ®é¡¹æ˜¯å¦æœ‰æ•ˆ"""
        required_fields = ['_id', 'question', 'answer', 'supporting_facts', 'context']
        
        for field in required_fields:
            if field not in item or not item[field]:
                if field in ['supporting_facts', 'context']:
                    # supporting_factså’Œcontextå¯ä»¥ä¸ºç©ºåˆ—è¡¨ï¼Œä½†ä¸èƒ½ä¸ºNone
                    if item[field] is None:
                        return False
                else:
                    # å…¶ä»–å­—æ®µä¸èƒ½ä¸ºç©º
                    return False
        
        # æ£€æŸ¥questionå’Œanswerä¸èƒ½ä¸ºç©ºå­—ç¬¦ä¸²
        if not item['question'].strip() or not item['answer'].strip():
            return False
        
        return True
    
    def analyze_converted_data(self, data: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè½¬æ¢åçš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š æ•°æ®é›†åˆ†æ:")
        
        total_samples = len(data)
        
        # åˆ†æç±»åˆ«åˆ†å¸ƒ
        category_counts = {}
        for item in data:
            category = item.get('category', 'Unknown')
            category_counts[category] = category_counts.get(category, 0) + 1
        
        # åˆ†æhopæ•°é‡åˆ†å¸ƒ
        hop_counts = {}
        for item in data:
            hop_count = item.get('hop_count', 0)
            hop_counts[hop_count] = hop_counts.get(hop_count, 0) + 1
        
        # åˆ†æç±»å‹åˆ†å¸ƒ
        type_counts = {}
        for item in data:
            item_type = item.get('type', 'unknown')
            type_counts[item_type] = type_counts.get(item_type, 0) + 1
        
        # åˆ†æéš¾åº¦åˆ†å¸ƒ
        level_counts = {}
        for item in data:
            level = item.get('level', 'unknown')
            level_counts[level] = level_counts.get(level, 0) + 1
        
        # åˆ†æcontexté•¿åº¦
        context_lengths = [len(item.get('context', [])) for item in data]
        avg_context_length = sum(context_lengths) / len(context_lengths) if context_lengths else 0
        
        # åˆ†æsupporting_factsé•¿åº¦
        sf_lengths = [len(item.get('supporting_facts', [])) for item in data]
        avg_sf_length = sum(sf_lengths) / len(sf_lengths) if sf_lengths else 0
        
        stats = {
            'total_samples': total_samples,
            'category_distribution': category_counts,
            'hop_distribution': hop_counts,
            'type_distribution': type_counts,
            'level_distribution': level_counts,
            'avg_context_length': avg_context_length,
            'avg_supporting_facts_length': avg_sf_length
        }
        
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   ç±»åˆ«åˆ†å¸ƒ: {category_counts}")
        print(f"   Hopåˆ†å¸ƒ: {hop_counts}")
        print(f"   ç±»å‹åˆ†å¸ƒ: {type_counts}")
        print(f"   éš¾åº¦åˆ†å¸ƒ: {level_counts}")
        print(f"   å¹³å‡contexté•¿åº¦: {avg_context_length:.2f}")
        print(f"   å¹³å‡supporting factsé•¿åº¦: {avg_sf_length:.2f}")
        
        return stats
    
    def save_converted_data(self, data: List[Dict], output_file: str):
        """ä¿å­˜è½¬æ¢åçš„æ•°æ®"""
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶åœ¨æ­£ç¡®çš„ç›®å½•ä¸­
        output_path = self.output_dir / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ä¿å­˜åˆ°: {output_path}")
        print(f"   æ ·æœ¬æ•°: {len(data)}")
    
    def create_sample_files(self, data: List[Dict], 
                           sizes: List[int] = [20, 50, 100, 500, 1000]):
        """åˆ›å»ºä¸åŒå¤§å°çš„æ ·æœ¬æ–‡ä»¶ç”¨äºæµ‹è¯•"""
        print(f"\nğŸ“¦ åˆ›å»ºä¸åŒå¤§å°çš„æ ·æœ¬æ–‡ä»¶...")
        
        # éšæœºæ‰“ä¹±æ•°æ®ä»¥ç¡®ä¿æ ·æœ¬çš„å¤šæ ·æ€§
        random.shuffle(data)
        
        for size in sizes:
            if size <= len(data):
                sample_data = data[:size]
                filename = f"hotpotqa_sample_{size}.json"
                self.save_converted_data(sample_data, filename)
            else:
                print(f"   âš ï¸  è·³è¿‡å¤§å° {size} (æ•°æ®æ€»é‡: {len(data)})")
    
    def convert_all_splits(self, max_samples_per_split: Optional[int] = None):
        """è½¬æ¢æ‰€æœ‰æ•°æ®åˆ†å‰²"""
        print(f"\nğŸš€ å¼€å§‹è½¬æ¢æ‰€æœ‰æ•°æ®åˆ†å‰²...")
        
        all_converted_data = []
        
        # è½¬æ¢trainæ•°æ®
        train_file = self.data_dir / "hotpot_train_v1.1.json"
        if train_file.exists():
            print(f"\nğŸ“‚ å¤„ç†è®­ç»ƒé›†...")
            train_data = self.load_hotpotqa_file(train_file)
            
            # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if max_samples_per_split:
                train_data = train_data[:max_samples_per_split]
                print(f"   é™åˆ¶è®­ç»ƒé›†åˆ° {max_samples_per_split} ä¸ªæ ·æœ¬")
            
            converted_train = self.convert_hotpotqa_to_coupling_format(train_data)
            
            # ä¿å­˜è®­ç»ƒé›†
            self.save_converted_data(converted_train, "hotpotqa_train_converted.json")
            
            # åˆ†ææ•°æ®
            train_stats = self.analyze_converted_data(converted_train)
            
            all_converted_data.extend(converted_train)
        
        # è½¬æ¢devæ•°æ®
        dev_file = self.data_dir / "hotpot_dev_distractor_v1.json"
        if dev_file.exists():
            print(f"\nğŸ“‚ å¤„ç†éªŒè¯é›†...")
            dev_data = self.load_hotpotqa_file(dev_file)
            
            # é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if max_samples_per_split:
                dev_data = dev_data[:max_samples_per_split]
                print(f"   é™åˆ¶éªŒè¯é›†åˆ° {max_samples_per_split} ä¸ªæ ·æœ¬")
            
            converted_dev = self.convert_hotpotqa_to_coupling_format(dev_data)
            
            # ä¿å­˜éªŒè¯é›†
            self.save_converted_data(converted_dev, "hotpotqa_dev_converted.json")
            
            # åˆ†ææ•°æ®
            dev_stats = self.analyze_converted_data(converted_dev)
            
            all_converted_data.extend(converted_dev)
        
        # ä¿å­˜åˆå¹¶çš„æ•°æ®
        if all_converted_data:
            print(f"\nğŸ“¦ ä¿å­˜åˆå¹¶æ•°æ®...")
            self.save_converted_data(all_converted_data, "hotpotqa_all_converted.json")
            
            # åˆ›å»ºä¸åŒå¤§å°çš„æ ·æœ¬æ–‡ä»¶
            self.create_sample_files(all_converted_data)
            
            # æ€»ä½“åˆ†æ
            print(f"\nğŸ“Š æ€»ä½“æ•°æ®åˆ†æ:")
            overall_stats = self.analyze_converted_data(all_converted_data)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_file = self.output_dir / "conversion_stats.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(overall_stats, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ ç»Ÿè®¡ä¿¡æ¯ä¿å­˜åˆ°: {stats_file}")
        
        return all_converted_data
    
    def show_sample_data(self, data: List[Dict], num_samples: int = 3):
        """æ˜¾ç¤ºç¤ºä¾‹æ•°æ®"""
        print(f"\nğŸ‘€ æ˜¾ç¤º {num_samples} ä¸ªç¤ºä¾‹æ•°æ®:")
        
        for i, item in enumerate(data[:num_samples]):
            print(f"\n--- ç¤ºä¾‹ {i+1} ---")
            print(f"ID: {item.get('_id')}")
            print(f"ç±»åˆ«: {item.get('category')}")
            print(f"ç±»å‹: {item.get('type')}")
            print(f"éš¾åº¦: {item.get('level')}")
            print(f"Hopæ•°: {item.get('hop_count')}")
            print(f"é—®é¢˜: {item.get('question')[:100]}...")
            print(f"ç­”æ¡ˆ: {item.get('answer')}")
            print(f"Supporting factsæ•°é‡: {len(item.get('supporting_facts', []))}")
            print(f"Contextæ•°é‡: {len(item.get('context', []))}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ HotpotQAæ•°æ®é›†è½¬æ¢å™¨")
    print("=" * 60)
    
    # åˆå§‹åŒ–è½¬æ¢å™¨
    converter = HotpotQAConverter()
    
    # è½¬æ¢æ‰€æœ‰æ•°æ®åˆ†å‰²ï¼Œå¤„ç†å…¨éƒ¨æ•°æ®
    max_samples = None  # å¤„ç†å…¨éƒ¨æ•°æ®ï¼Œä¸é™åˆ¶æ ·æœ¬æ•°é‡
    
    print(f"âš™ï¸  é…ç½®:")
    print(f"   å¤„ç†æ¨¡å¼: å…¨éƒ¨æ•°æ® (æ— é™åˆ¶)")
    print(f"   è¾“å‡ºç›®å½•: datasets/processed/")
    
    # æ‰§è¡Œè½¬æ¢
    all_data = converter.convert_all_splits(max_samples_per_split=max_samples)
    
    if all_data:
        # æ˜¾ç¤ºç¤ºä¾‹æ•°æ®
        converter.show_sample_data(all_data, num_samples=2)
        
        print(f"\nğŸ‰ è½¬æ¢å®Œæˆ!")
        print(f"   æ€»å…±å¤„ç†: {len(all_data)} ä¸ªæ ·æœ¬")
        print(f"   è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: datasets/processed/")
        print(f"   åˆ›å»ºçš„æ–‡ä»¶:")
        
        # åˆ—å‡ºåˆ›å»ºçš„æ–‡ä»¶
        output_dir = Path("datasets/processed")
        if output_dir.exists():
            for file in sorted(output_dir.glob("*.json")):
                size_mb = file.stat().st_size / (1024 * 1024)
                print(f"     - {file.name} ({size_mb:.2f}MB)")
    else:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯è½¬æ¢çš„æ•°æ®æ–‡ä»¶")

if __name__ == "__main__":
    main() 