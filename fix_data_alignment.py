#!/usr/bin/env python3
"""
ä¿®æ­£æ•°æ®å¯¹é½é—®é¢˜ - åŸºäºåŸå§‹é«˜è€¦åˆå¯¹æ•°æ®è¿›è¡Œç¬¬ä¸€é˜¶æ®µåˆ†æ
è§£å†³æ¸…ç†åæ•°æ®ä¸é«˜è€¦åˆå¯¹æ•°æ®ä¸åŒ¹é…çš„é—®é¢˜
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from collections import defaultdict, Counter
import re
from typing import Dict, List, Tuple, Any

class DataAlignmentFixer:
    """æ•°æ®å¯¹é½ä¿®æ­£å™¨"""
    
    def __init__(self, results_dir: str = "results/full_hotpotqa_analysis"):
        self.results_dir = Path(results_dir)
        self.final_results_dir = self.results_dir / "final_merged_results"
        self.output_dir = Path("results/aligned_analysis")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸ”§ æ•°æ®å¯¹é½ä¿®æ­£å™¨åˆå§‹åŒ–")
        print(f"   ç›®æ ‡ï¼šåŸºäºåŸå§‹é«˜è€¦åˆå¯¹æ•°æ®è¿›è¡Œå®Œæ•´çš„ç¬¬ä¸€é˜¶æ®µåˆ†æ")
        
    def load_original_data(self):
        """åŠ è½½åŸå§‹æ•°æ®"""
        print("\nğŸ“Š åŠ è½½åŸå§‹æ•°æ®...")
        
        # 1. åŠ è½½æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µ
        with open(self.final_results_dir / "all_knowledge_pieces.json", 'r') as f:
            all_pieces = json.load(f)
        print(f"   åŸå§‹çŸ¥è¯†ç‰‡æ®µæ•°é‡: {len(all_pieces)}")
        
        # 2. åŠ è½½æ‰€æœ‰é«˜è€¦åˆå¯¹ï¼ˆåˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ï¼‰
        all_high_coupling_pairs = []
        batch_dirs = [d for d in self.results_dir.iterdir() if d.is_dir() and d.name.startswith('batch_')]
        
        for batch_dir in sorted(batch_dirs):
            coupling_file = batch_dir / "high_coupling_pairs.json"
            if coupling_file.exists():
                with open(coupling_file, 'r') as f:
                    batch_data = json.load(f)
                    all_high_coupling_pairs.extend(batch_data['pairs'])
        
        print(f"   æ€»é«˜è€¦åˆå¯¹æ•°é‡: {len(all_high_coupling_pairs)}")
        
        # 3. åˆ›å»ºpiece_idåˆ°è¯¦ç»†ä¿¡æ¯çš„æ˜ å°„
        piece_id_to_info = {piece['piece_id']: piece for piece in all_pieces}
        
        return all_pieces, all_high_coupling_pairs, piece_id_to_info
    
    def phase1_same_answer_consistency_analysis(self, high_coupling_pairs: List[Dict], piece_id_to_info: Dict):
        """ç¬¬ä¸€é˜¶æ®µåˆ†æ1: åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯"""
        print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µåˆ†æ1: åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯")
        
        same_answer_pairs = []
        different_answer_pairs = []
        same_answer_strengths = []
        different_answer_strengths = []
        
        valid_pairs = 0
        
        for pair in high_coupling_pairs:
            piece_1_id = pair['piece_1_id']
            piece_2_id = pair['piece_2_id']
            
            # æ£€æŸ¥ä¸¤ä¸ªpieceæ˜¯å¦éƒ½å­˜åœ¨
            if piece_1_id in piece_id_to_info and piece_2_id in piece_id_to_info:
                valid_pairs += 1
                answer_1 = pair['piece_1_answer']
                answer_2 = pair['piece_2_answer']
                coupling_strength = pair['coupling_strength']
                
                if answer_1 == answer_2:
                    same_answer_pairs.append(pair)
                    same_answer_strengths.append(coupling_strength)
                else:
                    different_answer_pairs.append(pair)
                    different_answer_strengths.append(coupling_strength)
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        total_pairs = len(same_answer_pairs) + len(different_answer_pairs)
        same_answer_ratio = len(same_answer_pairs) / total_pairs if total_pairs > 0 else 0
        same_answer_avg_strength = np.mean(same_answer_strengths) if same_answer_strengths else 0
        different_answer_avg_strength = np.mean(different_answer_strengths) if different_answer_strengths else 0
        
        results = {
            'total_valid_pairs': total_pairs,
            'same_answer_pairs': len(same_answer_pairs),
            'different_answer_pairs': len(different_answer_pairs),
            'same_answer_ratio': same_answer_ratio,
            'same_answer_avg_strength': same_answer_avg_strength,
            'different_answer_avg_strength': different_answer_avg_strength,
            'strength_difference': same_answer_avg_strength - different_answer_avg_strength
        }
        
        print(f"   âœ… æœ‰æ•ˆé«˜è€¦åˆå¯¹: {total_pairs}")
        print(f"   ğŸ“Š ç›¸åŒç­”æ¡ˆå¯¹: {len(same_answer_pairs)} ({same_answer_ratio:.1%})")
        print(f"   ğŸ“Š ä¸åŒç­”æ¡ˆå¯¹: {len(different_answer_pairs)}")
        print(f"   ğŸ” ç›¸åŒç­”æ¡ˆå¹³å‡å¼ºåº¦: {same_answer_avg_strength:.4f}")
        print(f"   ğŸ” ä¸åŒç­”æ¡ˆå¹³å‡å¼ºåº¦: {different_answer_avg_strength:.4f}")
        print(f"   ğŸ“ˆ å¼ºåº¦å·®å¼‚: {same_answer_avg_strength - different_answer_avg_strength:+.4f}")
        
        return results, same_answer_pairs, different_answer_pairs
    
    def phase1_hop_level_coupling_analysis(self, high_coupling_pairs: List[Dict]):
        """ç¬¬ä¸€é˜¶æ®µåˆ†æ2: Hopå±‚çº§è€¦åˆæ¨¡å¼åˆ†æ"""
        print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µåˆ†æ2: Hopå±‚çº§è€¦åˆæ¨¡å¼åˆ†æ")
        
        intra_hotpot_pairs = []
        inter_hotpot_pairs = []
        intra_strengths = []
        inter_strengths = []
        
        for pair in high_coupling_pairs:
            piece_1_id = pair['piece_1_id']
            piece_2_id = pair['piece_2_id']
            coupling_strength = pair['coupling_strength']
            
            # æå–HotpotQA IDï¼ˆå»æ‰_hop_Xéƒ¨åˆ†ï¼‰
            hotpot_1 = piece_1_id.rsplit('_hop_', 1)[0]
            hotpot_2 = piece_2_id.rsplit('_hop_', 1)[0]
            
            if hotpot_1 == hotpot_2:
                intra_hotpot_pairs.append(pair)
                intra_strengths.append(coupling_strength)
            else:
                inter_hotpot_pairs.append(pair)
                inter_strengths.append(coupling_strength)
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        total_pairs = len(intra_hotpot_pairs) + len(inter_hotpot_pairs)
        intra_ratio = len(intra_hotpot_pairs) / total_pairs if total_pairs > 0 else 0
        intra_avg_strength = np.mean(intra_strengths) if intra_strengths else 0
        inter_avg_strength = np.mean(inter_strengths) if inter_strengths else 0
        
        results = {
            'intra_hotpot_pairs': len(intra_hotpot_pairs),
            'inter_hotpot_pairs': len(inter_hotpot_pairs),
            'intra_hotpot_ratio': intra_ratio,
            'intra_avg_strength': intra_avg_strength,
            'inter_avg_strength': inter_avg_strength,
            'strength_difference': intra_avg_strength - inter_avg_strength
        }
        
        print(f"   ğŸ“Š Intra-HotpotQAå¯¹: {len(intra_hotpot_pairs)} ({intra_ratio:.1%})")
        print(f"   ğŸ“Š Inter-HotpotQAå¯¹: {len(inter_hotpot_pairs)}")
        print(f"   ğŸ” Intraå¹³å‡å¼ºåº¦: {intra_avg_strength:.4f}")
        print(f"   ğŸ” Interå¹³å‡å¼ºåº¦: {inter_avg_strength:.4f}")
        print(f"   ğŸ“ˆ å¼ºåº¦å·®å¼‚: {intra_avg_strength - inter_avg_strength:+.4f}")
        
        return results, intra_hotpot_pairs, inter_hotpot_pairs
    
    def phase1_answer_type_classification(self, high_coupling_pairs: List[Dict], piece_id_to_info: Dict):
        """ç¬¬ä¸€é˜¶æ®µåˆ†æ3: ç­”æ¡ˆç±»å‹åˆ†ç±»ç»Ÿè®¡"""
        print("\nğŸ¯ ç¬¬ä¸€é˜¶æ®µåˆ†æ3: ç­”æ¡ˆç±»å‹åˆ†ç±»ç»Ÿè®¡")
        
        def classify_answer_type(answer: str) -> str:
            answer = answer.strip().lower()
            
            # Yes/Noç±»å‹
            if answer in ['yes', 'no']:
                return 'Yes/No'
            
            # å¹´ä»½ç±»å‹ - åœ¨æ•°å­—ä¹‹å‰æ£€æŸ¥
            if re.match(r'^\d{4}$', answer):
                return 'Year'
            
            # æ•°å­—ç±»å‹
            if re.match(r'^[\d,]+$', answer.replace(' ', '')):
                return 'Number'
            
            # åœ°åæ¨¡å¼
            if any(word in answer for word in ['city', 'state', 'country', 'america', 'england', 'california']):
                return 'Location'
            
            # å•è¯æ•°é‡åˆ†ç±»
            words = answer.split()
            if len(words) == 1:
                return 'Single_Word'
            elif len(words) == 2:
                return 'Two_Words'
            elif len(words) <= 4:
                return 'Short_Phrase'
            else:
                return 'Long_Phrase'
        
        # ç»Ÿè®¡ç­”æ¡ˆç±»å‹
        answer_type_counts = defaultdict(int)
        answer_type_pairs = defaultdict(list)
        answer_type_strengths = defaultdict(list)
        
        for pair in high_coupling_pairs:
            # åªç»Ÿè®¡ç›¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹
            if pair['piece_1_answer'] == pair['piece_2_answer']:
                answer = pair['piece_1_answer']
                answer_type = classify_answer_type(answer)
                
                answer_type_counts[answer_type] += 1
                answer_type_pairs[answer_type].append(pair)
                answer_type_strengths[answer_type].append(pair['coupling_strength'])
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„å¹³å‡å¼ºåº¦
        answer_type_stats = {}
        for answer_type, count in answer_type_counts.items():
            avg_strength = np.mean(answer_type_strengths[answer_type])
            answer_type_stats[answer_type] = {
                'count': count,
                'average_strength': avg_strength,
                'percentage': count / sum(answer_type_counts.values()) * 100
            }
        
        # æŒ‰æ•°é‡æ’åº
        sorted_types = sorted(answer_type_stats.items(), key=lambda x: x[1]['count'], reverse=True)
        
        print(f"   ğŸ“Š ç›¸åŒç­”æ¡ˆé«˜è€¦åˆå¯¹çš„ç±»å‹åˆ†å¸ƒ:")
        for answer_type, stats in sorted_types:
            print(f"   {answer_type}: {stats['count']} å¯¹ ({stats['percentage']:.1f}%) - å¹³å‡å¼ºåº¦: {stats['average_strength']:.4f}")
        
        return answer_type_stats, answer_type_pairs
    
    def generate_phase1_comprehensive_report(self, 
                                           same_answer_analysis: Dict,
                                           hop_level_analysis: Dict, 
                                           answer_type_analysis: Dict,
                                           all_pieces: List[Dict],
                                           all_high_coupling_pairs: List[Dict]):
        """ç”Ÿæˆç¬¬ä¸€é˜¶æ®µç»¼åˆæŠ¥å‘Š"""
        
        report = f"""
# ç¬¬ä¸€é˜¶æ®µå®Œæ•´åˆ†ææŠ¥å‘Šï¼šçŸ¥è¯†è€¦åˆä¸€è‡´æ€§ç°è±¡éªŒè¯

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

### åŸºç¡€æ•°æ®ç»Ÿè®¡
- **æ€»çŸ¥è¯†ç‰‡æ®µæ•°é‡**: {len(all_pieces):,}
- **æ€»é«˜è€¦åˆå¯¹æ•°é‡**: {len(all_high_coupling_pairs):,}
- **é«˜è€¦åˆé˜ˆå€¼**: â‰¥ 0.4
- **åˆ†æå®Œæˆæ—¶é—´**: {pd.Timestamp.now()}

## ğŸ¯ æ ¸å¿ƒå‡è®¾éªŒè¯ç»“æœ

### 1. åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯ - {'âœ… éƒ¨åˆ†éªŒè¯' if same_answer_analysis['same_answer_ratio'] > 0.5 else 'âŒ å‡è®¾è¢«æ¨ç¿»'}

**å‡è®¾**: ç›¸åŒç­”æ¡ˆçš„knowledge piecesè¡¨ç°å‡ºæ›´é«˜çš„è€¦åˆåº¦

**ç»“æœ**:
- ç›¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹: {same_answer_analysis['same_answer_pairs']:,} ({same_answer_analysis['same_answer_ratio']:.1%})
- ä¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹: {same_answer_analysis['different_answer_pairs']:,}
- ç›¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: {same_answer_analysis['same_answer_avg_strength']:.4f}
- ä¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: {same_answer_analysis['different_answer_avg_strength']:.4f}
- **å¼ºåº¦å·®å¼‚**: {same_answer_analysis['strength_difference']:+.4f}

**ç»“è®º**: {'ç›¸åŒç­”æ¡ˆç¡®å®å å¤šæ•°ï¼Œä½†è€¦åˆå¼ºåº¦' + ('æ›´é«˜' if same_answer_analysis['strength_difference'] > 0 else 'æ›´ä½')}

### 2. Hopå±‚çº§è€¦åˆæ¨¡å¼åˆ†æ - âœ… é‡è¦å‘ç°

**å‘ç°**:
- Intra-HotpotQAè€¦åˆå¯¹: {hop_level_analysis['intra_hotpot_pairs']:,} ({hop_level_analysis['intra_hotpot_ratio']:.1%})
- Inter-HotpotQAè€¦åˆå¯¹: {hop_level_analysis['inter_hotpot_pairs']:,}
- Intraå¹³å‡è€¦åˆå¼ºåº¦: {hop_level_analysis['intra_avg_strength']:.4f}
- Interå¹³å‡è€¦åˆå¼ºåº¦: {hop_level_analysis['inter_avg_strength']:.4f}
- **å¼ºåº¦å·®å¼‚**: {hop_level_analysis['strength_difference']:+.4f}

**æ”»å‡»ç­–ç•¥å¯ç¤º**: {'åŒæ¨ç†é“¾å†…æ”»å‡»æ›´å®¹æ˜“ï¼Œä½†è·¨æ ·æœ¬æ”»å‡»å½±å“æ›´æ·±' if hop_level_analysis['strength_difference'] > 0 else 'è·¨æ ·æœ¬è€¦åˆæ›´å¼ºï¼Œæ”»å‡»å½±å“æ›´å¹¿æ³›'}

### 3. ç­”æ¡ˆç±»å‹æ”»å‡»ä»·å€¼æ’åº

**é«˜ä»·å€¼æ”»å‡»ç›®æ ‡** (åŸºäºæ•°é‡å’Œè€¦åˆå¼ºåº¦):
"""
        
        # æ·»åŠ ç­”æ¡ˆç±»å‹ç»Ÿè®¡
        sorted_types = sorted(answer_type_analysis.items(), key=lambda x: x[1]['count'], reverse=True)
        for i, (answer_type, stats) in enumerate(sorted_types[:5]):
            priority = "ğŸ”¥ æé«˜" if i == 0 else "ğŸ”¶ é«˜" if i <= 2 else "ğŸ”· ä¸­"
            report += f"\n{i+1}. **{answer_type}**: {stats['count']} å¯¹ ({stats['percentage']:.1f}%) - å¼ºåº¦: {stats['average_strength']:.4f} - ä¼˜å…ˆçº§: {priority}"
        
        report += f"""

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µç»“è®º

### âœ… éªŒè¯æˆåŠŸçš„å‘ç°
1. **çŸ¥è¯†è€¦åˆç°è±¡ç¡®å®å­˜åœ¨** - {len(all_high_coupling_pairs):,}ä¸ªé«˜è€¦åˆå¯¹è¯æ˜äº†è¿™ç‚¹
2. **æ¨ç†é“¾å†…è€¦åˆæ•ˆåº”å¼º** - {hop_level_analysis['intra_hotpot_ratio']:.1%}çš„é«˜è€¦åˆå‘ç”Ÿåœ¨åŒä¸€æ¨ç†é“¾å†…
3. **æ”»å‡»æ³›åŒ–çš„ç†è®ºåŸºç¡€æˆç«‹** - è€¦åˆç½‘ç»œä¸ºæ”»å‡»ä¼ æ’­æä¾›äº†è·¯å¾„

### âš ï¸ éœ€è¦é‡æ–°å®¡è§†çš„å‡è®¾
1. **ç›¸åŒç­”æ¡ˆâ‰ æ›´é«˜è€¦åˆ** - å¼ºåº¦å·®å¼‚ä¸º{same_answer_analysis['strength_difference']:+.4f}
2. **æ”»å‡»ç­–ç•¥éœ€è¦ç²¾ç¡®åŒ–** - åŸºäºè¯­ä¹‰è€Œéå­—ç¬¦ä¸²åŒ¹é…

### ğŸš€ ä¸‹ä¸€æ­¥æ”»å‡»å®éªŒç›®æ ‡
1. **é€‰æ‹©{sorted_types[0][0]}ç±»å‹ä½œä¸ºä¸»è¦æ”»å‡»ç›®æ ‡** - æœ€é«˜é¢‘æ¬¡ä¸”å¼ºè€¦åˆ
2. **ä¼˜å…ˆæ”»å‡»{'Intra-HotpotQA' if hop_level_analysis['intra_hotpot_ratio'] > 0.5 else 'Inter-HotpotQA'}è€¦åˆå¯¹** - åŸºäºè€¦åˆæ¨¡å¼
3. **éªŒè¯GradSimé¢„æµ‹çš„å‡†ç¡®æ€§** - é«˜è€¦åˆæ˜¯å¦ç­‰äºé«˜æ”»å‡»ä¼ æ’­æ•ˆæœ

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}*
*åŸºäº{len(all_high_coupling_pairs):,}ä¸ªé«˜è€¦åˆå¯¹çš„å®Œæ•´åˆ†æ*
        """
        
        return report
    
    def run_complete_phase1_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ç¬¬ä¸€é˜¶æ®µåˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„ç¬¬ä¸€é˜¶æ®µåˆ†æ - åŸºäºåŸå§‹é«˜è€¦åˆå¯¹æ•°æ®")
        print("=" * 60)
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        all_pieces, all_high_coupling_pairs, piece_id_to_info = self.load_original_data()
        
        # 2. åŒç­”æ¡ˆä¸€è‡´æ€§åˆ†æ
        same_answer_analysis, same_answer_pairs, different_answer_pairs = \
            self.phase1_same_answer_consistency_analysis(all_high_coupling_pairs, piece_id_to_info)
        
        # 3. Hopå±‚çº§è€¦åˆåˆ†æ
        hop_level_analysis, intra_pairs, inter_pairs = \
            self.phase1_hop_level_coupling_analysis(all_high_coupling_pairs)
        
        # 4. ç­”æ¡ˆç±»å‹åˆ†æ
        answer_type_analysis, answer_type_pairs = \
            self.phase1_answer_type_classification(all_high_coupling_pairs, piece_id_to_info)
        
        # 5. ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'metadata': {
                'total_pieces': len(all_pieces),
                'total_high_coupling_pairs': len(all_high_coupling_pairs),
                'analysis_timestamp': pd.Timestamp.now().isoformat()
            },
            'same_answer_analysis': same_answer_analysis,
            'hop_level_analysis': hop_level_analysis,
            'answer_type_analysis': answer_type_analysis
        }
        
        with open(self.output_dir / "phase1_complete_analysis.json", 'w') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_phase1_comprehensive_report(
            same_answer_analysis, hop_level_analysis, answer_type_analysis,
            all_pieces, all_high_coupling_pairs
        )
        
        with open(self.output_dir / "phase1_comprehensive_report.md", 'w') as f:
            f.write(report)
        
        print(f"\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæ•´åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print(f"ğŸ“„ ç»¼åˆæŠ¥å‘Š: phase1_comprehensive_report.md")
        
        return detailed_results

def main():
    """ä¸»å‡½æ•°"""
    fixer = DataAlignmentFixer()
    results = fixer.run_complete_phase1_analysis()
    
    print("\nğŸ‰ æ•°æ®å¯¹é½ä¿®æ­£å®Œæˆï¼Œç¬¬ä¸€é˜¶æ®µåˆ†æç»“æœå¯é ï¼")
    print("ğŸš€ å‡†å¤‡è¿›å…¥ç¬¬äºŒé˜¶æ®µæ”»å‡»å®éªŒï¼")

if __name__ == "__main__":
    main() 