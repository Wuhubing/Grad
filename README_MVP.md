# ğŸ¤– Multi-Model Knowledge Coupling MVP System

**ä¸“é—¨ç”¨äºGPT-2å’ŒLLaMA-2çš„çŸ¥è¯†è€¦åˆåˆ†æå’Œæ¶Ÿæ¼ªæ•ˆåº”éªŒè¯ç³»ç»Ÿ**

éªŒè¯æ ¸å¿ƒå‡è®¾ï¼š**ã€ŒGradSimé«˜ â†” æ¶Ÿæ¼ªå¼ºã€**

## ğŸ¯ ç³»ç»Ÿæ¦‚è§ˆ

è¿™æ˜¯ä¸€ä¸ªæ”¯æŒå¤šç§æ¨¡å‹æ¶æ„çš„æœ€å°å¯è¡Œå®éªŒ(MVP)ç³»ç»Ÿï¼Œç”¨äºéªŒè¯æ¢¯åº¦ç›¸ä¼¼åº¦(GradSim)æ˜¯å¦èƒ½é¢„æµ‹çŸ¥è¯†ç¼–è¾‘ä¸­çš„æ¶Ÿæ¼ªæ•ˆåº”ã€‚

### ğŸš€ æ”¯æŒçš„æ¨¡å‹
- **GPT-2 ç³»åˆ—**: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **LLaMA-2 ç³»åˆ—**: `meta-llama/Llama-2-7b-hf`, `meta-llama/Llama-2-13b-hf`, `meta-llama/Llama-2-70b-hf`
- **è‡ªåŠ¨æ£€æµ‹**: æ”¯æŒä»»ä½•HuggingFaceå…¼å®¹çš„Causal LMæ¨¡å‹

### æ ¸å¿ƒç ”ç©¶é—®é¢˜
1. **çŸ¥è¯†ç‰‡æ®µé—´çš„è€¦åˆåº¦**ï¼šé€šè¿‡æ¢¯åº¦ç›¸ä¼¼åº¦é‡åŒ–çŸ¥è¯†ç‰‡æ®µåœ¨å‚æ•°ç©ºé—´çš„é‡å ç¨‹åº¦
2. **æ¶Ÿæ¼ªæ•ˆåº”é¢„æµ‹**ï¼šé«˜è€¦åˆåº¦æ˜¯å¦é¢„ç¤ºç€æ›´å¼ºçš„ç¼–è¾‘æ¶Ÿæ¼ªæ•ˆåº”
3. **è·¨æ¨¡å‹éªŒè¯**ï¼šä¸åŒæ¨¡å‹æ¶æ„ä¸‹çš„è€¦åˆ-æ¶Ÿæ¼ªå…³ç³»æ˜¯å¦ä¸€è‡´

### æŠ€æœ¯å®ç°
- **å¤šæ¨¡å‹æ”¯æŒ**ï¼šè‡ªåŠ¨é€‚é…GPT-2å’ŒLLaMA-2çš„ä¸åŒå±‚ç»“æ„
- **æ•°æ®**ï¼šHotpotQA 2-hopæ¨ç†é“¾
- **æ–¹æ³•**ï¼šæ¢¯åº¦è®¡ç®— + MEMITç¼–è¾‘ + æ¶Ÿæ¼ªæ•ˆåº”æµ‹é‡
- **å…¬å¼**ï¼š`GradSim(i,j) = cos(âˆ‡_Î¸ log P(a_i|q_i), âˆ‡_Î¸ log P(a_j|q_j))`

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚
```bash
pip install torch transformers datasets scikit-learn matplotlib seaborn pandas tqdm numpy
```

### æ•°æ®å‡†å¤‡
ä¸‹è½½HotpotQAæ•°æ®ï¼š
```bash
# ä¸‹è½½HotpotQAå¼€å‘é›†
wget https://hotpotqa.github.io/data/hotpot_dev_distractor_v1.json
```

### ğŸ§ª æµ‹è¯•å¤šæ¨¡å‹æ”¯æŒ
```bash
# æµ‹è¯•æ¨¡å‹å…¼å®¹æ€§
python test_multi_model.py
```

### è¿è¡Œåˆ†æ

#### GPT-2 ç‰ˆæœ¬ï¼ˆæ¨èå…¥é—¨ï¼‰
```bash
# GPT-2 Small (CPUå‹å¥½)
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path gpt2 \
    --max_samples 20 \
    --max_experiments 2 \
    --device cpu

# GPT-2 Medium (æ›´å¥½æ•ˆæœ)
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path gpt2-medium \
    --max_samples 50 \
    --max_experiments 3 \
    --device cuda
```

#### LLaMA-2 ç‰ˆæœ¬ï¼ˆéœ€è¦GPUï¼‰
```bash
# LLaMA-2 7B
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path meta-llama/Llama-2-7b-hf \
    --max_samples 50 \
    --max_experiments 3 \
    --device cuda

# LLaMA-2 13B (å¤§å†…å­˜GPU)
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path meta-llama/Llama-2-13b-hf \
    --max_samples 30 \
    --max_experiments 2 \
    --device cuda
```

## ğŸ“‹ ç³»ç»Ÿæ¶æ„

### ğŸ”— é˜¶æ®µ1ï¼šå¤šæ¨¡å‹çŸ¥è¯†è€¦åˆåˆ†æ (`knowledge_coupling_mvp.py`)
**ç›®æ ‡**ï¼šåœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸Šè®¡ç®—çŸ¥è¯†ç‰‡æ®µé—´çš„çœŸå®è€¦åˆåº¦

**å¤šæ¨¡å‹é€‚é…**ï¼š
- **è‡ªåŠ¨æ£€æµ‹**ï¼šæ ¹æ®æ¨¡å‹è·¯å¾„å’Œç»“æ„è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹
- **å±‚é€‰æ‹©**ï¼šè‡ªé€‚åº”é€‰æ‹©ç›®æ ‡å±‚
  - GPT-2: `transformer.h.{i}.mlp.c_proj`
  - LLaMA-2: `model.layers.{i}.mlp.down_proj`
- **ç»Ÿä¸€æ¥å£**ï¼šç›¸åŒçš„APIæ”¯æŒæ‰€æœ‰æ¨¡å‹

**æ ¸å¿ƒç±»**ï¼š
- `MultiModelKnowledgeCouplingMVP`ï¼šå¤šæ¨¡å‹ä¸»åˆ†æå™¨
- `KnowledgePiece`ï¼šçŸ¥è¯†ç‰‡æ®µè¡¨ç¤º

### ğŸŒŠ é˜¶æ®µ2ï¼šå¤šæ¨¡å‹MEMITæ¶Ÿæ¼ªå®éªŒ (`memit_ripple_analyzer.py`)
**ç›®æ ‡**ï¼šåœ¨ä¸åŒæ¨¡å‹ä¸Šæµ‹é‡çŸ¥è¯†ç¼–è¾‘çš„æ¶Ÿæ¼ªæ•ˆåº”

**å¤šæ¨¡å‹MEMIT**ï¼š
- **å±‚é€‚é…**ï¼šæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç¼–è¾‘å±‚
- **æƒé‡æ¢å¤**ï¼šå®‰å…¨çš„å¤šæ¨¡å‹æƒé‡æ“ä½œ
- **ç»Ÿä¸€è¯„ä¼°**ï¼šæ ‡å‡†åŒ–çš„Î”logPå’ŒEMæµ‹é‡

**æ ¸å¿ƒç±»**ï¼š
- `MultiModelMEMIT`ï¼šå¤šæ¨¡å‹MEMITå®ç°
- `MultiModelRippleAnalyzer`ï¼šæ¶Ÿæ¼ªæ•ˆåº”åˆ†æå™¨

### ğŸ“Š é˜¶æ®µ3ï¼šè·¨æ¨¡å‹ç›¸å…³æ€§éªŒè¯ (`run_complete_mvp.py`)
**ç›®æ ‡**ï¼šéªŒè¯GradSimä¸æ¶Ÿæ¼ªæ•ˆåº”åœ¨ä¸åŒæ¨¡å‹ä¸Šçš„ä¸€è‡´æ€§

**æµç¨‹**ï¼š
1. **æ¨¡å‹æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«å’Œé€‚é…æ¨¡å‹æ¶æ„
2. **ç»Ÿä¸€åˆ†æ**ï¼šç›¸åŒçš„åˆ†ææµç¨‹é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹
3. **ç»“æœå¯¹æ¯”**ï¼šæ”¯æŒè·¨æ¨¡å‹ç»“æœæ¯”è¾ƒ

## ğŸ”§ æ¨¡å‹ç‰¹å®šé…ç½®

### GPT-2 ç³»åˆ—é…ç½®
```python
# GPT-2 ç›®æ ‡å±‚
'gpt2': {
    'layer_pattern': 'transformer.h.{}.mlp.c_proj',
    'embed_tokens': 'transformer.wte',
    'num_layers': 12  # gpt2-xl: 48å±‚
}
```

### LLaMA-2 ç³»åˆ—é…ç½®
```python
# LLaMA-2 ç›®æ ‡å±‚
'llama': {
    'layer_pattern': 'model.layers.{}.mlp.down_proj',
    'embed_tokens': 'model.embed_tokens',
    'num_layers': 32  # 7B: 32å±‚, 13B: 40å±‚, 70B: 80å±‚
}
```

### è‡ªå®šä¹‰æ¨¡å‹æ”¯æŒ
```python
# æ·»åŠ æ–°æ¨¡å‹æ”¯æŒ
SUPPORTED_MODELS = {
    'your_model': {
        'patterns': ['your_model_name'],
        'tokenizer_class': YourTokenizer,
        'model_class': YourModel,
        'target_layers': 'your_target_layer',
        'layer_pattern': 'your.layer.pattern.{}'
    }
}
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

æ‰€æœ‰æ¨¡å‹ç”Ÿæˆç›¸åŒçš„è¾“å‡ºæ ¼å¼ï¼Œä¾¿äºæ¯”è¾ƒï¼š

### æ ¸å¿ƒæ•°æ®æ–‡ä»¶
- `knowledge_pieces.json` - æå–çš„çŸ¥è¯†ç‰‡æ®µæ•°æ®
- `coupling_matrix.npy` - çŸ¥è¯†è€¦åˆçŸ©é˜µï¼ˆNÃ—Nï¼‰
- `coupling_analysis.json` - è€¦åˆåˆ†æç»“æœï¼ˆåŒ…å«æ¨¡å‹ç±»å‹ï¼‰
- `ripple_experiments.json` - MEMITå®éªŒå’Œæ¶Ÿæ¼ªæ•ˆåº”æ•°æ®
- `correlation_analysis.json` - ç›¸å…³æ€§åˆ†æç»“æœ

### å¯è§†åŒ–æ–‡ä»¶
- `coupling_heatmap.png` - çŸ¥è¯†è€¦åˆçƒ­å›¾
- `comprehensive_analysis.png` - å…­é¢æ¿ç»¼åˆåˆ†æå›¾
- `final_mvp_report.md` - å®Œæ•´åˆ†ææŠ¥å‘Šï¼ˆåŒ…å«æ¨¡å‹ä¿¡æ¯ï¼‰

## ğŸ” è·¨æ¨¡å‹ç»“æœå¯¹æ¯”

### å‡è®¾éªŒè¯æ ‡å‡†ï¼ˆä¸€è‡´æ€§ï¼‰
ä¸åŒæ¨¡å‹åº”è¯¥æ˜¾ç¤ºç›¸ä¼¼çš„æ¨¡å¼ï¼š
- **GPT-2**: éªŒè¯åœ¨Transformeræ¶æ„ä¸Šçš„GradSim-æ¶Ÿæ¼ªå…³ç³»
- **LLaMA-2**: éªŒè¯åœ¨ç°ä»£å¤§æ¨¡å‹æ¶æ„ä¸Šçš„ä¸€è‡´æ€§
- **è·¨æ¨¡å‹**: æ¯”è¾ƒä¸åŒæ¶æ„çš„è€¦åˆæ¨¡å¼å·®å¼‚

### å…³é”®å¯¹æ¯”æŒ‡æ ‡
1. **è€¦åˆåˆ†å¸ƒ**: ä¸åŒæ¨¡å‹çš„è€¦åˆå¼ºåº¦åˆ†å¸ƒ
2. **æ¶Ÿæ¼ªæ¨¡å¼**: æ¶Ÿæ¼ªæ•ˆåº”åœ¨ä¸åŒæ¶æ„ä¸Šçš„è¡¨ç°
3. **ç›¸å…³æ€§ä¸€è‡´æ€§**: GradSim-æ¶Ÿæ¼ªå…³ç³»çš„è·¨æ¨¡å‹ç¨³å®šæ€§

## ğŸš§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**1. æ¨¡å‹æ£€æµ‹å¤±è´¥**
```bash
# æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹
python -c "from knowledge_coupling_mvp import MultiModelKnowledgeCouplingMVP; print('Supported patterns:', MultiModelKnowledgeCouplingMVP.SUPPORTED_MODELS)"
```

**2. å†…å­˜ä¸è¶³ï¼ˆå¤§æ¨¡å‹ï¼‰**
```bash
# ä½¿ç”¨æ›´å°çš„æ ·æœ¬æ•°
--max_samples 10 --max_experiments 1

# ä½¿ç”¨CPUï¼ˆæ…¢ä½†ç¨³å®šï¼‰
--device cpu
```

**3. å±‚åç§°ä¸åŒ¹é…**
```python
# æ£€æŸ¥æ¨¡å‹å±‚ç»“æ„
for name, param in model.named_parameters():
    if 'mlp' in name.lower():
        print(name)
```

**4. LLaMA-2 è®¿é—®æƒé™**
```bash
# éœ€è¦HuggingFace token
pip install huggingface_hub
huggingface-cli login
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### è®¡ç®—éœ€æ±‚

| æ¨¡å‹ | å†…å­˜éœ€æ±‚ | æ¨èè®¾å¤‡ | å¤„ç†é€Ÿåº¦ | å‡†ç¡®æ€§ |
|------|----------|----------|----------|--------|
| GPT-2 Small | ~2GB | CPU/GPU | å¿« | åŸºç¡€ |
| GPT-2 Medium | ~4GB | GPU | ä¸­ç­‰ | è‰¯å¥½ |
| GPT-2 Large | ~6GB | GPU | ä¸­ç­‰ | å¾ˆå¥½ |
| LLaMA-2 7B | ~14GB | GPU | æ…¢ | ä¼˜ç§€ |
| LLaMA-2 13B | ~26GB | é«˜ç«¯GPU | å¾ˆæ…¢ | å‡ºè‰² |

### å»ºè®®é…ç½®
- **å…¥é—¨å­¦ä¹ **: GPT-2 Small (CPU)
- **ç ”ç©¶éªŒè¯**: GPT-2 Medium/Large (GPU)
- **è®ºæ–‡å®éªŒ**: LLaMA-2 7B (GPU)
- **å®Œæ•´å¯¹æ¯”**: å¤šæ¨¡å‹ç»„åˆå®éªŒ

## ğŸ¯ å®é™…ä½¿ç”¨ç¤ºä¾‹

### å¿«é€ŸéªŒè¯ï¼ˆGPT-2ï¼‰
```bash
# 5åˆ†é’Ÿå¿«é€Ÿæµ‹è¯•
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path gpt2 \
    --max_samples 10 \
    --max_experiments 1 \
    --output_dir quick_test
```

### å®Œæ•´å®éªŒï¼ˆLLaMA-2ï¼‰
```bash
# å®Œæ•´ç ”ç©¶å®éªŒ
python run_complete_mvp.py \
    --hotpot_data hotpot_dev_distractor_v1.json \
    --model_path meta-llama/Llama-2-7b-hf \
    --max_samples 100 \
    --max_experiments 5 \
    --output_dir full_experiment
```

### è·¨æ¨¡å‹å¯¹æ¯”
```bash
# è¿è¡Œå¤šä¸ªæ¨¡å‹å¹¶æ¯”è¾ƒç»“æœ
for model in gpt2 gpt2-medium meta-llama/Llama-2-7b-hf; do
    python run_complete_mvp.py \
        --hotpot_data hotpot_dev_distractor_v1.json \
        --model_path $model \
        --output_dir results_$model \
        --max_samples 50
done
```

## ğŸ”¬ ç ”ç©¶æ‰©å±•

### æ–°å¢æ¨¡å‹æ”¯æŒ
1. åœ¨`SUPPORTED_MODELS`ä¸­æ·»åŠ é…ç½®
2. å®ç°æ¨¡å‹ç‰¹å®šçš„å±‚é€‰æ‹©é€»è¾‘
3. æµ‹è¯•æ¢¯åº¦è®¡ç®—å’ŒMEMITç¼–è¾‘

### æ–°å¢æ•°æ®é›†
1. å®ç°æ•°æ®é›†ç‰¹å®šçš„çŸ¥è¯†æå–å™¨
2. é€‚é…clozeé—®é¢˜ç”Ÿæˆé€»è¾‘
3. éªŒè¯è·¨æ•°æ®é›†ä¸€è‡´æ€§

## ğŸ“„ å¼•ç”¨

```bibtex
@software{multi_model_knowledge_coupling_mvp,
  title={Multi-Model Knowledge Coupling MVP: Cross-Architecture Gradient Similarity Analysis},
  author={Your Name},
  year={2024},
  note={Supports GPT-2 and LLaMA-2 architectures},
  url={https://github.com/your_repo}
}
```

---

**ğŸŒŸ æ–°ç‰¹æ€§**: ç°åœ¨æ”¯æŒGPT-2å’ŒLLaMA-2ï¼é€‰æ‹©æœ€é€‚åˆä½ ç ”ç©¶éœ€æ±‚çš„æ¨¡å‹æ¶æ„ã€‚ 