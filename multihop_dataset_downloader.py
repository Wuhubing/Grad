#!/usr/bin/env python3
"""
Multi-hop Reasoning Dataset Downloader and Converter

æ”¯æŒä¸‹è½½å’Œè½¬æ¢å¤šä¸ªå¤šè·³æ¨ç†æ•°æ®é›†ï¼š
- HotpotQA
- Musique
- WikiHop
- Bamboogle

ç»Ÿä¸€è½¬æ¢ä¸ºçŸ¥è¯†è€¦åˆåˆ†æç³»ç»Ÿéœ€è¦çš„æ ¼å¼
"""

import json
import os
import requests
import zipfile
import tarfile
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
from pathlib import Path
import pandas as pd
from tqdm import tqdm
import random

class MultihopDatasetDownloader:
    """å¤šè·³æ¨ç†æ•°æ®é›†ä¸‹è½½å™¨å’Œè½¬æ¢å™¨"""
    
    # æ•°æ®é›†é…ç½®
    DATASETS = {
        'hotpotqa': {
            'name': 'HotpotQA',
            'urls': {
                'train': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json',
                'dev': 'http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_dev_distractor_v1.json'
            },
            'format': 'json',
            'description': 'Multi-hop reasoning over Wikipedia'
        },
        'musique': {
            'name': 'MuSiQue',
            'urls': {
                'main': 'https://drive.google.com/file/d/1tGdADlNjWFaHLeZZGShh2IRcpO6Lv24h/view?usp=sharing'
            },
            'format': 'gdrive_zip',
            'description': 'Multi-hop questions with single supporting facts'
        },
        'wikihop': {
            'name': 'WikiHop',
            'urls': {
                'huggingface': 'huggingface://wiki_hop'
            },
            'format': 'huggingface_trust',
            'description': 'Reading comprehension with multiple documents'
        },
        'bamboogle': {
            'name': 'Bamboogle',
            'urls': {
                'google_sheets': 'https://docs.google.com/spreadsheets/d/1jwcsA5kE4TObr9YHn9Gc-wQHYjTbLhDGx6tmIzMhl_U/export?format=csv'
            },
            'format': 'csv',
            'description': 'Multi-hop QA with synthetic data'
        }
    }
    
    def __init__(self, data_dir: str = "datasets"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ—‚ï¸  Multi-hop Dataset Downloader initialized")
        print(f"   Data directory: {self.data_dir.absolute()}")
        print(f"   Supported datasets: {', '.join(self.DATASETS.keys())}")
    
    def download_dataset(self, dataset_name: str, force_download: bool = False) -> bool:
        """ä¸‹è½½æŒ‡å®šæ•°æ®é›†"""
        if dataset_name not in self.DATASETS:
            print(f"âŒ Unknown dataset: {dataset_name}")
            print(f"   Available: {', '.join(self.DATASETS.keys())}")
            return False
        
        dataset_config = self.DATASETS[dataset_name]
        dataset_dir = self.data_dir / dataset_name
        dataset_dir.mkdir(exist_ok=True)
        
        print(f"\nğŸ“¥ Downloading {dataset_config['name']}...")
        print(f"   Description: {dataset_config['description']}")
        
        success = True
        for split, url in dataset_config['urls'].items():
            # å¤„ç†HuggingFaceæ•°æ®é›†
            if dataset_config['format'] == 'huggingface':
                try:
                    print(f"   ğŸ”„ Loading {split} from HuggingFace")
                    # å®‰è£…datasetsåº“
                    import subprocess
                    subprocess.run(['pip', 'install', 'datasets'], check=True, capture_output=True)
                    
                    from datasets import load_dataset
                    
                    # æå–æ•°æ®é›†åç§°
                    dataset_name_hf = url.replace('huggingface://', '')
                    
                    # åŠ è½½æ•°æ®é›†
                    dataset = load_dataset(dataset_name_hf)
                    
                    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
                    for split_name, split_data in dataset.items():
                        output_file = dataset_dir / f"{dataset_name_hf}_{split_name}.json"
                        split_data.to_json(output_file)
                        print(f"   âœ… Saved {split_name}: {output_file}")
                    
                except Exception as e:
                    print(f"   âŒ Failed to load from HuggingFace: {e}")
                    success = False
                continue
                
            # å¤„ç†HuggingFaceæ•°æ®é›†ï¼ˆéœ€è¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼‰
            if dataset_config['format'] == 'huggingface_trust':
                try:
                    print(f"   ğŸ”„ Loading {split} from HuggingFace (trust_remote_code=True)")
                    # å®‰è£…datasetsåº“
                    import subprocess
                    subprocess.run(['pip', 'install', 'datasets'], check=True, capture_output=True)
                    
                    from datasets import load_dataset
                    
                    # æå–æ•°æ®é›†åç§°
                    dataset_name_hf = url.replace('huggingface://', '')
                    
                    # åŠ è½½æ•°æ®é›†ï¼Œä¿¡ä»»è¿œç¨‹ä»£ç 
                    dataset = load_dataset(dataset_name_hf, trust_remote_code=True)
                    
                    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
                    for split_name, split_data in dataset.items():
                        output_file = dataset_dir / f"{dataset_name_hf}_{split_name}.json"
                        split_data.to_json(output_file)
                        print(f"   âœ… Saved {split_name}: {output_file}")
                    
                except Exception as e:
                    print(f"   âŒ Failed to load from HuggingFace: {e}")
                    success = False
                continue
                
            # å¤„ç†Google Driveé“¾æ¥
            if dataset_config['format'] == 'gdrive_zip':
                # å¯¹äºGoogle Driveï¼Œä½¿ç”¨gdownä¸‹è½½
                try:
                    import subprocess
                    print(f"   ğŸ”„ Downloading {split} via Google Drive")
                    
                    # æå–Google Driveæ–‡ä»¶ID
                    if 'drive.google.com' in url:
                        if '/file/d/' in url:
                            file_id = url.split('/file/d/')[1].split('/')[0]
                        else:
                            print(f"   âŒ Cannot extract file ID from: {url}")
                            success = False
                            continue
                    
                    # è®¾ç½®è¾“å‡ºæ–‡ä»¶å
                    output_filename = f"musique_v1.0.zip"
                    output_path = dataset_dir / output_filename
                    
                    if output_path.exists() and not force_download:
                        print(f"   âœ… {split} already exists: {output_path}")
                    else:
                        # ç¡®ä¿å®‰è£…gdown
                        subprocess.run(['pip', 'install', 'gdown'], check=True, capture_output=True)
                        
                        # ä½¿ç”¨gdownä¸‹è½½
                        cmd = ['gdown', '--id', file_id, '--output', str(output_path)]
                        result = subprocess.run(cmd, capture_output=True, text=True)
                        
                        if result.returncode != 0:
                            print(f"   âŒ gdown failed: {result.stderr}")
                            success = False
                            continue
                        
                        print(f"   âœ… Downloaded {split}: {output_path}")
                    
                    # è§£å‹ZIPæ–‡ä»¶
                    if output_path.exists():
                        self._extract_zip(output_path, dataset_dir)
                        # æ¸…ç†ä¸‹è½½çš„zipæ–‡ä»¶
                        output_path.unlink()
                    
                except Exception as e:
                    print(f"   âŒ Failed to download {split} via Google Drive: {e}")
                    success = False
                continue
                
            # åŸæœ‰çš„ä¸‹è½½é€»è¾‘
            filename = self._get_filename_from_url(url)
            file_path = dataset_dir / filename
            
            if file_path.exists() and not force_download:
                print(f"   âœ… {split} already exists: {file_path}")
                continue
            
            try:
                print(f"   ğŸ”„ Downloading {split} from {url}")
                response = requests.get(url, stream=True)
                response.raise_for_status()
                
                total_size = int(response.headers.get('content-length', 0))
                
                with open(file_path, 'wb') as f:
                    with tqdm(total=total_size, unit='B', unit_scale=True, desc=f"   {split}") as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
                
                # è§£å‹ç¼©æ–‡ä»¶
                if dataset_config['format'] == 'zip':
                    self._extract_zip(file_path, dataset_dir)
                elif dataset_config['format'] == 'tar.gz':
                    self._extract_tar(file_path, dataset_dir)
                
                print(f"   âœ… Downloaded {split}: {file_path}")
                
            except Exception as e:
                print(f"   âŒ Failed to download {split}: {e}")
                success = False
        
        return success
    
    def _get_filename_from_url(self, url: str) -> str:
        """ä»URLæå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        if not filename:
            filename = url.split('/')[-1]
        return filename or "data.json"
    
    def _extract_zip(self, zip_path: Path, extract_dir: Path):
        """è§£å‹ZIPæ–‡ä»¶"""
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        print(f"   ğŸ“¦ Extracted: {zip_path}")
    
    def _extract_tar(self, tar_path: Path, extract_dir: Path):
        """è§£å‹TARæ–‡ä»¶"""
        with tarfile.open(tar_path, 'r:*') as tar_ref:
            tar_ref.extractall(extract_dir)
        print(f"   ğŸ“¦ Extracted: {tar_path}")
    
    def convert_hotpotqa(self, file_path: Path, max_samples: int = None) -> List[Dict]:
        """è½¬æ¢HotpotQAæ ¼å¼"""
        print(f"ğŸ”„ Converting HotpotQA: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if max_samples:
            data = data[:max_samples]
        
        converted = []
        for i, item in enumerate(data):
            # HotpotQAå·²ç»æ˜¯æˆ‘ä»¬éœ€è¦çš„æ ¼å¼
            converted_item = {
                '_id': item.get('_id', f'hotpotqa_{i}'),
                'question': item['question'],
                'answer': item['answer'],
                'supporting_facts': item.get('supporting_facts', []),
                'context': item.get('context', []),
                'dataset': 'hotpotqa',
                'level': item.get('level', 'hard'),
                'type': item.get('type', 'bridge')
            }
            converted.append(converted_item)
        
        print(f"   âœ… Converted {len(converted)} HotpotQA samples")
        return converted
    
    def convert_musique(self, file_path: Path, max_samples: int = None) -> List[Dict]:
        """è½¬æ¢MuSiQueæ ¼å¼"""
        print(f"ğŸ”„ Converting MuSiQue: {file_path}")
        
        # MuSiQueæ•°æ®é›†å¯èƒ½æ˜¯JSONæˆ–JSONLæ ¼å¼
        converted = []
        
        try:
            # å°è¯•ä½œä¸ºJSONæ–‡ä»¶è¯»å–
            with open(file_path, 'r', encoding='utf-8') as f:
                if file_path.suffix == '.jsonl':
                    # JSONLæ ¼å¼
                    for i, line in enumerate(f):
                        if max_samples and i >= max_samples:
                            break
                        
                        try:
                            item = json.loads(line.strip())
                            converted_item = self._convert_musique_item(item, i)
                            if converted_item:
                                converted.append(converted_item)
                        except Exception as e:
                            print(f"   âš ï¸  Error processing line {i}: {e}")
                            continue
                else:
                    # JSONæ ¼å¼
                    data = json.load(f)
                    if isinstance(data, list):
                        # æ•°æ®æ˜¯åˆ—è¡¨
                        for i, item in enumerate(data):
                            if max_samples and i >= max_samples:
                                break
                            converted_item = self._convert_musique_item(item, i)
                            if converted_item:
                                converted.append(converted_item)
                    else:
                        # æ•°æ®å¯èƒ½æœ‰å…¶ä»–ç»“æ„
                        print(f"   âš ï¸  Unknown JSON structure in {file_path}")
                        return []
                    
        except Exception as e:
            print(f"   âŒ Error reading {file_path}: {e}")
            return []
        
        print(f"   âœ… Converted {len(converted)} MuSiQue samples")
        return converted
    
    def _convert_musique_item(self, item: Dict, index: int) -> Dict:
        """è½¬æ¢å•ä¸ªMuSiQueæ•°æ®é¡¹"""
        try:
            # æå–supporting facts
            supporting_facts = []
            if 'supporting_facts' in item:
                for fact in item['supporting_facts']:
                    if isinstance(fact, dict):
                        supporting_facts.append([fact.get('title', ''), fact.get('sent_id', 0)])
                    elif isinstance(fact, list) and len(fact) >= 2:
                        supporting_facts.append([fact[0], fact[1]])
            
            # æ„å»ºcontext
            context = []
            if 'paragraphs' in item:
                for para in item['paragraphs']:
                    title = para.get('title', f'para_{len(context)}')
                    if 'paragraph_text' in para:
                        sentences = para['paragraph_text'].split('. ')
                    else:
                        sentences = para.get('sentences', [])
                    context.append([title, sentences])
            elif 'context' in item:
                # å¦‚æœcontextå·²ç»æ˜¯æˆ‘ä»¬éœ€è¦çš„æ ¼å¼
                context = item['context']
            
            converted_item = {
                '_id': item.get('id', item.get('_id', f'musique_{index}')),
                'question': item.get('question', ''),
                'answer': item.get('answer', ''),
                'supporting_facts': supporting_facts,
                'context': context,
                'dataset': 'musique',
                'question_type': item.get('question_type', 'multihop'),
                'answerable': item.get('answerable', True)
            }
            return converted_item
            
        except Exception as e:
            print(f"   âš ï¸  Error converting item {index}: {e}")
            return None
    
    def convert_wikihop(self, file_path: Path, max_samples: int = None) -> List[Dict]:
        """è½¬æ¢WikiHopæ ¼å¼"""
        print(f"ğŸ”„ Converting WikiHop: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if max_samples:
            data = data[:max_samples]
        
        converted = []
        for i, item in enumerate(data):
            try:
                # WikiHopæ ¼å¼è½¬æ¢
                supports = item.get('supports', [])
                supporting_facts = []
                
                # åˆ›å»ºsupporting facts
                for j, support_id in enumerate(supports[:2]):  # åªå–å‰2ä¸ª
                    supporting_facts.append([f'support_{j}', 0])
                
                # æ„å»ºcontext
                context = []
                candidates = item.get('candidates', [])
                documents = item.get('supports', [])
                
                # æ·»åŠ å€™é€‰ç­”æ¡ˆä½œä¸ºcontext
                if candidates:
                    context.append(['candidates', candidates[:5]])  # åªå–å‰5ä¸ªå€™é€‰
                
                # æ·»åŠ æ”¯æŒæ–‡æ¡£
                for j, doc in enumerate(documents[:3]):  # åªå–å‰3ä¸ªæ–‡æ¡£
                    sentences = doc.split('. ')[:3]  # æ¯ä¸ªæ–‡æ¡£åªå–å‰3å¥
                    context.append([f'document_{j}', sentences])
                
                converted_item = {
                    '_id': item.get('id', f'wikihop_{i}'),
                    'question': item.get('query', ''),
                    'answer': item.get('answer', ''),
                    'supporting_facts': supporting_facts,
                    'context': context,
                    'dataset': 'wikihop',
                    'candidates': candidates,
                    'query_type': 'bridge'
                }
                converted.append(converted_item)
                
            except Exception as e:
                print(f"   âš ï¸  Error processing item {i}: {e}")
                continue
        
        print(f"   âœ… Converted {len(converted)} WikiHop samples")
        return converted
    
    def convert_bamboogle(self, file_path: Path, max_samples: int = None) -> List[Dict]:
        """è½¬æ¢Bamboogleæ ¼å¼"""
        print(f"ğŸ”„ Converting Bamboogle: {file_path}")
        
        # Bamboogleå¯èƒ½æ˜¯CSVæˆ–JSONæ ¼å¼
        converted = []
        
        try:
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
                if max_samples:
                    df = df.head(max_samples)
                
                for i, row in df.iterrows():
                    # åŸºäºCSVåˆ—æ„å»ºæ•°æ®
                    converted_item = {
                        '_id': f'bamboogle_{i}',
                        'question': row.get('question', ''),
                        'answer': row.get('answer', ''),
                        'supporting_facts': [['context', 0]],  # ç®€åŒ–çš„supporting facts
                        'context': [['context', [row.get('context', '')]]],
                        'dataset': 'bamboogle',
                        'difficulty': row.get('difficulty', 'medium')
                    }
                    converted.append(converted_item)
            else:
                # JSONæ ¼å¼
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if max_samples:
                    data = data[:max_samples]
                
                for i, item in enumerate(data):
                    converted_item = {
                        '_id': item.get('id', f'bamboogle_{i}'),
                        'question': item.get('question', ''),
                        'answer': item.get('answer', ''),
                        'supporting_facts': item.get('supporting_facts', [['context', 0]]),
                        'context': item.get('context', [['context', ['No context available']]]),
                        'dataset': 'bamboogle',
                        'type': item.get('type', 'synthetic')
                    }
                    converted.append(converted_item)
                    
        except Exception as e:
            print(f"   âŒ Error converting Bamboogle: {e}")
            return []
        
        print(f"   âœ… Converted {len(converted)} Bamboogle samples")
        return converted
    
    def convert_dataset(self, dataset_name: str, split: str = 'train', 
                       max_samples: int = None) -> List[Dict]:
        """è½¬æ¢æŒ‡å®šæ•°æ®é›†çš„æŒ‡å®šåˆ†å‰²"""
        dataset_dir = self.data_dir / dataset_name
        
        if dataset_name == 'hotpotqa':
            if split == 'train':
                file_path = dataset_dir / "hotpot_train_v1.1.json"
            else:  # dev
                file_path = dataset_dir / "hotpot_dev_distractor_v1.json"
            
            if file_path.exists():
                return self.convert_hotpotqa(file_path, max_samples)
        
        elif dataset_name == 'musique':
            # MuSiQueæ–‡ä»¶åœ¨dataå­ç›®å½•ä¸­
            data_dir = dataset_dir / "data"
            if split == 'train':
                file_path = data_dir / "musique_ans_v1.0_train.jsonl"
            elif split == 'dev':
                file_path = data_dir / "musique_ans_v1.0_dev.jsonl"
            else:  # test
                file_path = data_dir / "musique_ans_v1.0_test.jsonl"
            
            if file_path.exists():
                return self.convert_musique(file_path, max_samples)
        
        elif dataset_name == 'wikihop':
            # WikiHopä»HuggingFaceä¸‹è½½åçš„æ–‡ä»¶
            train_file = dataset_dir / "wiki_hop_train.json"
            dev_file = dataset_dir / "wiki_hop_validation.json"
            
            if split == 'train' and train_file.exists():
                return self.convert_wikihop(train_file, max_samples)
            elif split == 'dev' and dev_file.exists():
                return self.convert_wikihop(dev_file, max_samples)
            else:
                # å¤‡ç”¨ï¼šæŸ¥æ‰¾ä»»ä½•JSONæ–‡ä»¶
                possible_files = list(dataset_dir.glob("**/*.json"))
                if possible_files:
                    return self.convert_wikihop(possible_files[0], max_samples)
        
        elif dataset_name == 'bamboogle':
            # Bamboogleä»Google Sheetsä¸‹è½½çš„CSVæ–‡ä»¶
            bamboogle_file = dataset_dir / "bamboogle.csv"
            if bamboogle_file.exists():
                return self.convert_bamboogle(bamboogle_file, max_samples)
            else:
                # å¤‡ç”¨ï¼šæŸ¥æ‰¾ä»»ä½•CSVæ–‡ä»¶
                possible_files = list(dataset_dir.glob("**/*.csv"))
                if possible_files:
                    return self.convert_bamboogle(possible_files[0], max_samples)
        
        print(f"âŒ No data file found for {dataset_name} ({split})")
        return []
    
    def create_mixed_dataset(self, datasets: List[str], samples_per_dataset: int = 50,
                           output_file: str = "mixed_multihop_dataset.json") -> List[Dict]:
        """åˆ›å»ºæ··åˆæ•°æ®é›†"""
        print(f"\nğŸ”€ Creating mixed dataset from: {', '.join(datasets)}")
        
        all_data = []
        
        for dataset_name in datasets:
            print(f"\nğŸ“š Processing {dataset_name}...")
            converted_data = self.convert_dataset(dataset_name, max_samples=samples_per_dataset)
            all_data.extend(converted_data)
        
        # éšæœºæ‰“ä¹±
        random.shuffle(all_data)
        
        # ä¿å­˜æ··åˆæ•°æ®é›†
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Mixed dataset created!")
        print(f"   Total samples: {len(all_data)}")
        print(f"   Saved to: {output_file}")
        
        # ç»Ÿè®¡å„æ•°æ®é›†çš„æ ·æœ¬æ•°
        dataset_counts = {}
        for item in all_data:
            dataset = item.get('dataset', 'unknown')
            dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
        
        print(f"   Dataset distribution:")
        for dataset, count in dataset_counts.items():
            print(f"     {dataset}: {count} samples")
        
        return all_data
    
    def download_all_datasets(self, force_download: bool = False):
        """ä¸‹è½½æ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†"""
        print(f"\nğŸš€ Downloading all multihop datasets...")
        
        for dataset_name in self.DATASETS:
            print(f"\n" + "="*50)
            success = self.download_dataset(dataset_name, force_download)
            if success:
                print(f"âœ… {dataset_name} downloaded successfully")
            else:
                print(f"âŒ {dataset_name} download failed")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("ğŸš€ Multi-hop Dataset Downloader and Converter")
    print("=" * 60)
    
    downloader = MultihopDatasetDownloader()
    
    # ä¸‹è½½æ‰€æœ‰å››ä¸ªæ•°æ®é›†
    all_datasets = ['hotpotqa', 'musique', 'wikihop', 'bamboogle']
    
    print(f"\nğŸ“¥ Step 1: Downloading all datasets ({', '.join(all_datasets)})...")
    for dataset in all_datasets:
        print(f"\n{'='*30}")
        success = downloader.download_dataset(dataset)
        if success:
            print(f"âœ… {dataset} downloaded successfully")
        else:
            print(f"âŒ {dataset} download failed - continuing with next dataset")
    
    print(f"\nğŸ”„ Step 2: Creating mixed dataset from all available datasets...")
    # åªä½¿ç”¨æˆåŠŸä¸‹è½½çš„æ•°æ®é›†
    available_datasets = []
    for dataset in all_datasets:
        dataset_dir = downloader.data_dir / dataset
        if dataset_dir.exists():
            available_datasets.append(dataset)
    
    if available_datasets:
        mixed_data = downloader.create_mixed_dataset(
            datasets=available_datasets,
            samples_per_dataset=25,  # æ¯ä¸ªæ•°æ®é›†25ä¸ªæ ·æœ¬ï¼Œ4ä¸ªæ•°æ®é›†æ€»å…±100ä¸ªæ ·æœ¬
            output_file="mixed_multihop_100.json"
        )
        
        print(f"\nğŸ¯ Step 3: Dataset ready for knowledge coupling analysis!")
        print(f"   Use: python knowledge_coupling_mvp.py --hotpot_data mixed_multihop_100.json")
    else:
        print(f"\nâŒ No datasets available for mixed dataset creation")
    
    print(f"\nğŸ“Š Final download summary:")
    for dataset in all_datasets:
        dataset_dir = downloader.data_dir / dataset
        if dataset_dir.exists():
            print(f"   âœ… {dataset}: Downloaded")
        else:
            print(f"   âŒ {dataset}: Failed or not available")

if __name__ == "__main__":
    main() 