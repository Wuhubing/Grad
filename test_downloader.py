#!/usr/bin/env python3
"""
æµ‹è¯•å¤šæ•°æ®é›†ä¸‹è½½å™¨çš„åŠŸèƒ½
"""

from multihop_dataset_downloader import MultihopDatasetDownloader
import json

def test_downloader():
    """æµ‹è¯•ä¸‹è½½å™¨åŠŸèƒ½"""
    print("ğŸ§ª Testing Multi-hop Dataset Downloader")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¸‹è½½å™¨
    downloader = MultihopDatasetDownloader(data_dir="test_datasets")
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ•°æ®é›†
    print(f"\nğŸ“‹ Supported datasets:")
    for name, config in downloader.DATASETS.items():
        print(f"   {name}: {config['description']}")
    
    # æµ‹è¯•ï¼šå…ˆä¸ä¸‹è½½ï¼Œåªè½¬æ¢ç°æœ‰çš„testæ•°æ®
    print(f"\nğŸ”„ Testing format conversion with existing test data...")
    
    # å¦‚æœæœ‰ç°æœ‰çš„test_hotpot_20.jsonï¼Œè½¬æ¢å®ƒ
    try:
        with open("test_hotpot_20.json", 'r') as f:
            hotpot_data = json.load(f)
        
        print(f"âœ… Found test_hotpot_20.json with {len(hotpot_data)} samples")
        
        # åˆ›å»ºä¸€ä¸ªå°çš„æ··åˆæ•°æ®é›†
        mixed_data = []
        
        # è½¬æ¢å‰5ä¸ªHotpotQAæ ·æœ¬
        for i, item in enumerate(hotpot_data[:5]):
            converted_item = {
                '_id': item.get('_id', f'test_hotpot_{i}'),
                'question': item['question'],
                'answer': item['answer'],
                'supporting_facts': item.get('supporting_facts', []),
                'context': item.get('context', []),
                'dataset': 'hotpotqa_test',
                'source': 'existing_test_data'
            }
            mixed_data.append(converted_item)
        
        # ä¿å­˜æµ‹è¯•æ•°æ®é›†
        output_file = "test_mixed_dataset.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(mixed_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Created test mixed dataset: {output_file}")
        print(f"   Samples: {len(mixed_data)}")
        
        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡
        dataset_counts = {}
        for item in mixed_data:
            dataset = item.get('dataset', 'unknown')
            dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
        
        print(f"   Dataset distribution:")
        for dataset, count in dataset_counts.items():
            print(f"     {dataset}: {count} samples")
        
        # æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
        print(f"\nğŸ“š Sample data:")
        sample = mixed_data[0]
        print(f"   ID: {sample['_id']}")
        print(f"   Question: {sample['question'][:80]}...")
        print(f"   Answer: {sample['answer']}")
        print(f"   Dataset: {sample['dataset']}")
        
        return True
        
    except FileNotFoundError:
        print(f"âŒ test_hotpot_20.json not found")
        print(f"   To test with real downloads, run:")
        print(f"   python multihop_dataset_downloader.py")
        return False

def demo_download_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„ä¸‹è½½å·¥ä½œæµç¨‹ï¼ˆæ³¨é‡Šæ‰å®é™…ä¸‹è½½ï¼‰"""
    print(f"\nğŸ¯ Demo: Complete Download Workflow")
    print("=" * 40)
    
    print(f"ğŸ“ Workflow steps:")
    print(f"   1. Initialize downloader")
    print(f"   2. Download datasets (HotpotQA, MuSiQue)")
    print(f"   3. Convert to unified format")
    print(f"   4. Create mixed dataset")
    print(f"   5. Ready for knowledge coupling analysis")
    
    print(f"\nğŸ’¡ To run actual downloads:")
    print(f"   python multihop_dataset_downloader.py")
    
    print(f"\nğŸ”§ To use with knowledge coupling analysis:")
    print(f"   python knowledge_coupling_mvp.py --hotpot_data mixed_multihop_30.json")

if __name__ == "__main__":
    # å…ˆæµ‹è¯•åŸºæœ¬åŠŸèƒ½
    success = test_downloader()
    
    # ç„¶åæ¼”ç¤ºå·¥ä½œæµç¨‹
    demo_download_workflow()
    
    if success:
        print(f"\nâœ… Downloader test completed successfully!")
        print(f"ğŸ¯ Ready to test with knowledge coupling analysis:")
        print(f"   python knowledge_coupling_mvp.py --hotpot_data test_mixed_dataset.json --max_samples 5")
    else:
        print(f"\nâš ï¸  Downloader test completed with warnings")
        print(f"   Some features require downloading actual datasets") 