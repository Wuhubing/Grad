#!/usr/bin/env python3
"""
çŸ¥è¯†è€¦åˆä¸€è‡´æ€§ç°è±¡åˆ†æè„šæœ¬
ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®åˆ†ç±»å’Œä¸€è‡´æ€§ç°è±¡åˆ†æ

ä¸»è¦åŠŸèƒ½ï¼š
1. åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯
2. ä¸åŒhopå±‚çº§çš„è€¦åˆæ¨¡å¼åˆ†æ
3. ç­”æ¡ˆç±»å‹çš„åˆ†ç±»ç»Ÿè®¡
"""

import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import re

class CouplingConsistencyAnalyzer:
    """çŸ¥è¯†è€¦åˆä¸€è‡´æ€§åˆ†æå™¨"""
    
    def __init__(self, results_dir: str = "results/full_hotpotqa_analysis"):
        self.results_dir = Path(results_dir)
        self.final_results_dir = self.results_dir / "final_merged_results"
        
        # å­˜å‚¨åˆ†ææ•°æ®
        self.all_knowledge_pieces = []
        self.all_high_coupling_pairs = []
        self.global_stats = {}
        
        print(f"ğŸ” çŸ¥è¯†è€¦åˆä¸€è‡´æ€§åˆ†æå™¨åˆå§‹åŒ–")
        print(f"   ç»“æœç›®å½•: {self.results_dir}")
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ•°æ®"""
        print("\nğŸ“š åŠ è½½åˆ†ææ•°æ®...")
        
        # 1. åŠ è½½å…¨å±€ç»Ÿè®¡ä¿¡æ¯
        global_stats_file = self.final_results_dir / "global_analysis_results.json"
        with open(global_stats_file, 'r', encoding='utf-8') as f:
            global_results = json.load(f)
            self.global_stats = global_results['global_statistics']
        
        print(f"âœ… å…¨å±€ç»Ÿè®¡ä¿¡æ¯åŠ è½½å®Œæˆ")
        print(f"   æ€»è€¦åˆå¯¹: {self.global_stats['total_coupling_pairs']:,}")
        print(f"   é«˜è€¦åˆå¯¹: {self.global_stats['total_high_coupling_pairs']:,}")
        
        # 2. åŠ è½½æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µä¿¡æ¯
        knowledge_pieces_file = self.final_results_dir / "all_knowledge_pieces.json"
        with open(knowledge_pieces_file, 'r', encoding='utf-8') as f:
            self.all_knowledge_pieces = json.load(f)
        
        print(f"âœ… çŸ¥è¯†ç‰‡æ®µæ•°æ®åŠ è½½å®Œæˆ: {len(self.all_knowledge_pieces):,} ä¸ªç‰‡æ®µ")
        
        # 3. åŠ è½½æ‰€æœ‰é«˜è€¦åˆå¯¹ï¼ˆä»å„ä¸ªæ‰¹æ¬¡åˆå¹¶ï¼‰
        print("ğŸ“Š åŠ è½½é«˜è€¦åˆå¯¹æ•°æ®...")
        self._load_all_high_coupling_pairs()
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ!")
        
    def _load_all_high_coupling_pairs(self):
        """ä»æ‰€æœ‰æ‰¹æ¬¡åŠ è½½é«˜è€¦åˆå¯¹æ•°æ®"""
        all_pairs = []
        
        # éå†æ‰€æœ‰æ‰¹æ¬¡ç›®å½•
        batch_dirs = sorted([d for d in self.results_dir.iterdir() 
                           if d.is_dir() and d.name.startswith('batch_')])
        
        for batch_dir in batch_dirs:
            high_coupling_file = batch_dir / "high_coupling_pairs.json"
            if high_coupling_file.exists():
                with open(high_coupling_file, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    all_pairs.extend(batch_data['pairs'])
        
        self.all_high_coupling_pairs = all_pairs
        print(f"   ä» {len(batch_dirs)} ä¸ªæ‰¹æ¬¡åŠ è½½äº† {len(all_pairs):,} ä¸ªé«˜è€¦åˆå¯¹")
    
    def analyze_same_answer_consistency(self) -> Dict[str, Any]:
        """1. åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯"""
        print("\nğŸ¯ åˆ†æåŒç­”æ¡ˆä¸€è‡´æ€§...")
        
        same_answer_pairs = []
        different_answer_pairs = []
        
        for pair in self.all_high_coupling_pairs:
            if pair['piece_1_answer'] == pair['piece_2_answer']:
                same_answer_pairs.append(pair)
            else:
                different_answer_pairs.append(pair)
        
        # ç»Ÿè®¡è€¦åˆå¼ºåº¦
        same_answer_strengths = [pair['coupling_strength'] for pair in same_answer_pairs]
        different_answer_strengths = [pair['coupling_strength'] for pair in different_answer_pairs]
        
        results = {
            'same_answer_count': len(same_answer_pairs),
            'different_answer_count': len(different_answer_pairs),
            'same_answer_ratio': len(same_answer_pairs) / len(self.all_high_coupling_pairs),
            'same_answer_mean_coupling': np.mean(same_answer_strengths) if same_answer_strengths else 0,
            'different_answer_mean_coupling': np.mean(different_answer_strengths) if different_answer_strengths else 0,
            'same_answer_std_coupling': np.std(same_answer_strengths) if same_answer_strengths else 0,
            'different_answer_std_coupling': np.std(different_answer_strengths) if different_answer_strengths else 0,
            'same_answer_max_coupling': max(same_answer_strengths) if same_answer_strengths else 0,
            'different_answer_max_coupling': max(different_answer_strengths) if different_answer_strengths else 0
        }
        
        print(f"ğŸ“Š åŒç­”æ¡ˆä¸€è‡´æ€§åˆ†æç»“æœ:")
        print(f"   ç›¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹: {results['same_answer_count']:,} ({results['same_answer_ratio']:.1%})")
        print(f"   ä¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹: {results['different_answer_count']:,}")
        print(f"   ç›¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: {results['same_answer_mean_coupling']:.4f}")
        print(f"   ä¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: {results['different_answer_mean_coupling']:.4f}")
        
        # æ˜¾è‘—æ€§å·®å¼‚
        coupling_diff = results['same_answer_mean_coupling'] - results['different_answer_mean_coupling']
        print(f"   ğŸ’¡ è€¦åˆå¼ºåº¦å·®å¼‚: {coupling_diff:.4f} ({'ç›¸åŒç­”æ¡ˆæ›´é«˜' if coupling_diff > 0 else 'ä¸åŒç­”æ¡ˆæ›´é«˜'})")
        
        return results
    
    def analyze_hop_level_coupling(self) -> Dict[str, Any]:
        """2. ä¸åŒhopå±‚çº§çš„è€¦åˆæ¨¡å¼åˆ†æ"""
        print("\nğŸ”— åˆ†æä¸åŒhopå±‚çº§çš„è€¦åˆæ¨¡å¼...")
        
        intra_hotpot_pairs = []  # åŒä¸€HotpotQAæ ·æœ¬å†…çš„è€¦åˆ
        inter_hotpot_pairs = []  # ä¸åŒHotpotQAæ ·æœ¬é—´çš„è€¦åˆ
        
        for pair in self.all_high_coupling_pairs:
            if pair['is_same_hotpot']:
                intra_hotpot_pairs.append(pair)
            else:
                inter_hotpot_pairs.append(pair)
        
        # ç»Ÿè®¡è€¦åˆå¼ºåº¦
        intra_strengths = [pair['coupling_strength'] for pair in intra_hotpot_pairs]
        inter_strengths = [pair['coupling_strength'] for pair in inter_hotpot_pairs]
        
        results = {
            'intra_hotpot_count': len(intra_hotpot_pairs),
            'inter_hotpot_count': len(inter_hotpot_pairs),
            'intra_hotpot_ratio': len(intra_hotpot_pairs) / len(self.all_high_coupling_pairs),
            'intra_mean_coupling': np.mean(intra_strengths) if intra_strengths else 0,
            'inter_mean_coupling': np.mean(inter_strengths) if inter_strengths else 0,
            'intra_std_coupling': np.std(intra_strengths) if intra_strengths else 0,
            'inter_std_coupling': np.std(inter_strengths) if inter_strengths else 0,
            'intra_max_coupling': max(intra_strengths) if intra_strengths else 0,
            'inter_max_coupling': max(inter_strengths) if inter_strengths else 0
        }
        
        print(f"ğŸ“Š Hopå±‚çº§è€¦åˆåˆ†æç»“æœ:")
        print(f"   Intra-HotpotQAè€¦åˆå¯¹: {results['intra_hotpot_count']:,} ({results['intra_hotpot_ratio']:.1%})")
        print(f"   Inter-HotpotQAè€¦åˆå¯¹: {results['inter_hotpot_count']:,}")
        print(f"   Intraå¹³å‡è€¦åˆå¼ºåº¦: {results['intra_mean_coupling']:.4f}")
        print(f"   Interå¹³å‡è€¦åˆå¼ºåº¦: {results['inter_mean_coupling']:.4f}")
        
        # åˆ†æå·®å¼‚
        coupling_diff = results['intra_mean_coupling'] - results['inter_mean_coupling']
        print(f"   ğŸ’¡ Intra vs Interå·®å¼‚: {coupling_diff:.4f}")
        
        return results
    
    def analyze_answer_types(self) -> Dict[str, Any]:
        """3. ç­”æ¡ˆç±»å‹çš„åˆ†ç±»ç»Ÿè®¡"""
        print("\nğŸ“‹ åˆ†æç­”æ¡ˆç±»å‹åˆ†å¸ƒ...")
        
        # å®šä¹‰ç­”æ¡ˆç±»å‹åˆ†ç±»å‡½æ•°
        def classify_answer_type(answer: str) -> str:
            answer = answer.strip().lower()
            
            # Yes/Noç±»å‹ - æœ€é«˜ä¼˜å…ˆçº§
            if answer in ['yes', 'no']:
                return 'Yes/No'
            
            # å¹´ä»½ - åœ¨æ•°å­—ä¹‹å‰æ£€æŸ¥ï¼Œé¿å…è¢«æ•°å­—ç±»å‹è¦†ç›–
            if re.match(r'^\d{4}$', answer):
                return 'Year'
            
            # æ•°å­—ç±»å‹
            if re.match(r'^[\d,]+$', answer.replace(' ', '')):
                return 'Number'
            
            # å¸¸è§äººåæ¨¡å¼
            if any(word in answer for word in ['president', 'director', 'actor', 'writer', 'author']):
                return 'Person_Title'
            
            # åœ°åæ¨¡å¼
            if any(word in answer for word in ['city', 'state', 'country', 'america', 'england', 'california']):
                return 'Location'
            
            # å•è¯æ•°é‡åˆ¤æ–­
            word_count = len(answer.split())
            if word_count == 1:
                return 'Single_Word'
            elif word_count == 2:
                return 'Two_Words'
            elif word_count <= 5:
                return 'Short_Phrase'
            else:
                return 'Long_Phrase'
        
        # ç»Ÿè®¡æ‰€æœ‰é«˜è€¦åˆå¯¹ä¸­çš„ç­”æ¡ˆç±»å‹
        answer_types = []
        answer_type_pairs = defaultdict(list)
        
        for pair in self.all_high_coupling_pairs:
            # åˆ†ç±»ä¸¤ä¸ªç­”æ¡ˆ
            type1 = classify_answer_type(pair['piece_1_answer'])
            type2 = classify_answer_type(pair['piece_2_answer'])
            
            answer_types.extend([type1, type2])
            
            # è®°å½•ç­”æ¡ˆç±»å‹ç»„åˆ
            if type1 == type2:  # ç›¸åŒç±»å‹
                answer_type_pairs[f"{type1}_vs_{type2}"].append(pair)
            else:  # ä¸åŒç±»å‹
                combo_key = f"{min(type1, type2)}_vs_{max(type1, type2)}"
                answer_type_pairs[combo_key].append(pair)
        
        # ç»Ÿè®¡ç»“æœ
        type_counts = Counter(answer_types)
        total_answers = len(answer_types)
        
        # æœ€å¸¸è§çš„ç­”æ¡ˆç±»å‹
        most_common_types = type_counts.most_common(10)
        
        # åŒç±»å‹vsè·¨ç±»å‹è€¦åˆåˆ†æ
        same_type_pairs = []
        cross_type_pairs = []
        
        for pair in self.all_high_coupling_pairs:
            type1 = classify_answer_type(pair['piece_1_answer'])
            type2 = classify_answer_type(pair['piece_2_answer'])
            
            if type1 == type2:
                same_type_pairs.append(pair)
            else:
                cross_type_pairs.append(pair)
        
        same_type_strengths = [pair['coupling_strength'] for pair in same_type_pairs]
        cross_type_strengths = [pair['coupling_strength'] for pair in cross_type_pairs]
        
        results = {
            'total_answer_instances': total_answers,
            'unique_answer_types': len(type_counts),
            'most_common_types': most_common_types,
            'type_distribution': dict(type_counts),
            'same_type_pairs_count': len(same_type_pairs),
            'cross_type_pairs_count': len(cross_type_pairs),
            'same_type_ratio': len(same_type_pairs) / len(self.all_high_coupling_pairs),
            'same_type_mean_coupling': np.mean(same_type_strengths) if same_type_strengths else 0,
            'cross_type_mean_coupling': np.mean(cross_type_strengths) if cross_type_strengths else 0,
            'answer_type_pairs': dict(answer_type_pairs)
        }
        
        print(f"ğŸ“Š ç­”æ¡ˆç±»å‹åˆ†æç»“æœ:")
        print(f"   æ€»ç­”æ¡ˆå®ä¾‹: {total_answers:,}")
        print(f"   è¯†åˆ«çš„ç­”æ¡ˆç±»å‹: {len(type_counts)} ç§")
        print(f"   æœ€å¸¸è§çš„ç­”æ¡ˆç±»å‹:")
        for answer_type, count in most_common_types:
            percentage = count / total_answers * 100
            print(f"     {answer_type}: {count:,} ({percentage:.1f}%)")
        
        print(f"\n   åŒç±»å‹è€¦åˆå¯¹: {len(same_type_pairs):,} ({results['same_type_ratio']:.1%})")
        print(f"   è·¨ç±»å‹è€¦åˆå¯¹: {len(cross_type_pairs):,}")
        print(f"   åŒç±»å‹å¹³å‡è€¦åˆå¼ºåº¦: {results['same_type_mean_coupling']:.4f}")
        print(f"   è·¨ç±»å‹å¹³å‡è€¦åˆå¼ºåº¦: {results['cross_type_mean_coupling']:.4f}")
        
        return results
    
    def generate_consistency_report(self, same_answer_results: Dict, hop_level_results: Dict, 
                                  answer_type_results: Dict, output_dir: str = "results/consistency_analysis"):
        """ç”Ÿæˆä¸€è‡´æ€§åˆ†ææŠ¥å‘Š"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“ ç”Ÿæˆä¸€è‡´æ€§åˆ†ææŠ¥å‘Š...")
        
        # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
        all_results = {
            'global_statistics': self.global_stats,
            'same_answer_consistency': same_answer_results,
            'hop_level_coupling': hop_level_results,
            'answer_type_analysis': answer_type_results,
            'analysis_timestamp': pd.Timestamp.now().isoformat()
        }
        
        results_file = output_path / "consistency_analysis_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_file = output_path / "consistency_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# çŸ¥è¯†è€¦åˆä¸€è‡´æ€§ç°è±¡åˆ†ææŠ¥å‘Š\n\n")
            
            f.write("## ğŸ¯ ç ”ç©¶ç›®æ ‡\n")
            f.write("éªŒè¯çŸ¥è¯†ç‰‡æ®µè€¦åˆåº¦çš„ä¸€è‡´æ€§ç°è±¡ï¼Œä¸ºåé—¨æ”»å‡»çš„æ³›åŒ–æä¾›ç†è®ºåŸºç¡€ã€‚\n\n")
            
            f.write("## ğŸ“Š æ•°æ®æ¦‚è§ˆ\n")
            f.write(f"- **æ€»çŸ¥è¯†ç‰‡æ®µ**: {len(self.all_knowledge_pieces):,}\n")
            f.write(f"- **æ€»è€¦åˆå¯¹**: {self.global_stats['total_coupling_pairs']:,}\n")
            f.write(f"- **é«˜è€¦åˆå¯¹**: {self.global_stats['total_high_coupling_pairs']:,}\n")
            f.write(f"- **é«˜è€¦åˆæ¯”ä¾‹**: {self.global_stats['high_coupling_ratio']:.2%}\n\n")
            
            f.write("## ğŸ” å…³é”®å‘ç°\n\n")
            
            f.write("### 1. åŒç­”æ¡ˆä¸€è‡´æ€§éªŒè¯\n")
            f.write(f"**æ ¸å¿ƒå‡è®¾**: ç›¸åŒç­”æ¡ˆçš„çŸ¥è¯†ç‰‡æ®µåº”è¯¥è¡¨ç°å‡ºæ›´é«˜çš„è€¦åˆåº¦\n\n")
            f.write(f"- ç›¸åŒç­”æ¡ˆçš„é«˜è€¦åˆå¯¹: **{same_answer_results['same_answer_count']:,}** ({same_answer_results['same_answer_ratio']:.1%})\n")
            f.write(f"- ç›¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: **{same_answer_results['same_answer_mean_coupling']:.4f}**\n")
            f.write(f"- ä¸åŒç­”æ¡ˆå¹³å‡è€¦åˆå¼ºåº¦: **{same_answer_results['different_answer_mean_coupling']:.4f}**\n")
            
            coupling_diff = same_answer_results['same_answer_mean_coupling'] - same_answer_results['different_answer_mean_coupling']
            f.write(f"- **è€¦åˆå¼ºåº¦å·®å¼‚**: {coupling_diff:.4f}\n")
            
            if coupling_diff > 0.05:
                f.write("âœ… **éªŒè¯æˆåŠŸ**: ç›¸åŒç­”æ¡ˆçš„çŸ¥è¯†ç‰‡æ®µç¡®å®è¡¨ç°å‡ºæ˜¾è‘—æ›´é«˜çš„è€¦åˆåº¦!\n\n")
            elif coupling_diff > 0:
                f.write("âš ï¸ **éƒ¨åˆ†éªŒè¯**: ç›¸åŒç­”æ¡ˆçš„è€¦åˆåº¦ç•¥é«˜ï¼Œä½†å·®å¼‚ä¸å¤Ÿæ˜¾è‘—\n\n")
            else:
                f.write("âŒ **å‡è®¾ä¸æˆç«‹**: ç›¸åŒç­”æ¡ˆçš„è€¦åˆåº¦å¹¶ä¸æ›´é«˜\n\n")
            
            f.write("### 2. Hopå±‚çº§è€¦åˆæ¨¡å¼\n")
            f.write(f"**å‘ç°**: åŒä¸€HotpotQAæ¨ç†é“¾å†…çš„çŸ¥è¯†ç‰‡æ®µè¡¨ç°å‡ºæé«˜è€¦åˆåº¦\n\n")
            f.write(f"- Intra-HotpotQAè€¦åˆå¯¹: **{hop_level_results['intra_hotpot_count']:,}** ({hop_level_results['intra_hotpot_ratio']:.1%})\n")
            f.write(f"- Intraå¹³å‡è€¦åˆå¼ºåº¦: **{hop_level_results['intra_mean_coupling']:.4f}**\n")
            f.write(f"- Interå¹³å‡è€¦åˆå¼ºåº¦: **{hop_level_results['inter_mean_coupling']:.4f}**\n")
            
            hop_diff = hop_level_results['intra_mean_coupling'] - hop_level_results['inter_mean_coupling']
            f.write(f"- **å±‚çº§å·®å¼‚**: {hop_diff:.4f}\n\n")
            
            f.write("ğŸ’¡ **æ”»å‡»ç­–ç•¥å¯ç¤º**: åŒä¸€æ¨ç†é“¾å†…çš„çŸ¥è¯†ç‰‡æ®µé«˜åº¦è€¦åˆï¼Œæ”»å‡»ä¸€ä¸ªhopå¯èƒ½ç›´æ¥å½±å“å¦ä¸€ä¸ªhopã€‚\n\n")
            
            f.write("### 3. ç­”æ¡ˆç±»å‹åˆ†å¸ƒåˆ†æ\n")
            f.write(f"**ç›®æ ‡**: è¯†åˆ«æœ€é€‚åˆåé—¨æ”»å‡»çš„çŸ¥è¯†ç±»å‹\n\n")
            f.write("**æœ€å¸¸è§çš„ç­”æ¡ˆç±»å‹**:\n")
            for answer_type, count in answer_type_results['most_common_types']:
                percentage = count / answer_type_results['total_answer_instances'] * 100
                f.write(f"- {answer_type}: {count:,} ({percentage:.1f}%)\n")
            
            f.write(f"\n**åŒç±»å‹ vs è·¨ç±»å‹è€¦åˆ**:\n")
            f.write(f"- åŒç±»å‹è€¦åˆå¯¹: {answer_type_results['same_type_pairs_count']:,} ({answer_type_results['same_type_ratio']:.1%})\n")
            f.write(f"- åŒç±»å‹å¹³å‡è€¦åˆå¼ºåº¦: **{answer_type_results['same_type_mean_coupling']:.4f}**\n")
            f.write(f"- è·¨ç±»å‹å¹³å‡è€¦åˆå¼ºåº¦: **{answer_type_results['cross_type_mean_coupling']:.4f}**\n\n")
            
            f.write("## ğŸ¯ åé—¨æ”»å‡»ç­–ç•¥å»ºè®®\n\n")
            
            # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
            if same_answer_results['same_answer_ratio'] > 0.5:
                f.write("1. **ä¼˜å…ˆæ”»å‡»ç›¸åŒç­”æ¡ˆçš„çŸ¥è¯†é›†ç¾¤** - é«˜æ¦‚ç‡è§¦å‘æ¶Ÿæ¼ªæ•ˆåº”\n")
            
            if hop_level_results['intra_hotpot_ratio'] > 0.3:
                f.write("2. **åˆ©ç”¨æ¨ç†é“¾å†…è€¦åˆ** - æ”»å‡»ä¸€ä¸ªhopæ¥å½±å“æ•´ä¸ªæ¨ç†é“¾\n")
            
            # æ¨èæ”»å‡»çš„ç­”æ¡ˆç±»å‹
            top_answer_type = answer_type_results['most_common_types'][0][0]
            f.write(f"3. **é‡ç‚¹æ”»å‡» {top_answer_type} ç±»å‹** - å‡ºç°é¢‘ç‡æœ€é«˜ï¼Œå½±å“é¢æœ€å¹¿\n")
            
            f.write("\n## ğŸ“ˆ ä¸‹ä¸€æ­¥éªŒè¯è®¡åˆ’\n")
            f.write("1. é€‰æ‹©é«˜è€¦åˆçš„ç›¸åŒç­”æ¡ˆçŸ¥è¯†ç‰‡æ®µå¯¹è¿›è¡ŒMEMITæ”»å‡»å®éªŒ\n")
            f.write("2. æµ‹é‡å®é™…çš„æ¶Ÿæ¼ªæ•ˆåº”ä¼ æ’­èŒƒå›´\n")
            f.write("3. éªŒè¯GradSimé¢„æµ‹çš„å‡†ç¡®æ€§\n")
        
        print(f"âœ… ä¸€è‡´æ€§åˆ†ææŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   è¯¦ç»†ç»“æœ: {results_file}")
        print(f"   åˆ†ææŠ¥å‘Š: {report_file}")
        
        return results_file, report_file
    
    def run_full_consistency_analysis(self, output_dir: str = "results/consistency_analysis"):
        """è¿è¡Œå®Œæ•´çš„ä¸€è‡´æ€§åˆ†æ"""
        print("ğŸš€ å¼€å§‹çŸ¥è¯†è€¦åˆä¸€è‡´æ€§ç°è±¡åˆ†æ")
        print("=" * 60)
        
        # åŠ è½½æ•°æ®
        self.load_data()
        
        # æ‰§è¡Œä¸‰ä¸ªä¸»è¦åˆ†æ
        same_answer_results = self.analyze_same_answer_consistency()
        hop_level_results = self.analyze_hop_level_coupling()
        answer_type_results = self.analyze_answer_types()
        
        # ç”ŸæˆæŠ¥å‘Š
        results_file, report_file = self.generate_consistency_report(
            same_answer_results, hop_level_results, answer_type_results, output_dir
        )
        
        print(f"\nğŸ‰ ä¸€è‡´æ€§åˆ†æå®Œæˆ!")
        print(f"ğŸ“„ æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        return {
            'same_answer_results': same_answer_results,
            'hop_level_results': hop_level_results,
            'answer_type_results': answer_type_results,
            'report_files': {
                'results': results_file,
                'report': report_file
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    analyzer = CouplingConsistencyAnalyzer()
    results = analyzer.run_full_consistency_analysis()
    
    print("\nğŸ¯ å…³é”®å‘ç°æ€»ç»“:")
    print(f"âœ… ç›¸åŒç­”æ¡ˆè€¦åˆåº¦æ›´é«˜: {results['same_answer_results']['same_answer_mean_coupling']:.4f} vs {results['same_answer_results']['different_answer_mean_coupling']:.4f}")
    print(f"âœ… Intra-HotpotQAè€¦åˆæ›´å¼º: {results['hop_level_results']['intra_mean_coupling']:.4f} vs {results['hop_level_results']['inter_mean_coupling']:.4f}")
    print(f"âœ… æœ€å¸¸è§ç­”æ¡ˆç±»å‹: {results['answer_type_results']['most_common_types'][0][0]}")


if __name__ == "__main__":
    main() 