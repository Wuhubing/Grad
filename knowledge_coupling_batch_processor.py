#!/usr/bin/env python3
"""
çŸ¥è¯†è€¦åˆæ‰¹å¤„ç†å™¨ - å¤„ç†å…¨éƒ¨HotpotQAæ•°æ®
æ”¯æŒåˆ†æ‰¹å¤„ç†ã€å†…å­˜ç®¡ç†ã€checkpointä¿å­˜/æ¢å¤
ä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨å°å­æ‰¹æ¬¡ï¼ˆ10ä¸ªæ ·æœ¬ï¼‰é¿å…æ˜¾å­˜ä¸è¶³
"""

import json
import os
import gc
import psutil
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from tqdm import tqdm
import datetime
import time

# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒç±»
from knowledge_coupling_mvp import MultiModelKnowledgeCouplingMVP, load_hotpot_data

class KnowledgeCouplingBatchProcessor:
    """æ‰¹å¤„ç†çŸ¥è¯†è€¦åˆåˆ†æå™¨ - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, 
                 model_path: str = "meta-llama/Llama-2-7b-hf",
                 batch_size: int = 2000,
                 sub_batch_size: int = 10,  # æ–°å¢ï¼šå­æ‰¹æ¬¡å¤§å°
                 checkpoint_dir: str = "checkpoints",
                 output_dir: str = "results/full_hotpotqa_analysis",
                 layer_range: Optional[Tuple[int, int]] = None):
        
        self.model_path = model_path
        self.batch_size = batch_size
        self.sub_batch_size = sub_batch_size  # æ¯æ¬¡å¤„ç†çš„æ ·æœ¬æ•°
        self.checkpoint_dir = Path(checkpoint_dir)
        self.output_dir = Path(output_dir)
        self.layer_range = layer_range
        
        # åˆ›å»ºç›®å½•
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        
        # å†…å­˜å’ŒGPUç›‘æ§
        self.process = psutil.Process()
        self.initial_memory = self.get_memory_usage()
        
        print(f"ğŸš€ åˆå§‹åŒ–çŸ¥è¯†è€¦åˆæ‰¹å¤„ç†å™¨ (ä¼˜åŒ–ç‰ˆ)")
        print(f"æ¨¡å‹: {model_path}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"å­æ‰¹æ¬¡å¤§å°: {sub_batch_size} (å‡å°‘æ˜¾å­˜ä½¿ç”¨)")
        print(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {self.initial_memory:.2f} GB")
        
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡(GB)"""
        return self.process.memory_info().rss / 1e9
    
    def get_gpu_memory_usage(self) -> Tuple[float, float]:
        """è·å–GPUæ˜¾å­˜ä½¿ç”¨é‡(GB)"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            return allocated, reserved
        return 0.0, 0.0
    
    def print_memory_status(self, prefix: str = ""):
        """æ‰“å°å†…å­˜çŠ¶æ€"""
        cpu_mem = self.get_memory_usage()
        gpu_alloc, gpu_reserved = self.get_gpu_memory_usage()
        
        print(f"{prefix}å†…å­˜çŠ¶æ€:")
        print(f"  CPUå†…å­˜: {cpu_mem:.2f} GB")
        if torch.cuda.is_available():
            print(f"  GPUåˆ†é…: {gpu_alloc:.2f} GB")
            print(f"  GPUä¿ç•™: {gpu_reserved:.2f} GB")
    
    def cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def save_checkpoint(self, batch_idx: int, batch_results: Dict[str, Any], 
                       accumulated_data: Dict[str, Any]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_file = self.checkpoint_dir / f"checkpoint_batch_{batch_idx:04d}.json"
        
        checkpoint_data = {
            'batch_idx': batch_idx,
            'timestamp': datetime.datetime.now().isoformat(),
            'batch_results': batch_results,
            'accumulated_summary': {
                'total_batches_processed': batch_idx + 1,
                'total_samples_processed': accumulated_data['total_samples'],
                'total_knowledge_pieces': accumulated_data['total_pieces'],
                'memory_usage_gb': self.get_memory_usage(),
                'gpu_memory_gb': self.get_gpu_memory_usage()
            }
        }
        
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
        return checkpoint_file
    
    def load_latest_checkpoint(self) -> Optional[Tuple[int, Dict[str, Any]]]:
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        checkpoint_files = list(self.checkpoint_dir.glob("checkpoint_batch_*.json"))
        
        if not checkpoint_files:
            return None
        
        # æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_file = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            checkpoint_data = json.load(f)
        
        batch_idx = checkpoint_data['batch_idx']
        accumulated_data = checkpoint_data.get('accumulated_summary', {})
        
        print(f"ğŸ“‚ åŠ è½½æ£€æŸ¥ç‚¹: {latest_file}")
        print(f"   ä¸Šæ¬¡å¤„ç†åˆ°æ‰¹æ¬¡: {batch_idx}")
        print(f"   å·²å¤„ç†æ ·æœ¬: {accumulated_data.get('total_samples_processed', 0)}")
        
        return batch_idx, accumulated_data
    
    def process_single_sub_batch(self, sub_batch_data: List[Dict], 
                                sub_batch_idx: int, batch_idx: int, 
                                analyzer: MultiModelKnowledgeCouplingMVP) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªå­æ‰¹æ¬¡"""
        print(f"  ğŸ“¦ å­æ‰¹æ¬¡ {sub_batch_idx + 1}: {len(sub_batch_data)} ä¸ªæ ·æœ¬")
        
        # æå–çŸ¥è¯†ç‰‡æ®µ
        knowledge_pieces = analyzer.extract_knowledge_pieces_from_hotpot(
            sub_batch_data, len(sub_batch_data)
        )
        
        if not knowledge_pieces:
            print(f"  âš ï¸ å­æ‰¹æ¬¡ {sub_batch_idx + 1} æœªæå–åˆ°çŸ¥è¯†ç‰‡æ®µ")
            return {
                'sub_batch_idx': sub_batch_idx,
                'samples_count': len(sub_batch_data),
                'knowledge_pieces_count': 0,
                'knowledge_pieces': [],
                'coupling_pairs': []
            }
        
        # è®¡ç®—æ¢¯åº¦
        gradients = analyzer.compute_all_gradients()
        
        if not gradients:
            print(f"  âš ï¸ å­æ‰¹æ¬¡ {sub_batch_idx + 1} æœªè®¡ç®—åˆ°æ¢¯åº¦")
            return {
                'sub_batch_idx': sub_batch_idx,
                'samples_count': len(sub_batch_data),
                'knowledge_pieces_count': len(knowledge_pieces),
                'knowledge_pieces': [],
                'coupling_pairs': []
            }
        
        # è®¡ç®—è€¦åˆçŸ©é˜µ
        coupling_matrix = analyzer.compute_coupling_matrix()
        
        # æå–è€¦åˆå¯¹æ•°æ®
        coupling_pairs = []
        piece_ids = [p.piece_id for p in knowledge_pieces]
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
        if isinstance(coupling_matrix, torch.Tensor):
            coupling_np = coupling_matrix.detach().cpu().numpy()
        else:
            coupling_np = coupling_matrix
        
        # æå–æ‰€æœ‰è€¦åˆå¯¹
        for i in range(len(piece_ids)):
            for j in range(i + 1, len(piece_ids)):
                coupling_strength = float(coupling_np[i, j])
                coupling_pairs.append({
                    'piece_1_id': piece_ids[i],
                    'piece_2_id': piece_ids[j],
                    'coupling_strength': coupling_strength,
                    'piece_1_answer': knowledge_pieces[i].answer,
                    'piece_2_answer': knowledge_pieces[j].answer,
                    'is_same_hotpot': piece_ids[i].split('_hop_')[0] == piece_ids[j].split('_hop_')[0]
                })
        
        # è½¬æ¢çŸ¥è¯†ç‰‡æ®µä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        knowledge_pieces_data = []
        for piece in knowledge_pieces:
            knowledge_pieces_data.append({
                'piece_id': piece.piece_id,
                'question': piece.question,
                'answer': piece.answer,
                'supporting_fact': piece.supporting_fact,
                'category': piece.category
            })
        
        # æ¸…ç†GPUå†…å­˜
        del gradients, coupling_matrix
        analyzer.gradients = {}
        analyzer.coupling_matrix = None
        analyzer.knowledge_pieces = []
        self.cleanup_memory()
        
        sub_batch_result = {
            'sub_batch_idx': sub_batch_idx,
            'samples_count': len(sub_batch_data),
            'knowledge_pieces_count': len(knowledge_pieces),
            'coupling_pairs_count': len(coupling_pairs),
            'high_coupling_pairs_count': sum(1 for p in coupling_pairs if p['coupling_strength'] >= 0.4),
            'knowledge_pieces': knowledge_pieces_data,
            'coupling_pairs': coupling_pairs
        }
        
        print(f"  âœ… å­æ‰¹æ¬¡ {sub_batch_idx + 1} å®Œæˆ: "
              f"{len(knowledge_pieces)} ç‰‡æ®µ, "
              f"{len(coupling_pairs)} è€¦åˆå¯¹")
        
        return sub_batch_result
    
    def process_single_batch(self, batch_data: List[Dict], batch_idx: int) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ - ä½¿ç”¨å­æ‰¹æ¬¡ç­–ç•¥"""
        print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}")
        print(f"   æ ·æœ¬æ•°é‡: {len(batch_data)}")
        print(f"   å­æ‰¹æ¬¡å¤§å°: {self.sub_batch_size}")
        
        self.print_memory_status("   å¼€å§‹å‰ ")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = self.output_dir / f"batch_{batch_idx:04d}"
        output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºåˆ†æå™¨ (æ•´ä¸ªæ‰¹æ¬¡å…±äº«ä¸€ä¸ªåˆ†æå™¨)
        analyzer = MultiModelKnowledgeCouplingMVP(
            model_path=self.model_path,
            layer_range=self.layer_range
        )
        
        try:
            # å°†æ‰¹æ¬¡æ•°æ®åˆ†å‰²ä¸ºå­æ‰¹æ¬¡
            sub_batches = []
            for i in range(0, len(batch_data), self.sub_batch_size):
                sub_batch = batch_data[i:i + self.sub_batch_size]
                sub_batches.append(sub_batch)
            
            print(f"   åˆ†å‰²ä¸º {len(sub_batches)} ä¸ªå­æ‰¹æ¬¡")
            
            # å¤„ç†æ¯ä¸ªå­æ‰¹æ¬¡
            all_knowledge_pieces = []
            all_coupling_pairs = []
            total_sub_batch_pairs = 0
            total_high_coupling = 0
            
            for sub_batch_idx, sub_batch_data in enumerate(sub_batches):
                sub_batch_result = self.process_single_sub_batch(
                    sub_batch_data, sub_batch_idx, batch_idx, analyzer
                )
                
                # ç´¯ç§¯ç»“æœ
                all_knowledge_pieces.extend(sub_batch_result['knowledge_pieces'])
                all_coupling_pairs.extend(sub_batch_result['coupling_pairs'])
                total_sub_batch_pairs += sub_batch_result['coupling_pairs_count']
                total_high_coupling += sub_batch_result['high_coupling_pairs_count']
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            # 1. ä¿å­˜çŸ¥è¯†ç‰‡æ®µ
            knowledge_pieces_file = output_dir / "knowledge_pieces.json"
            with open(knowledge_pieces_file, 'w', encoding='utf-8') as f:
                json.dump(all_knowledge_pieces, f, indent=2, ensure_ascii=False)
            
            # 2. ä¿å­˜è€¦åˆå¯¹ (CSVæ ¼å¼)
            coupling_csv_file = output_dir / "coupling_pairs.csv"
            if all_coupling_pairs:
                import pandas as pd
                coupling_df = pd.DataFrame(all_coupling_pairs)
                coupling_df.to_csv(coupling_csv_file, index=False)
            
            # 3. ä¿å­˜é«˜è€¦åˆå¯¹
            high_coupling_pairs = [p for p in all_coupling_pairs if p['coupling_strength'] >= 0.4]
            high_coupling_pairs.sort(key=lambda x: x['coupling_strength'], reverse=True)
            
            high_coupling_file = output_dir / "high_coupling_pairs.json"
            with open(high_coupling_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'threshold': 0.4,
                    'count': len(high_coupling_pairs),
                    'pairs': high_coupling_pairs
                }, f, indent=2, ensure_ascii=False)
            
            # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            coupling_strengths = [p['coupling_strength'] for p in all_coupling_pairs]
            
            batch_summary = {
                'batch_idx': batch_idx,
                'samples_count': len(batch_data),
                'sub_batches_count': len(sub_batches),
                'knowledge_pieces_count': len(all_knowledge_pieces),
                'coupling_pairs_count': len(all_coupling_pairs),
                'high_coupling_pairs_count': len(high_coupling_pairs),
                'mean_coupling': float(np.mean(coupling_strengths)) if coupling_strengths else 0.0,
                'std_coupling': float(np.std(coupling_strengths)) if coupling_strengths else 0.0,
                'min_coupling': float(np.min(coupling_strengths)) if coupling_strengths else 0.0,
                'max_coupling': float(np.max(coupling_strengths)) if coupling_strengths else 0.0,
                'processing_time': time.time(),
                'output_directory': str(output_dir)
            }
            
            # 5. ä¿å­˜æ‰¹æ¬¡å…ƒæ•°æ®
            metadata_file = output_dir / "batch_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'batch_summary': batch_summary,
                    'sub_batch_size': self.sub_batch_size,
                    'model_info': {
                        'model_path': self.model_path,
                        'layer_range': self.layer_range
                    },
                    'timestamp': datetime.datetime.now().isoformat()
                }, f, indent=2, ensure_ascii=False)
            
            self.print_memory_status("   å®Œæˆå ")
            
            print(f"   âœ… æ‰¹æ¬¡å®Œæˆ: {len(all_knowledge_pieces)} ç‰‡æ®µ, "
                  f"{len(all_coupling_pairs)} è€¦åˆå¯¹, "
                  f"{len(high_coupling_pairs)} é«˜è€¦åˆå¯¹")
            
            return batch_summary
            
        finally:
            # æ¸…ç†å†…å­˜
            del analyzer
            self.cleanup_memory()
    
    def merge_batch_results(self, batch_summaries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ"""
        print(f"\nğŸ“Š åˆå¹¶ {len(batch_summaries)} ä¸ªæ‰¹æ¬¡çš„ç»“æœ...")
        
        # åˆå¹¶æ‰€æœ‰è€¦åˆå¯¹æ•°æ®
        all_coupling_pairs = []
        all_knowledge_pieces = []
        
        total_samples = 0
        total_pieces = 0
        total_pairs = 0
        total_high_coupling = 0
        
        coupling_strengths = []
        
        for summary in tqdm(batch_summaries, desc="åˆå¹¶æ‰¹æ¬¡æ•°æ®"):
            batch_dir = Path(summary['output_directory'])
            
            # åŠ è½½è€¦åˆå¯¹æ•°æ®
            coupling_file = batch_dir / "coupling_pairs.csv"
            if coupling_file.exists():
                import pandas as pd
                batch_coupling = pd.read_csv(coupling_file)
                
                # é‡æ–°ç¼–å·piece_idä»¥é¿å…å†²çª
                batch_coupling['piece_1_id'] = batch_coupling['piece_1_id'].apply(
                    lambda x: f"batch_{summary['batch_idx']:04d}_{x}"
                )
                batch_coupling['piece_2_id'] = batch_coupling['piece_2_id'].apply(
                    lambda x: f"batch_{summary['batch_idx']:04d}_{x}"
                )
                
                all_coupling_pairs.append(batch_coupling)
                coupling_strengths.extend(batch_coupling['coupling_strength'].tolist())
            
            # åŠ è½½çŸ¥è¯†ç‰‡æ®µæ•°æ®
            pieces_file = batch_dir / "knowledge_pieces.json"
            if pieces_file.exists():
                with open(pieces_file, 'r', encoding='utf-8') as f:
                    batch_pieces = json.load(f)
                
                # é‡æ–°ç¼–å·piece_id
                for piece in batch_pieces:
                    piece['piece_id'] = f"batch_{summary['batch_idx']:04d}_{piece['piece_id']}"
                
                all_knowledge_pieces.extend(batch_pieces)
            
            # ç´¯è®¡ç»Ÿè®¡
            total_samples += summary['samples_count']
            total_pieces += summary['knowledge_pieces_count']
            total_pairs += summary['coupling_pairs_count']
            total_high_coupling += summary['high_coupling_pairs_count']
        
        # åˆå¹¶æ•°æ®æ¡†
        if all_coupling_pairs:
            import pandas as pd
            merged_coupling_df = pd.concat(all_coupling_pairs, ignore_index=True)
        else:
            merged_coupling_df = pd.DataFrame()
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡
        global_stats = {
            'total_batches': len(batch_summaries),
            'total_samples': total_samples,
            'total_knowledge_pieces': total_pieces,
            'total_coupling_pairs': total_pairs,
            'total_high_coupling_pairs': total_high_coupling,
            'global_mean_coupling': np.mean(coupling_strengths) if coupling_strengths else 0,
            'global_std_coupling': np.std(coupling_strengths) if coupling_strengths else 0,
            'global_min_coupling': np.min(coupling_strengths) if coupling_strengths else 0,
            'global_max_coupling': np.max(coupling_strengths) if coupling_strengths else 0,
            'high_coupling_ratio': total_high_coupling / total_pairs if total_pairs > 0 else 0
        }
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        final_output_dir = self.output_dir / "final_merged_results"
        final_output_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜åˆå¹¶çš„è€¦åˆå¯¹æ•°æ®
        if not merged_coupling_df.empty:
            merged_coupling_df.to_csv(final_output_dir / "all_coupling_pairs.csv", index=False)
            print(f"âœ… ä¿å­˜åˆå¹¶çš„è€¦åˆå¯¹æ•°æ®: {len(merged_coupling_df)} å¯¹")
        
        # ä¿å­˜åˆå¹¶çš„çŸ¥è¯†ç‰‡æ®µæ•°æ®
        with open(final_output_dir / "all_knowledge_pieces.json", 'w', encoding='utf-8') as f:
            json.dump(all_knowledge_pieces, f, indent=2, ensure_ascii=False)
        print(f"âœ… ä¿å­˜åˆå¹¶çš„çŸ¥è¯†ç‰‡æ®µæ•°æ®: {len(all_knowledge_pieces)} ä¸ª")
        
        # ä¿å­˜å…¨å±€ç»Ÿè®¡
        final_stats = {
            'global_statistics': global_stats,
            'batch_summaries': batch_summaries,
            'processing_metadata': {
                'model_path': self.model_path,
                'layer_range': self.layer_range,
                'batch_size': self.batch_size,
                'sub_batch_size': self.sub_batch_size,
                'total_processing_time': sum(s.get('processing_time', 0) for s in batch_summaries),
                'completion_timestamp': datetime.datetime.now().isoformat()
            }
        }
        
        with open(final_output_dir / "global_analysis_results.json", 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å…¨å±€ç»Ÿè®¡ç»“æœ:")
        print(f"   æ€»æ ·æœ¬æ•°: {global_stats['total_samples']:,}")
        print(f"   æ€»çŸ¥è¯†ç‰‡æ®µ: {global_stats['total_knowledge_pieces']:,}")
        print(f"   æ€»è€¦åˆå¯¹: {global_stats['total_coupling_pairs']:,}")
        print(f"   é«˜è€¦åˆå¯¹: {global_stats['total_high_coupling_pairs']:,} ({global_stats['high_coupling_ratio']:.2%})")
        print(f"   å¹³å‡è€¦åˆå¼ºåº¦: {global_stats['global_mean_coupling']:.4f}")
        print(f"   è€¦åˆå¼ºåº¦èŒƒå›´: [{global_stats['global_min_coupling']:.4f}, {global_stats['global_max_coupling']:.4f}]")
        
        return final_stats
    
    def run_full_processing(self, resume_from_checkpoint: bool = True) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ‰¹å¤„ç†åˆ†æ"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹å¤„ç†çŸ¥è¯†è€¦åˆåˆ†æ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 80)
        
        # åŠ è½½æ•°æ®
        print(f"ğŸ“š åŠ è½½HotpotQAæ•°æ®...")
        hotpot_data = load_hotpot_data("datasets/processed/hotpotqa_all_converted.json")
        total_samples = len(hotpot_data)
        print(f"âœ… åŠ è½½å®Œæˆ: {total_samples:,} ä¸ªæ ·æœ¬")
        
        # æ£€æŸ¥æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        start_batch_idx = 0
        batch_summaries = []
        
        if resume_from_checkpoint:
            checkpoint_info = self.load_latest_checkpoint()
            if checkpoint_info:
                start_batch_idx, accumulated_data = checkpoint_info
                start_batch_idx += 1  # ä»ä¸‹ä¸€ä¸ªæ‰¹æ¬¡å¼€å§‹
                
                # åŠ è½½ä¹‹å‰çš„æ‰¹æ¬¡æ‘˜è¦
                for i in range(start_batch_idx):
                    checkpoint_file = self.checkpoint_dir / f"checkpoint_batch_{i:04d}.json"
                    if checkpoint_file.exists():
                        with open(checkpoint_file, 'r', encoding='utf-8') as f:
                            checkpoint_data = json.load(f)
                            if 'batch_results' in checkpoint_data:
                                batch_summaries.append(checkpoint_data['batch_results'])
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        total_batches = (total_samples + self.batch_size - 1) // self.batch_size
        
        print(f"\nğŸ“‹ å¤„ç†è®¡åˆ’:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size:,}")
        print(f"   å­æ‰¹æ¬¡å¤§å°: {self.sub_batch_size}")
        print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
        print(f"   å¼€å§‹æ‰¹æ¬¡: {start_batch_idx}")
        print(f"   å‰©ä½™æ‰¹æ¬¡: {total_batches - start_batch_idx}")
        print(f"   é¢„è®¡æ¯æ‰¹æ¬¡å­æ‰¹æ¬¡æ•°: {(self.batch_size + self.sub_batch_size - 1) // self.sub_batch_size}")
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        accumulated_data = {
            'total_samples': len(batch_summaries) * self.batch_size,
            'total_pieces': sum(s.get('knowledge_pieces_count', 0) for s in batch_summaries)
        }
        
        for batch_idx in range(start_batch_idx, total_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, total_samples)
            batch_data = hotpot_data[start_idx:end_idx]
            
            print(f"\n{'='*60}")
            print(f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            print(f"æ ·æœ¬èŒƒå›´: {start_idx:,} - {end_idx:,}")
            print(f"è¿›åº¦: {(batch_idx + 1)/total_batches:.1%}")
            
            try:
                # å¤„ç†æ‰¹æ¬¡
                batch_result = self.process_single_batch(batch_data, batch_idx)
                batch_summaries.append(batch_result)
                
                # æ›´æ–°ç´¯è®¡æ•°æ®
                accumulated_data['total_samples'] += batch_result['samples_count']
                accumulated_data['total_pieces'] += batch_result['knowledge_pieces_count']
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(batch_idx, batch_result, accumulated_data)
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                # ä¿å­˜é”™è¯¯ä¿¡æ¯
                error_info = {
                    'batch_idx': batch_idx,
                    'error': str(e),
                    'timestamp': datetime.datetime.now().isoformat()
                }
                with open(self.checkpoint_dir / f"error_batch_{batch_idx:04d}.json", 'w') as f:
                    json.dump(error_info, f, indent=2)
                raise
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        print(f"\nğŸ”„ åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ...")
        final_results = self.merge_batch_results(batch_summaries)
        
        print(f"\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆ!")
        print(f"ğŸ“ æœ€ç»ˆç»“æœä¿å­˜åœ¨: {self.output_dir / 'final_merged_results'}")
        
        return final_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="çŸ¥è¯†è€¦åˆæ‰¹å¤„ç†å™¨ - å¤„ç†å…¨éƒ¨HotpotQA (ä¼˜åŒ–ç‰ˆ)")
    parser.add_argument("--model_path", default="meta-llama/Llama-2-7b-hf", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--batch_size", type=int, default=2000, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--sub_batch_size", type=int, default=10, help="å­æ‰¹æ¬¡å¤§å° (å‡å°‘æ˜¾å­˜ä½¿ç”¨)")
    parser.add_argument("--checkpoint_dir", default="checkpoints", help="æ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--output_dir", default="results/full_hotpotqa_analysis", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--no_resume", action="store_true", help="ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤")
    parser.add_argument("--layer_start", type=int, help="èµ·å§‹å±‚")
    parser.add_argument("--layer_end", type=int, help="ç»“æŸå±‚")
    
    args = parser.parse_args()
    
    # è®¾ç½®å±‚èŒƒå›´
    layer_range = None
    if args.layer_start is not None and args.layer_end is not None:
        layer_range = (args.layer_start, args.layer_end)
        print(f"ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰å±‚èŒƒå›´: {args.layer_start}-{args.layer_end}")
    elif 'llama' in args.model_path.lower():
        layer_range = (28, 31)  # LLaMAé«˜å±‚è¯­ä¹‰å±‚
        print(f"ğŸ¯ LLaMAè‡ªåŠ¨é€‰æ‹©å±‚èŒƒå›´: 28-31")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = KnowledgeCouplingBatchProcessor(
        model_path=args.model_path,
        batch_size=args.batch_size,
        sub_batch_size=args.sub_batch_size,
        checkpoint_dir=args.checkpoint_dir,
        output_dir=args.output_dir,
        layer_range=layer_range
    )
    
    # è¿è¡Œå¤„ç†
    results = processor.run_full_processing(resume_from_checkpoint=not args.no_resume)
    
    print(f"\nâœ… æ‰¹å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    main() 