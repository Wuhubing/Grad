#!/usr/bin/env python3
"""
Knowledge Coupling MVP for Multi-Model Support (LLaMA-2 & GPT-2)

çœŸæ­£çš„çŸ¥è¯†ç‰‡æ®µé—´è€¦åˆåº¦è®¡ç®—å’Œæ¶Ÿæ¼ªæ•ˆåº”åˆ†æç³»ç»Ÿ
ç°åœ¨æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„ï¼šLLaMA-2 7B/13B/70B å’Œ GPT-2 (small/medium/large/xl)

æ ¸å¿ƒæ–¹æ³•ï¼š
1. æ¢¯åº¦æŠ½å–ï¼šé’ˆå¯¹æ¯ä¸ªçŸ¥è¯†ç‰‡æ®µçš„çœŸå®ç­”æ¡ˆtokenè®¡ç®—æ¢¯åº¦
2. GradSimè®¡ç®—ï¼šçŸ¥è¯†ç‰‡æ®µé—´å‚æ•°ç©ºé—´é‡å åº¦é‡
3. è‡ªé€‚åº”å±‚é€‰æ‹©ï¼šæ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©ç›®æ ‡å±‚
4. ç»Ÿä¸€æ¥å£ï¼šæ”¯æŒå¤šç§æ¨¡å‹çš„ç»Ÿä¸€åˆ†ææ¥å£

Formula: GradSim(i,j) = cos(âˆ‡_Î¸ log P(a_i|q_i), âˆ‡_Î¸ log P(a_j|q_j))
"""

import json
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn.functional as F
from transformers import (
    LlamaTokenizer, LlamaForCausalLM,
    GPT2Tokenizer, GPT2LMHeadModel,
    AutoTokenizer, AutoModelForCausalLM
)
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class KnowledgePiece:
    """å•ä¸ªçŸ¥è¯†ç‰‡æ®µçš„è¡¨ç¤º"""
    def __init__(self, piece_id: str, question: str, answer: str, 
                 supporting_fact: str, category: str):
        self.piece_id = piece_id
        self.question = question  # é’ˆå¯¹è¿™ä¸ªçŸ¥è¯†ç‰‡æ®µçš„clozeé—®é¢˜
        self.answer = answer      # çœŸå®ç­”æ¡ˆtoken
        self.supporting_fact = supporting_fact  # æ”¯æŒäº‹å®
        self.category = category


class MultiModelKnowledgeCouplingMVP:
    """å¤šæ¨¡å‹çŸ¥è¯†è€¦åˆMVPåˆ†æå™¨ - æ”¯æŒLLaMA-2å’ŒGPT-2"""
    
    # æ”¯æŒçš„æ¨¡å‹é…ç½®
    SUPPORTED_MODELS = {
        'llama': {
            'patterns': ['llama', 'Llama'],
            'tokenizer_class': LlamaTokenizer,
            'model_class': LlamaForCausalLM,
            'target_layers': 'down_proj',  # LLaMA MLPå±‚
            'layer_pattern': 'model.layers.{}.mlp.down_proj'
        },
        'gpt2': {
            'patterns': ['gpt2', 'GPT2'],
            'tokenizer_class': GPT2Tokenizer,
            'model_class': GPT2LMHeadModel,
            'target_layers': 'mlp.c_proj',  # GPT-2 MLPå±‚
            'layer_pattern': 'transformer.h.{}.mlp.c_proj'
        }
    }
    
    def __init__(self, model_path: str = "gpt2", device: str = None, 
                 layer_range: Optional[Tuple[int, int]] = None):
        self.model_path = model_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model_type = self._detect_model_type(model_path)
        self.layer_range = layer_range  # æ–°å¢ï¼šæŒ‡å®šå±‚èŒƒå›´
        
        print(f"ğŸ¤– Initializing Multi-Model Knowledge Coupling MVP")
        print(f"Model: {model_path}")
        print(f"Type: {self.model_type.upper()}")
        print(f"Device: {self.device}")
        if layer_range:
            print(f"Layer range: {layer_range[0]}-{layer_range[1]} (focusing on high-level semantic layers)")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½ç›¸åº”çš„tokenizerå’Œmodel
        self._load_model()
        
        # å­˜å‚¨ç»“æœ
        self.knowledge_pieces = []
        self.gradients = {}
        self.coupling_matrix = None
        self.edit_results = []
    
    def _detect_model_type(self, model_path: str) -> str:
        """æ£€æµ‹æ¨¡å‹ç±»å‹"""
        model_path_lower = model_path.lower()
        
        for model_type, config in self.SUPPORTED_MODELS.items():
            for pattern in config['patterns']:
                if pattern.lower() in model_path_lower:
                    return model_type
        
        # é»˜è®¤å°è¯•autoæ£€æµ‹
        print(f"âš ï¸ Unknown model type for {model_path}, trying auto-detection...")
        return 'auto'
    
    def _load_model(self):
        """æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½å¯¹åº”çš„æ¨¡å‹å’Œtokenizer"""
        if self.model_type == 'auto':
            # ä½¿ç”¨AutoTokenizerå’ŒAutoModelForCausalLM
            print("ğŸ” Using auto-detection for model loading...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            # å°è¯•æ¨æ–­å®é™…çš„æ¨¡å‹ç±»å‹
            self._infer_model_type_from_loaded_model()
        else:
            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç±»å‹
            config = self.SUPPORTED_MODELS[self.model_type]
            print(f"ğŸ“¦ Loading {self.model_type.upper()} model...")
            
            self.tokenizer = config['tokenizer_class'].from_pretrained(self.model_path)
            self.model = config['model_class'].from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            if hasattr(self.tokenizer, 'eos_token') and self.tokenizer.eos_token:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        self.model.eval()
        print(f"âœ… Model loaded successfully!")
    
    def _infer_model_type_from_loaded_model(self):
        """ä»å·²åŠ è½½çš„æ¨¡å‹æ¨æ–­æ¨¡å‹ç±»å‹"""
        model_class_name = self.model.__class__.__name__
        
        if 'Llama' in model_class_name:
            self.model_type = 'llama'
        elif 'GPT2' in model_class_name:
            self.model_type = 'gpt2'
        else:
            # æ£€æŸ¥æ¨¡å‹ç»“æ„æ¥æ¨æ–­
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
                self.model_type = 'llama'
            elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):
                self.model_type = 'gpt2'
            else:
                print("âš ï¸ Could not infer model type, defaulting to gpt2")
                self.model_type = 'gpt2'
        
        print(f"ğŸ” Inferred model type: {self.model_type.upper()}")
    
    def _get_target_layers(self) -> List[str]:
        """è·å–ç›®æ ‡å±‚åç§°åˆ—è¡¨ - æ”¯æŒæŒ‡å®šå±‚èŒƒå›´"""
        if self.model_type not in self.SUPPORTED_MODELS:
            # é€šç”¨æ–¹æ³•ï¼šæŸ¥æ‰¾æ‰€æœ‰MLPç›¸å…³å±‚
            target_layers = []
            for name, param in self.model.named_parameters():
                if any(keyword in name.lower() for keyword in ['mlp', 'ffn', 'feed_forward']):
                    if any(keyword in name.lower() for keyword in ['proj', 'linear', 'dense']):
                        target_layers.append(name)
            return target_layers
        
        config = self.SUPPORTED_MODELS[self.model_type]
        target_layers = []
        
        # è·å–å±‚æ•°
        if self.model_type == 'llama':
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
                num_layers = len(self.model.model.layers)
            else:
                num_layers = 32  # é»˜è®¤å€¼
        elif self.model_type == 'gpt2':
            if hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):
                num_layers = len(self.model.transformer.h)
            else:
                num_layers = 12  # é»˜è®¤å€¼
        else:
            num_layers = 12
        
        # ç¡®å®šå±‚èŒƒå›´
        if self.layer_range:
            start_layer, end_layer = self.layer_range
            start_layer = max(0, start_layer)
            end_layer = min(num_layers, end_layer + 1)  # +1 because range is exclusive
        else:
            # é»˜è®¤ä½¿ç”¨æœ€å4å±‚ï¼ˆé«˜å±‚è¯­ä¹‰è¡¨ç¤ºï¼‰
            if self.model_type == 'llama':
                start_layer = max(0, num_layers - 4)  # æœ€å4å±‚
                end_layer = num_layers
            else:  # gpt2
                start_layer = max(0, num_layers - 4)  # æœ€å4å±‚
                end_layer = num_layers
        
        # ç”Ÿæˆç›®æ ‡å±‚åç§°
        layer_pattern = config['layer_pattern']
        for i in range(start_layer, end_layer):
            layer_name = layer_pattern.format(i)
            target_layers.append(layer_name)
        
        print(f"ğŸ¯ Using layers {start_layer}-{end_layer-1} ({len(target_layers)} layers total)")
        return target_layers
    
    def extract_knowledge_pieces_from_hotpot(self, hotpot_data: List[Dict], 
                                           max_samples: int = 100) -> List[KnowledgePiece]:
        """ä»HotpotQAæå–2-hopçŸ¥è¯†ç‰‡æ®µ"""
        print(f"ğŸ“š Extracting knowledge pieces from HotpotQA...")
        
        knowledge_pieces = []
        processed = 0
        
        for sample in hotpot_data:
            if processed >= max_samples:
                break
                
            if len(sample['supporting_facts']) < 2:
                continue
                
            question = sample['question']
            answer = sample['answer']
            supporting_facts = sample['supporting_facts']
            
            # å–å‰ä¸¤ä¸ªæ”¯æŒäº‹å®ä½œä¸ºhop-1å’Œhop-2
            for hop_idx, (title, sent_id) in enumerate(supporting_facts[:2]):
                # ä»contextä¸­æ‰¾åˆ°å¯¹åº”çš„å¥å­
                sent_text = None
                for context_title, context_sents in sample['context']:
                    if context_title == title and sent_id < len(context_sents):
                        sent_text = context_sents[sent_id]
                        break
                
                if sent_text:
                    # åˆ›å»ºé’ˆå¯¹è¿™ä¸ªknowledge pieceçš„clozeé—®é¢˜
                    # å°†supporting factè½¬æ¢ä¸ºclozeæ ¼å¼
                    cloze_question = self._create_cloze_question(sent_text, answer)
                    
                    # ç¡®å®šç±»åˆ«
                    category = self._categorize_knowledge(sent_text, answer)
                    
                    piece = KnowledgePiece(
                        piece_id=f"{sample['_id']}_hop_{hop_idx}",
                        question=cloze_question,
                        answer=answer,
                        supporting_fact=sent_text,
                        category=category
                    )
                    
                    knowledge_pieces.append(piece)
            
            processed += 1
        
        self.knowledge_pieces = knowledge_pieces
        print(f"âœ… Extracted {len(knowledge_pieces)} knowledge pieces from {processed} samples")
        
        return knowledge_pieces
    
    def _create_cloze_question(self, supporting_fact: str, answer: str) -> str:
        """å°†æ”¯æŒäº‹å®è½¬æ¢ä¸ºclozeé—®é¢˜"""
        # ç®€å•çš„æ›¿æ¢ç­–ç•¥ï¼šå°†ç­”æ¡ˆæ›¿æ¢ä¸ºç©ºç™½
        if answer.lower() in supporting_fact.lower():
            cloze = supporting_fact.replace(answer, "___", 1)
            return f"Fill in the blank: {cloze}"
        else:
            # å¦‚æœç›´æ¥æ›¿æ¢ä¸è¡Œï¼Œåˆ›å»ºä¸€ä¸ªåŸºäºäº‹å®çš„é—®é¢˜
            return f"Based on the fact '{supporting_fact}', what is the answer? ___"
    
    def _categorize_knowledge(self, supporting_fact: str, answer: str) -> str:
        """ç®€å•çš„çŸ¥è¯†ç±»åˆ«åˆ†ç±»"""
        fact_lower = supporting_fact.lower()
        answer_lower = answer.lower()
        
        # åŸºäºå…³é”®è¯çš„ç®€å•åˆ†ç±»
        if any(word in fact_lower for word in ['born', 'actor', 'director', 'person', 'he', 'she']):
            return 'Person'
        elif any(word in fact_lower for word in ['company', 'organization', 'corporation', 'founded']):
            return 'Organization'  
        elif any(word in fact_lower for word in ['city', 'country', 'located', 'state', 'region']):
            return 'Location'
        elif any(word in fact_lower for word in ['film', 'movie', 'book', 'album', 'song']):
            return 'Work'
        elif any(word in fact_lower for word in ['year', 'date', 'century', 'time', 'when']):
            return 'Time'
        else:
            return 'Science'
    
    def compute_knowledge_gradient(self, piece: KnowledgePiece) -> Optional[torch.Tensor]:
        """è®¡ç®—çŸ¥è¯†ç‰‡æ®µçš„æ¢¯åº¦å‘é‡ - GPUä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæŒ‡å®šå±‚èŒƒå›´"""
        try:
            prompt = piece.question + " Answer:"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # ç¼–ç ç­”æ¡ˆ
            answer_tokens = self.tokenizer(piece.answer, add_special_tokens=False)['input_ids']
            if not answer_tokens:
                print(f"Warning: No tokens for answer '{piece.answer}'")
                return None
            
            target_token_id = answer_tokens[0]
            
            # è®¡ç®—logitså¹¶è®¾ç½®æ¢¯åº¦
            self.model.eval()
            self.model.zero_grad()
            
            outputs = self.model(**inputs)
            logits = outputs.logits
            
            # è®¡ç®—ç›®æ ‡ç­”æ¡ˆçš„log probability
            log_probs = F.log_softmax(logits[0, -1, :], dim=-1)
            target_logp = log_probs[target_token_id]
            
            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            target_logp.backward()
            
            # æå–ç›®æ ‡å±‚çš„æ¢¯åº¦å¹¶åœ¨GPUä¸Šæ‹¼æ¥
            target_layers = self._get_target_layers()
            gradient_tensors = []
            
            for name, param in self.model.named_parameters():
                if any(target_layer in name for target_layer in target_layers):
                    if param.grad is not None:
                        # ä¿æŒåœ¨GPUä¸Šï¼Œå±•å¹³ä¸º1Då¼ é‡
                        flat_grad = param.grad.flatten()
                        gradient_tensors.append(flat_grad)
            
            if gradient_tensors:
                # åœ¨GPUä¸Šæ‹¼æ¥æ‰€æœ‰æ¢¯åº¦
                full_gradient = torch.cat(gradient_tensors, dim=0)
                # æ ‡å‡†åŒ–
                full_gradient = F.normalize(full_gradient, p=2, dim=0)
                return full_gradient
            else:
                print(f"Warning: No gradients found for {piece.piece_id}")
                return None
                
        except Exception as e:
            print(f"Error computing gradient for {piece.piece_id}: {e}")
            return None
        finally:
            self.model.zero_grad()

    def compute_all_gradients(self) -> Dict[str, torch.Tensor]:
        """è®¡ç®—æ‰€æœ‰çŸ¥è¯†ç‰‡æ®µçš„æ¢¯åº¦ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.knowledge_pieces:
            raise ValueError("No knowledge pieces loaded.")
        
        print(f"ğŸ”¬ Computing gradients for all knowledge pieces using {self.model_type}...")
        
        gradients = {}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼Œä½†ä¿æŒå¼ é‡åœ¨GPUä¸Š
        for piece in tqdm(self.knowledge_pieces, desc="Computing gradients"):
            gradient = self.compute_knowledge_gradient(piece)
            if gradient is not None:
                gradients[piece.piece_id] = gradient  # ä¿æŒGPUå¼ é‡
        
        print(f"âœ… Successfully computed {len(gradients)} gradients")
        
        # æ›´æ–°ç±»å±æ€§
        self.gradients = gradients
        
        # æ˜¾ç¤ºç›®æ ‡å±‚ä¿¡æ¯
        target_layers = self._get_target_layers()
        print(f"ğŸ¯ Target layers used: {len(target_layers)} layers")
        if len(target_layers) <= 5:
            for layer in target_layers[:5]:
                print(f"   - {layer}")
        else:
            print(f"   - {target_layers[0]} ... {target_layers[-1]} (showing first and last)")
        
        return gradients
    
    def compute_coupling_matrix(self) -> torch.Tensor:
        """è®¡ç®—çŸ¥è¯†ç‰‡æ®µé—´çš„è€¦åˆçŸ©é˜µ - GPUä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.gradients:
            raise ValueError("No gradients computed. Run compute_all_gradients() first.")
        
        print("ğŸ”— Computing knowledge coupling matrix...")
        
        # å‡†å¤‡æ¢¯åº¦çŸ©é˜µ - åœ¨GPUä¸Šæ“ä½œ
        piece_ids = list(self.gradients.keys())
        gradient_list = [self.gradients[pid] for pid in piece_ids]
        
        # å †å ä¸ºçŸ©é˜µ [n_pieces, gradient_dim]
        gradient_matrix = torch.stack(gradient_list, dim=0)
        
        print(f"ğŸ“Š Gradient matrix shape: {gradient_matrix.shape}")
        print(f"ğŸš€ Computing on device: {gradient_matrix.device}")
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ - åœ¨GPUä¸Š
        # gradient_matrixå·²ç»æ ‡å‡†åŒ–äº†ï¼Œæ‰€ä»¥ç›´æ¥çŸ©é˜µä¹˜æ³•
        coupling_matrix = torch.mm(gradient_matrix, gradient_matrix.t())
        
        # å°†å¯¹è§’çº¿è®¾ä¸º1.0ï¼ˆè‡ªç›¸ä¼¼ï¼‰
        coupling_matrix.fill_diagonal_(1.0)
        
        self.coupling_matrix = coupling_matrix
        self.piece_ids_order = piece_ids  # ä¿å­˜é¡ºåº
        
        print(f"ğŸ“Š Computed {coupling_matrix.shape[0]}Ã—{coupling_matrix.shape[1]} coupling matrix on GPU")
        return coupling_matrix

    def save_analysis_results(self, output_dir: str = "results/coupling_analysis", dataset_info: Dict[str, Any] = None):
        """ä¿å­˜åˆ†æç»“æœä¸ºè·¨potåˆ†ææ‰€éœ€æ ¼å¼"""
        import os
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ’¾ Saving analysis results to {output_dir}/...")
        
        # 1. ä¿å­˜çŸ¥è¯†ç‰‡æ®µä¸ºJSON
        pieces_data = []
        for piece in self.knowledge_pieces:
            pieces_data.append({
                'piece_id': piece.piece_id,
                'question': piece.question,
                'answer': piece.answer,
                'supporting_fact': piece.supporting_fact,
                'category': piece.category
            })
        
        pieces_file = os.path.join(output_dir, "knowledge_pieces.json")
        with open(pieces_file, 'w', encoding='utf-8') as f:
            json.dump(pieces_data, f, indent=2, ensure_ascii=False)
        print(f"âœ… Saved {len(pieces_data)} knowledge pieces to knowledge_pieces.json")
        
        # 2. ä¿å­˜è€¦åˆçŸ©é˜µä¸ºnumpyæ ¼å¼
        if self.coupling_matrix is not None:
            # è½¬ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpy
            coupling_np = self.coupling_matrix.detach().cpu().numpy()
            coupling_file = os.path.join(output_dir, "coupling_matrix.npy")
            np.save(coupling_file, coupling_np)
            print(f"âœ… Saved coupling matrix ({coupling_np.shape}) to coupling_matrix.npy")
            
            # 3. ä¿å­˜ç‰‡æ®µIDé¡ºåº
            order_file = os.path.join(output_dir, "piece_ids_order.json")
            with open(order_file, 'w', encoding='utf-8') as f:
                json.dump(self.piece_ids_order, f, indent=2)
            print(f"âœ… Saved piece IDs order to piece_ids_order.json")
        
        # 4. ä¿å­˜GPUæ€§èƒ½ä¿¡æ¯å’Œæ•°æ®é›†ä¿¡æ¯
        analysis_metadata = {
            'model_info': {
                'model_path': self.model_path,
                'model_type': self.model_type,
                'layer_range': self.layer_range,
                'target_layers_count': len(self._get_target_layers())
            },
            'dataset_info': dataset_info or {},
            'analysis_timestamp': self._get_timestamp()
        }
        
        if torch.cuda.is_available():
            analysis_metadata['gpu_info'] = {
                'device': str(self.device),
                'gpu_name': torch.cuda.get_device_name(0),
                'gpu_memory_allocated': torch.cuda.memory_allocated() / 1e9,
                'gpu_memory_reserved': torch.cuda.memory_reserved() / 1e9,
                'gradient_computation': 'GPU',
                'coupling_computation': 'GPU'
            }
            
        metadata_file = os.path.join(output_dir, "analysis_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_metadata, f, indent=2)
        print(f"âœ… Saved analysis metadata and dataset info to analysis_metadata.json")
        
        print(f"ğŸ¯ All results saved to {output_dir}/ - Ready for analysis!")

    def analyze_coupling_patterns(self) -> Dict[str, Any]:
        """åˆ†æè€¦åˆæ¨¡å¼ - å…¼å®¹GPUå¼ é‡"""
        if self.coupling_matrix is None:
            raise ValueError("Coupling matrix not computed.")
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œåˆ†æï¼ˆå¦‚æœæ˜¯GPUå¼ é‡ï¼‰
        if isinstance(self.coupling_matrix, torch.Tensor):
            coupling_np = self.coupling_matrix.detach().cpu().numpy()
        else:
            coupling_np = self.coupling_matrix
        
        # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªç›¸ä¼¼ï¼‰
        n = coupling_np.shape[0]
        mask = ~np.eye(n, dtype=bool)
        off_diagonal = coupling_np[mask]
        
        # åˆ†æè€¦åˆå¼ºåº¦åˆ†å¸ƒ
        high_coupling = np.sum(off_diagonal >= 0.4)
        moderate_coupling = np.sum((off_diagonal >= 0.1) & (off_diagonal < 0.4))
        low_coupling = np.sum(off_diagonal < 0.1)
        
        total_pairs = len(off_diagonal)
        
        analysis = {
            'total_pairs': total_pairs,
            'high_coupling_pairs': high_coupling,
            'moderate_coupling_pairs': moderate_coupling,
            'low_coupling_pairs': low_coupling,
            'high_coupling_ratio': high_coupling / total_pairs,
            'mean_coupling': float(np.mean(off_diagonal)),
            'std_coupling': float(np.std(off_diagonal)),
            'max_coupling': float(np.max(off_diagonal)),
            'min_coupling': float(np.min(off_diagonal))
        }
        
        return analysis

    def predict_ripple_candidates(self, threshold: float = 0.4) -> List[Tuple[str, str, float]]:
        """é¢„æµ‹é«˜è€¦åˆçš„çŸ¥è¯†ç‰‡æ®µå¯¹ï¼ˆæ¶Ÿæ¼ªæ•ˆåº”å€™é€‰ï¼‰- å…¼å®¹GPUå¼ é‡"""
        if self.coupling_matrix is None:
            raise ValueError("Coupling matrix not computed.")
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œåˆ†æï¼ˆå¦‚æœæ˜¯GPUå¼ é‡ï¼‰
        if isinstance(self.coupling_matrix, torch.Tensor):
            coupling_np = self.coupling_matrix.detach().cpu().numpy()
        else:
            coupling_np = self.coupling_matrix
        
        piece_ids = [p.piece_id for p in self.knowledge_pieces]
        high_coupling_pairs = []
        
        n = len(piece_ids)
        for i in range(n):
            for j in range(i + 1, n):
                coupling_strength = coupling_np[i, j]
                if coupling_strength >= threshold:
                    high_coupling_pairs.append((piece_ids[i], piece_ids[j], float(coupling_strength)))
        
        # æŒ‰è€¦åˆå¼ºåº¦æ’åº
        high_coupling_pairs.sort(key=lambda x: x[2], reverse=True)
        
        print(f"ğŸ”¥ Found {len(high_coupling_pairs)} high coupling pairs (threshold={threshold})")
        return high_coupling_pairs
    
    def simulate_memit_edit(self, target_piece_id: str, new_answer: str) -> Dict[str, Any]:
        """æ¨¡æ‹ŸMEMITç¼–è¾‘æ“ä½œï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æ‰¾åˆ°ç›®æ ‡çŸ¥è¯†ç‰‡æ®µ
        target_piece = None
        for piece in self.knowledge_pieces:
            if piece.piece_id == target_piece_id:
                target_piece = piece
                break
        
        if target_piece is None:
            raise ValueError(f"Knowledge piece {target_piece_id} not found.")
        
        print(f"ğŸ”§ Simulating MEMIT edit on {target_piece_id}")
        print(f"Original answer: {target_piece.answer}")
        print(f"New answer: {new_answer}")
        
        # è®¡ç®—ç¼–è¾‘å‰çš„logP
        original_logp = self._compute_logp(target_piece.question, target_piece.answer)
        
        # æ¨¡æ‹Ÿç¼–è¾‘åçš„logPå˜åŒ–ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        # åœ¨çœŸå®å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥ç”¨MEMITä¿®æ”¹æ¨¡å‹å‚æ•°
        edited_logp = self._compute_logp(target_piece.question, new_answer)
        
        delta_logp = edited_logp - original_logp
        
        edit_result = {
            'target_piece_id': target_piece_id,
            'original_answer': target_piece.answer,
            'new_answer': new_answer,
            'original_logp': original_logp,
            'edited_logp': edited_logp,
            'delta_logp': delta_logp,
            'edit_success': delta_logp > 0  # ç®€åŒ–çš„æˆåŠŸåˆ¤æ–­
        }
        
        return edit_result
    
    def _compute_logp(self, question: str, answer: str) -> float:
        """è®¡ç®—é—®é¢˜-ç­”æ¡ˆå¯¹çš„log probability"""
        try:
            prompt = f"{question} Answer:"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            answer_tokens = self.tokenizer(answer, add_special_tokens=False)['input_ids']
            if not answer_tokens:
                return float('-inf')
            
            target_token_id = answer_tokens[0]
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                log_probs = F.log_softmax(logits[0, -1, :], dim=-1)
                return log_probs[target_token_id].item()
                
        except Exception as e:
            print(f"Error computing logP: {e}")
            return float('-inf')
    
    def _find_piece_by_id(self, piece_id: str) -> Optional[KnowledgePiece]:
        """æ ¹æ®IDæŸ¥æ‰¾çŸ¥è¯†ç‰‡æ®µ"""
        for piece in self.knowledge_pieces:
            if piece.piece_id == piece_id:
                return piece
        return None
    
    def _analyze_cross_pot_patterns(self) -> Dict[str, Any]:
        """åˆ†æè·¨potçš„è€¦åˆæ¨¡å¼"""
        if not self.knowledge_pieces or self.coupling_matrix is None:
            return {}
        
        # æ„å»ºpotæ˜ å°„
        pot_to_pieces = {}
        piece_to_pot = {}
        
        for i, piece in enumerate(self.knowledge_pieces):
            pot_id = piece.piece_id.split('_hop_')[0]  # hotpot_train_X
            
            if pot_id not in pot_to_pieces:
                pot_to_pieces[pot_id] = []
            pot_to_pieces[pot_id].append(i)
            piece_to_pot[i] = pot_id
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œåˆ†æ
        if isinstance(self.coupling_matrix, torch.Tensor):
            coupling_np = self.coupling_matrix.detach().cpu().numpy()
        else:
            coupling_np = self.coupling_matrix
        
        # åˆ†æè·¨pot vs åŒpotè€¦åˆ
        cross_pot_couplings = []
        same_pot_couplings = []
        
        for i in range(len(self.knowledge_pieces)):
            for j in range(i+1, len(self.knowledge_pieces)):
                coupling_strength = coupling_np[i, j]
                pot_i = piece_to_pot[i]
                pot_j = piece_to_pot[j]
                
                if pot_i != pot_j:
                    cross_pot_couplings.append(coupling_strength)
                else:
                    same_pot_couplings.append(coupling_strength)
        
        return {
            'unique_pots': len(pot_to_pieces),
            'cross_pot_pairs': len(cross_pot_couplings),
            'same_pot_pairs': len(same_pot_couplings),
            'cross_pot_mean': float(np.mean(cross_pot_couplings)) if cross_pot_couplings else 0.0,
            'same_pot_mean': float(np.mean(same_pot_couplings)) if same_pot_couplings else 0.0,
            'cross_pot_std': float(np.std(cross_pot_couplings)) if cross_pot_couplings else 0.0,
            'same_pot_std': float(np.std(same_pot_couplings)) if same_pot_couplings else 0.0,
            'high_cross_pot_pairs': sum(1 for c in cross_pot_couplings if c >= 0.4)
        }
    
    def _get_category_stats(self) -> Dict[str, int]:
        """è·å–çŸ¥è¯†ç‰‡æ®µç±»åˆ«ç»Ÿè®¡"""
        category_counts = {}
        for piece in self.knowledge_pieces:
            category = piece.category
            category_counts[category] = category_counts.get(category, 0) + 1
        return category_counts
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def measure_ripple_effects(self, edit_result: Dict[str, Any], 
                             coupling_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """æµ‹é‡ç¼–è¾‘çš„æ¶Ÿæ¼ªæ•ˆåº”"""
        target_piece_id = edit_result['target_piece_id']
        
        # æ‰¾åˆ°ä¸ç›®æ ‡ç‰‡æ®µé«˜è€¦åˆçš„å…¶ä»–ç‰‡æ®µ
        target_idx = None
        piece_ids = [p.piece_id for p in self.knowledge_pieces]
        
        for i, pid in enumerate(piece_ids):
            if pid == target_piece_id:
                target_idx = i
                break
        
        if target_idx is None:
            return []
        
        ripple_effects = []
        
        for i, piece in enumerate(self.knowledge_pieces):
            if i != target_idx:
                coupling_strength = self.coupling_matrix[target_idx, i]
                
                if coupling_strength >= coupling_threshold:
                    # æµ‹é‡è¿™ä¸ªç‰‡æ®µçš„å˜åŒ–
                    original_logp = self._compute_logp(piece.question, piece.answer)
                    
                    # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥ç”¨ç¼–è¾‘åçš„æ¨¡å‹é‡æ–°è®¡ç®—
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå‡è®¾å˜åŒ–ä¸è€¦åˆå¼ºåº¦æˆæ­£æ¯”
                    predicted_delta = edit_result['delta_logp'] * coupling_strength
                    
                    ripple_effect = {
                        'affected_piece_id': piece.piece_id,
                        'coupling_strength': coupling_strength,
                        'original_logp': original_logp,
                        'predicted_delta_logp': predicted_delta,
                        'category': piece.category
                    }
                    
                    ripple_effects.append(ripple_effect)
        
        # æŒ‰è€¦åˆå¼ºåº¦æ’åº
        ripple_effects.sort(key=lambda x: x['coupling_strength'], reverse=True)
        
        print(f"ğŸŒŠ Measured {len(ripple_effects)} ripple effects")
        return ripple_effects
    
    def plot_coupling_heatmap(self, save_path: str = None, output_dir: str = "results/coupling_analysis"):
        """ç»˜åˆ¶è€¦åˆçƒ­å›¾"""
        if self.coupling_matrix is None:
            raise ValueError("Coupling matrix not computed.")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œä½¿ç”¨output_dir
        if save_path is None:
            os.makedirs(output_dir, exist_ok=True)
            save_path = os.path.join(output_dir, "coupling_heatmap.png")
        
        plt.figure(figsize=(12, 10))
        
        # åˆ›å»ºæ ‡ç­¾
        labels = [f"K{i}" for i in range(len(self.knowledge_pieces))]
        
        # è½¬æ¢GPUå¼ é‡ä¸ºnumpyæ•°ç»„
        if isinstance(self.coupling_matrix, torch.Tensor):
            coupling_data = self.coupling_matrix.detach().cpu().numpy()
        else:
            coupling_data = self.coupling_matrix
        
        sns.heatmap(
            coupling_data,
            annot=False,
            cmap='RdYlBu_r',
            square=True,
            cbar_kws={'label': 'Knowledge Coupling Strength'},
            xticklabels=labels,
            yticklabels=labels
        )
        
        plt.title(f'Knowledge Coupling Matrix ({self.model_type.upper()})')
        plt.xlabel('Knowledge Pieces')
        plt.ylabel('Knowledge Pieces')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ¨ Coupling heatmap saved to: {save_path}")
        return save_path
    
    def generate_mvp_report(self, save_path: str = None, output_dir: str = "results/coupling_analysis") -> str:
        """ç”ŸæˆMVPåˆ†ææŠ¥å‘Š"""
        if self.coupling_matrix is None:
            raise ValueError("Analysis not completed. Run full analysis first.")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œä½¿ç”¨output_dir
        if save_path is None:
            os.makedirs(output_dir, exist_ok=True)
            save_path = os.path.join(output_dir, "mvp_report.md")
        
        coupling_analysis = self.analyze_coupling_patterns()
        high_coupling_pairs = self.predict_ripple_candidates()
        
        with open(save_path, 'w') as f:
            f.write("# Knowledge Coupling MVP Analysis Report\n\n")
            f.write("## ğŸ¯ Research Goal\n")
            f.write(f"Quantify knowledge piece coupling in {self.model_type.upper()} using gradient similarity analysis.\n\n")
            
            f.write("## ğŸ“Š Key Findings\n\n")
            f.write(f"**Total Knowledge Pieces:** {len(self.knowledge_pieces)}  \n")
            f.write(f"**Mean Coupling Strength:** {coupling_analysis['mean_coupling']:.4f}  \n")
            f.write(f"**High Coupling Pairs:** {coupling_analysis['high_coupling_pairs']} ({coupling_analysis['high_coupling_ratio']:.2%})  \n")
            f.write(f"**Coupling Range:** [{coupling_analysis['min_coupling']:.4f}, {coupling_analysis['max_coupling']:.4f}]  \n\n")
            
            f.write("## ğŸ”¥ Top High-Coupling Pairs\n\n")
            f.write("| Knowledge Piece 1 | Knowledge Piece 2 | Coupling Strength |\n")
            f.write("|-------------------|-------------------|-------------------|\n")
            
            for pair in high_coupling_pairs[:10]:
                f.write(f"| {pair[0]} | {pair[1]} | {pair[2]:.4f} |\n")
            
            f.write("\n## ğŸ”¬ Methodology\n\n")
            f.write("1. **Knowledge Extraction:** HotpotQA 2-hop chains â†’ cloze questions\n")
            f.write(f"2. **Gradient Computation:** âˆ‡_Î¸ log P(answer|question) targeting {self.model_type.upper()} layers\n")
            f.write("3. **Coupling Measurement:** Cosine similarity between gradient vectors\n")
            f.write("4. **Ripple Prediction:** High coupling (â‰¥0.4) â†’ potential ripple effects\n\n")
            
            f.write("## ğŸ¯ Next Steps\n\n")
            f.write("1. **MEMIT Integration:** Implement actual knowledge editing\n")
            f.write("2. **Ripple Measurement:** Î”logP and EM evaluation on edited model\n")
            f.write("3. **Validation:** GradSim vs actual ripple correlation analysis\n\n")
            
            f.write("---\n*Generated by Multi-Model Knowledge Coupling MVP*\n")
        
        print(f"ğŸ“ MVP report saved to: {save_path}")
        return save_path
    
    def save_coupling_results_for_validation(self, output_dir: str = "results/coupling_analysis", dataset_info: Dict[str, Any] = None):
        """ä¿å­˜è€¦åˆåº¦è®¡ç®—ç»“æœç”¨äºéªŒè¯ - ä¸“æ³¨äºæ ¸å¿ƒæ•°æ®"""
        import os
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ’¾ Saving coupling results for validation to {output_dir}/...")
        
        # è½¬æ¢è€¦åˆçŸ©é˜µä¸ºnumpy
        if isinstance(self.coupling_matrix, torch.Tensor):
            coupling_np = self.coupling_matrix.detach().cpu().numpy()
        else:
            coupling_np = self.coupling_matrix
        
        # 1. ä¿å­˜è¯¦ç»†çš„è€¦åˆåº¦ç»“æœ
        coupling_results = {
            'metadata': {
                'model_path': self.model_path,
                'model_type': self.model_type,
                'layer_range': self.layer_range,
                'total_knowledge_pieces': len(self.knowledge_pieces),
                'matrix_shape': coupling_np.shape,
                'timestamp': self._get_timestamp()
            },
            'dataset_info': dataset_info or {},
            'knowledge_pieces': [],
            'coupling_pairs': [],
            'statistics': {}
        }
        
        # æ·»åŠ çŸ¥è¯†ç‰‡æ®µä¿¡æ¯
        for i, piece in enumerate(self.knowledge_pieces):
            coupling_results['knowledge_pieces'].append({
                'index': i,
                'piece_id': piece.piece_id,
                'question': piece.question,
                'answer': piece.answer,
                'category': piece.category
            })
        
        # æ·»åŠ æ‰€æœ‰è€¦åˆå¯¹ï¼ˆä¸åªæ˜¯é«˜è€¦åˆçš„ï¼‰
        piece_ids = [p.piece_id for p in self.knowledge_pieces]
        for i in range(len(piece_ids)):
            for j in range(i + 1, len(piece_ids)):
                coupling_strength = float(coupling_np[i, j])
                coupling_results['coupling_pairs'].append({
                    'piece_1_index': i,
                    'piece_2_index': j,
                    'piece_1_id': piece_ids[i],
                    'piece_2_id': piece_ids[j],
                    'coupling_strength': coupling_strength,
                    'is_same_hotpot': piece_ids[i].split('_hop_')[0] == piece_ids[j].split('_hop_')[0]
                })
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        off_diagonal = coupling_np[~np.eye(coupling_np.shape[0], dtype=bool)]
        coupling_results['statistics'] = {
            'mean_coupling': float(np.mean(off_diagonal)),
            'std_coupling': float(np.std(off_diagonal)),
            'min_coupling': float(np.min(off_diagonal)),
            'max_coupling': float(np.max(off_diagonal)),
            'high_coupling_count': int(np.sum(off_diagonal >= 0.4)),
            'moderate_coupling_count': int(np.sum((off_diagonal >= 0.1) & (off_diagonal < 0.4))),
            'low_coupling_count': int(np.sum(off_diagonal < 0.1))
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        coupling_file = os.path.join(output_dir, "coupling_results_for_validation.json")
        with open(coupling_file, 'w', encoding='utf-8') as f:
            json.dump(coupling_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Saved coupling results for validation: coupling_results_for_validation.json")
        
        # 2. ä¿å­˜ç®€åŒ–çš„CSVæ ¼å¼ç”¨äºå¿«é€ŸéªŒè¯
        csv_file = os.path.join(output_dir, "coupling_pairs.csv")
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write("piece_1_id,piece_2_id,coupling_strength,is_same_hotpot,piece_1_answer,piece_2_answer\n")
            for pair in coupling_results['coupling_pairs']:
                p1_answer = coupling_results['knowledge_pieces'][pair['piece_1_index']]['answer']
                p2_answer = coupling_results['knowledge_pieces'][pair['piece_2_index']]['answer']
                f.write(f"{pair['piece_1_id']},{pair['piece_2_id']},{pair['coupling_strength']:.6f},{pair['is_same_hotpot']},{p1_answer},{p2_answer}\n")
        
        print(f"âœ… Saved CSV format for quick analysis: coupling_pairs.csv")
        
        # 3. ä¿å­˜é«˜è€¦åˆå¯¹çš„è¯¦ç»†ä¿¡æ¯
        high_coupling_pairs = [p for p in coupling_results['coupling_pairs'] if p['coupling_strength'] >= 0.4]
        high_coupling_pairs.sort(key=lambda x: x['coupling_strength'], reverse=True)
        
        high_coupling_file = os.path.join(output_dir, "high_coupling_pairs.json")
        with open(high_coupling_file, 'w', encoding='utf-8') as f:
            json.dump({
                'threshold': 0.4,
                'count': len(high_coupling_pairs),
                'pairs': high_coupling_pairs
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Saved high coupling pairs: high_coupling_pairs.json ({len(high_coupling_pairs)} pairs)")
        
        return coupling_file

    def run_mvp_analysis(self, hotpot_data: List[Dict], max_samples: int = 50, 
                         output_dir: str = "results/coupling_analysis", 
                         dataset_file_path: str = None,
                         generate_report: bool = False,
                         generate_heatmap: bool = False) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„MVPåˆ†æ"""
        print("\n" + "ğŸ¤–" + "="*50 + "ğŸ¤–")
        print("MULTI-MODEL KNOWLEDGE COUPLING MVP ANALYSIS")
        print("="*52)
        
        # å‡†å¤‡æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'dataset_file_path': dataset_file_path,
            'total_samples_in_file': len(hotpot_data),
            'samples_processed': min(max_samples, len(hotpot_data)),
            'dataset_name': 'HotpotQA',
            'data_format': 'multi-hop QA'
        }
        
        # åˆ†ææ•°æ®é›†åŸºæœ¬ä¿¡æ¯
        if hotpot_data:
            sample_fields = list(hotpot_data[0].keys()) if hotpot_data else []
            dataset_info['sample_fields'] = sample_fields
            
            # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ
            categories = {}
            for sample in hotpot_data[:max_samples]:
                category = sample.get('category', 'Unknown')
                categories[category] = categories.get(category, 0) + 1
            dataset_info['category_distribution'] = categories
        
        # Step 1: æå–çŸ¥è¯†ç‰‡æ®µ
        knowledge_pieces = self.extract_knowledge_pieces_from_hotpot(hotpot_data, max_samples)
        
        # Step 2: è®¡ç®—æ¢¯åº¦
        gradients = self.compute_all_gradients()
        
        # Step 3: è®¡ç®—è€¦åˆçŸ©é˜µ
        coupling_matrix = self.compute_coupling_matrix()
        
        # Step 4: åˆ†æè€¦åˆæ¨¡å¼
        coupling_analysis = self.analyze_coupling_patterns()
        
        # Step 5: é¢„æµ‹æ¶Ÿæ¼ªå€™é€‰
        high_coupling_pairs = self.predict_ripple_candidates()
        
        # Step 6: ä¿å­˜åˆ†æç»“æœï¼ˆä¸ºè·¨potåˆ†æå‡†å¤‡ï¼‰
        self.save_analysis_results(output_dir, dataset_info)
        
        # Step 7: ä¿å­˜ä¸“é—¨ç”¨äºéªŒè¯çš„è€¦åˆåº¦ç»“æœ
        coupling_validation_file = self.save_coupling_results_for_validation(output_dir, dataset_info)
        
        # Step 8: å¯é€‰çš„å¯è§†åŒ–å’ŒæŠ¥å‘Š
        heatmap_path = None
        report_path = None
        
        if generate_heatmap:
            heatmap_path = self.plot_coupling_heatmap(output_dir=output_dir)
        
        if generate_report:
            report_path = self.generate_mvp_report(output_dir=output_dir)
        
        # å‡†å¤‡è¯¦ç»†çš„çŸ¥è¯†ç‰‡æ®µä¿¡æ¯
        knowledge_pieces_detailed = []
        for piece in self.knowledge_pieces:
            piece_info = {
                'piece_id': piece.piece_id,
                'question': piece.question,
                'answer': piece.answer,
                'supporting_fact': piece.supporting_fact,
                'category': piece.category
            }
            knowledge_pieces_detailed.append(piece_info)
        
        # å‡†å¤‡æ¶Ÿæ¼ªæ•ˆåº”å€™é€‰å¯¹çš„è¯¦ç»†ä¿¡æ¯
        ripple_candidates_detailed = []
        for i, (source_id, target_id, coupling_strength) in enumerate(high_coupling_pairs[:10]):
            source_piece = self._find_piece_by_id(source_id)
            target_piece = self._find_piece_by_id(target_id)
            
            candidate_info = {
                'rank': i + 1,
                'source_piece_id': source_id,
                'target_piece_id': target_id,
                'coupling_strength': float(coupling_strength),
                'source_answer': source_piece.answer if source_piece else "Unknown",
                'target_answer': target_piece.answer if target_piece else "Unknown",
                'source_question': source_piece.question if source_piece else "Unknown",
                'target_question': target_piece.question if target_piece else "Unknown",
                'source_category': source_piece.category if source_piece else "Unknown",
                'target_category': target_piece.category if target_piece else "Unknown"
            }
            ripple_candidates_detailed.append(candidate_info)
        
        # åˆ†æä¸åŒpotçš„è€¦åˆæƒ…å†µ
        cross_pot_analysis = self._analyze_cross_pot_patterns()
        
        # GPUæ€§èƒ½ä¿¡æ¯
        gpu_performance = {}
        if torch.cuda.is_available():
            gpu_performance = {
                'device': str(self.device),
                'gpu_name': torch.cuda.get_device_name(0),
                'gpu_memory_allocated_gb': torch.cuda.memory_allocated() / 1e9,
                'gpu_memory_reserved_gb': torch.cuda.memory_reserved() / 1e9,
                'model_type': self.model_type,
                'gradient_computation': 'GPU',
                'coupling_computation': 'GPU'
            }
        
        # æ„å»ºå®Œæ•´çš„results
        results = {
            # åŸºæœ¬ä¿¡æ¯
            'metadata': {
                'model_path': self.model_path,
                'model_type': self.model_type,
                'device': str(self.device),
                'layer_range': self.layer_range,
                'total_samples_processed': len(knowledge_pieces),
                'max_samples_requested': max_samples,
                'output_directory': output_dir,
                'analysis_timestamp': self._get_timestamp()
            },
            
            # æ•°æ®é›†ä¿¡æ¯
            'dataset_info': dataset_info,
            
            # çŸ¥è¯†ç‰‡æ®µè¯¦ç»†ä¿¡æ¯
            'knowledge_pieces': {
                'count': len(knowledge_pieces),
                'details': knowledge_pieces_detailed,
                'categories': self._get_category_stats()
            },
            
            # æ¢¯åº¦è®¡ç®—ç»“æœ
            'gradient_analysis': {
                'total_gradients_computed': len(gradients),
                'target_layers_count': len(self._get_target_layers()),
                'target_layers': self._get_target_layers()[:5] + (['...'] if len(self._get_target_layers()) > 5 else []),
                'gradient_dimension': gradients[list(gradients.keys())[0]].shape[0] if gradients else 0
            },
            
            # è€¦åˆåˆ†æç»“æœ
            'coupling_analysis': coupling_analysis,
            
            # é«˜è€¦åˆå¯¹è¯¦ç»†ä¿¡æ¯
            'high_coupling_pairs': {
                'count': len(high_coupling_pairs),
                'threshold': 0.4,
                'top_10_detailed': ripple_candidates_detailed,
                'all_pairs': [(pair[0], pair[1], float(pair[2])) for pair in high_coupling_pairs]
            },
            
            # è·¨potåˆ†æ
            'cross_pot_analysis': cross_pot_analysis,
            
            # æ–‡ä»¶è·¯å¾„
            'generated_files': {
                'coupling_matrix': f"{output_dir}/coupling_matrix.npy",
                'knowledge_pieces_json': f"{output_dir}/knowledge_pieces.json",
                'piece_ids_order': f"{output_dir}/piece_ids_order.json",
                'analysis_metadata': f"{output_dir}/analysis_metadata.json",
                'coupling_validation_file': coupling_validation_file,
                'coupling_pairs_csv': f"{output_dir}/coupling_pairs.csv",
                'high_coupling_pairs': f"{output_dir}/high_coupling_pairs.json",
                'heatmap_visualization': heatmap_path if generate_heatmap else None,
                'analysis_report': report_path if generate_report else None
            },
            
            # GPUæ€§èƒ½ä¿¡æ¯
            'gpu_performance': gpu_performance,
            
            # å®éªŒå‡†å¤‡ä¿¡æ¯
            'experiment_readiness': {
                'ripple_candidates_available': len(high_coupling_pairs) > 0,
                'recommended_candidates': min(5, len(high_coupling_pairs)),
                'next_steps': [
                    "Validate coupling results using generated files",
                    "Execute ripple effect validation",
                    "Validate GradSim hypothesis"
                ]
            }
        }
        
        print(f"\nâœ… MVP Analysis completed!")
        print(f"ğŸ“Š Mean coupling: {coupling_analysis['mean_coupling']:.4f}")
        print(f"ğŸ”¥ High coupling pairs: {len(high_coupling_pairs)}")
        print(f"ğŸ§ª Ripple candidates: {len(ripple_candidates_detailed)}")
        print(f"ğŸ’¾ Results saved to: {output_dir}/")
        print(f"ğŸ¯ Coupling validation file: {coupling_validation_file}")
        if report_path:
            print(f"ğŸ“ Report: {report_path}")
        print(f"ğŸ¯ Detailed results available in returned dictionary")
        
        return results


def load_hotpot_data(file_path: str) -> List[Dict]:
    """åŠ è½½HotpotQAæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Multi-Model Knowledge Coupling MVP")
    parser.add_argument("--hotpot_data", required=True, help="Path to HotpotQA JSON file")
    parser.add_argument("--model_path", default="gpt2", help="Model path")
    parser.add_argument("--max_samples", type=int, default=50, help="Maximum samples to process")
    parser.add_argument("--device", help="Device (cuda/cpu)")
    parser.add_argument("--output_dir", default="results/coupling_analysis", help="Output directory for results")
    parser.add_argument("--layer_start", type=int, help="Start layer for gradient computation (default: auto)")
    parser.add_argument("--layer_end", type=int, help="End layer for gradient computation (default: auto)")
    parser.add_argument("--no_report", action="store_true", help="Skip generating analysis report (default: skip)")
    parser.add_argument("--no_heatmap", action="store_true", help="Skip generating coupling heatmap (default: skip)")
    parser.add_argument("--generate_report", action="store_true", help="Generate analysis report")
    parser.add_argument("--generate_heatmap", action="store_true", help="Generate coupling heatmap")
    
    args = parser.parse_args()
    
    # å¤„ç†å±‚èŒƒå›´å‚æ•°
    layer_range = None
    if args.layer_start is not None and args.layer_end is not None:
        layer_range = (args.layer_start, args.layer_end)
        print(f"ğŸ¯ Using custom layer range: {args.layer_start}-{args.layer_end}")
    elif args.model_path and 'llama' in args.model_path.lower():
        # å¯¹äºLLaMAæ¨¡å‹ï¼Œæ¨èä½¿ç”¨é«˜å±‚ (æœ€å4å±‚ï¼š28,29,30,31)
        layer_range = (28, 31)
        print(f"ğŸ¯ Auto-selected high semantic layers for LLaMA: 28-31 (recommended by boss)")
    elif args.model_path and 'gpt2' in args.model_path.lower():
        # å¯¹äºGPT2æ¨¡å‹ï¼Œä½¿ç”¨æœ€åå‡ å±‚
        if 'large' in args.model_path.lower() or 'xl' in args.model_path.lower():
            layer_range = (44, 47)  # GPT2-large/xlçš„æœ€åå‡ å±‚
        else:
            layer_range = (8, 11)   # GPT2-small/mediumçš„æœ€åå‡ å±‚
        print(f"ğŸ¯ Auto-selected high semantic layers for GPT2: {layer_range}")
    
    # å¤„ç†æŠ¥å‘Šå’Œå¯è§†åŒ–ç”Ÿæˆé€‰é¡¹
    generate_report = args.generate_report and not args.no_report
    generate_heatmap = args.generate_heatmap and not args.no_heatmap
    
    if generate_report:
        print("ğŸ“ Will generate analysis report")
    else:
        print("â­ï¸  Skipping analysis report generation (use --generate_report to enable)")
    
    if generate_heatmap:
        print("ğŸ¨ Will generate coupling heatmap")
    else:
        print("â­ï¸  Skipping heatmap generation (use --generate_heatmap to enable)")
    
    # åŠ è½½æ•°æ®
    print(f"ğŸ“š Loading HotpotQA data from {args.hotpot_data}")
    hotpot_data = load_hotpot_data(args.hotpot_data)
    print(f"Loaded {len(hotpot_data)} samples")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = MultiModelKnowledgeCouplingMVP(
        model_path=args.model_path,
        device=args.device,
        layer_range=layer_range
    )
    
    # è¿è¡Œåˆ†æ
    results = analyzer.run_mvp_analysis(
        hotpot_data, 
        args.max_samples, 
        args.output_dir,
        dataset_file_path=args.hotpot_data,  # ä¼ é€’æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        generate_report=generate_report,
        generate_heatmap=generate_heatmap
    )
    
    print(f"\nğŸ¯ MVP Analysis completed successfully!")
    print(f"\nğŸ“ Key validation files generated:")
    print(f"   ğŸ“Š coupling_results_for_validation.json - Complete coupling analysis")
    print(f"   ğŸ“ˆ coupling_pairs.csv - Quick CSV format for analysis")
    print(f"   ğŸ”¥ high_coupling_pairs.json - High coupling pairs (â‰¥0.4)")
    print(f"   ğŸ’¾ coupling_matrix.npy - Raw coupling matrix")


if __name__ == "__main__":
    main() 