#!/usr/bin/env python3
"""
åˆ†æ"ç›¸åŒç­”æ¡ˆ"çš„knowledge piecesçš„è¯­ä¹‰å«ä¹‰
"""

import json
import pandas as pd
from pathlib import Path
from collections import defaultdict, Counter
import random

def load_knowledge_pieces():
    """åŠ è½½çŸ¥è¯†ç‰‡æ®µè¯¦ç»†ä¿¡æ¯"""
    knowledge_pieces_file = "results/full_hotpotqa_analysis/final_merged_results/all_knowledge_pieces.json"
    with open(knowledge_pieces_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_same_answer_examples():
    """åˆ†æç›¸åŒç­”æ¡ˆçš„å…·ä½“ä¾‹å­"""
    print("ğŸ” åˆ†æç›¸åŒç­”æ¡ˆçš„knowledge pieceså«ä¹‰...")
    
    # åŠ è½½çŸ¥è¯†ç‰‡æ®µ
    all_pieces = load_knowledge_pieces()
    
    # åˆ›å»ºpiece_idåˆ°è¯¦ç»†ä¿¡æ¯çš„æ˜ å°„
    piece_id_to_info = {piece['piece_id']: piece for piece in all_pieces}
    
    # åŠ è½½ä¸€ä¸ªæ‰¹æ¬¡çš„é«˜è€¦åˆå¯¹è¿›è¡Œåˆ†æ
    batch_0_file = "results/full_hotpotqa_analysis/batch_0000/high_coupling_pairs.json"
    with open(batch_0_file, 'r') as f:
        batch_data = json.load(f)
        pairs = batch_data['pairs']
    
    # æŒ‰ç­”æ¡ˆç±»å‹åˆ†ç»„
    answer_groups = defaultdict(list)
    
    for pair in pairs:
        answer = pair['piece_1_answer']  # ç›¸åŒç­”æ¡ˆçš„æƒ…å†µä¸‹ä¸¤ä¸ªç­”æ¡ˆä¸€æ ·
        if pair['piece_1_answer'] == pair['piece_2_answer']:
            answer_groups[answer].append(pair)
    
    print(f"âœ… æ‰¾åˆ° {len(answer_groups)} ç§ç›¸åŒç­”æ¡ˆç±»å‹")
    
    # åˆ†ææœ€å¸¸è§çš„ç›¸åŒç­”æ¡ˆ
    answer_counts = [(answer, len(pairs)) for answer, pairs in answer_groups.items()]
    answer_counts.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ“Š æœ€å¸¸è§çš„ç›¸åŒç­”æ¡ˆç±»å‹:")
    for answer, count in answer_counts[:10]:
        print(f"   '{answer}': {count} ä¸ªé«˜è€¦åˆå¯¹")
    
    # æ·±å…¥åˆ†æå‡ ä¸ªå…¸å‹ç­”æ¡ˆ
    analyze_answer_semantics("yes", answer_groups["yes"][:5], piece_id_to_info)
    analyze_answer_semantics("no", answer_groups.get("no", [])[:5], piece_id_to_info)
    
    # åˆ†æå¹´ä»½ç±»ç­”æ¡ˆ
    year_answers = [answer for answer in answer_groups.keys() if answer.isdigit() and len(answer) == 4]
    if year_answers:
        sample_year = random.choice(year_answers)
        analyze_answer_semantics(sample_year, answer_groups[sample_year][:3], piece_id_to_info)
    
    return answer_groups

def analyze_answer_semantics(answer, pairs, piece_id_to_info):
    """åˆ†æç‰¹å®šç­”æ¡ˆçš„è¯­ä¹‰å«ä¹‰"""
    print(f"\nğŸ¯ æ·±å…¥åˆ†æç­”æ¡ˆ '{answer}' çš„è¯­ä¹‰å«ä¹‰:")
    print(f"   é«˜è€¦åˆå¯¹æ•°é‡: {len(pairs)}")
    
    for i, pair in enumerate(pairs):
        print(f"\n--- ä¾‹å­ {i+1} ---")
        
        piece_1_id = pair['piece_1_id']
        piece_2_id = pair['piece_2_id']
        coupling_strength = pair['coupling_strength']
        
        piece_1 = piece_id_to_info.get(piece_1_id, {})
        piece_2 = piece_id_to_info.get(piece_2_id, {})
        
        print(f"è€¦åˆå¼ºåº¦: {coupling_strength:.4f}")
        print(f"æ˜¯å¦åŒä¸€HotpotQA: {pair['is_same_hotpot']}")
        
        if piece_1:
            print(f"ç‰‡æ®µ1 é—®é¢˜: {piece_1.get('question', 'N/A')[:100]}...")
            print(f"ç‰‡æ®µ1 æ”¯æŒäº‹å®: {piece_1.get('supporting_fact', 'N/A')[:100]}...")
        
        if piece_2:
            print(f"ç‰‡æ®µ2 é—®é¢˜: {piece_2.get('question', 'N/A')[:100]}...")
            print(f"ç‰‡æ®µ2 æ”¯æŒäº‹å®: {piece_2.get('supporting_fact', 'N/A')[:100]}...")

def analyze_classification_validity():
    """åˆ†æåˆ†ç±»çš„æœ‰æ•ˆæ€§ - æ˜¯å¦èƒ½å¯¹åº”åˆ°æ¨¡å‹ä¸­çš„çŸ¥è¯†èŠ‚ç‚¹"""
    print(f"\n\nğŸ§  åˆ†æåˆ†ç±»çš„æœ‰æ•ˆæ€§...")
    
    # åŠ è½½çŸ¥è¯†ç‰‡æ®µ
    all_pieces = load_knowledge_pieces()
    
    # æŒ‰ç­”æ¡ˆåˆ†ç»„åˆ†æ
    answer_to_pieces = defaultdict(list)
    
    for piece in all_pieces[:10000]:  # åˆ†æå‰10000ä¸ªç‰‡æ®µ
        answer = piece.get('answer', '').strip()
        if answer:
            answer_to_pieces[answer].append(piece)
    
    # åˆ†æç›¸åŒç­”æ¡ˆä½†ä¸åŒè¯­ä¹‰çš„æƒ…å†µ
    print(f"\nğŸ“Š ç›¸åŒç­”æ¡ˆçš„è¯­ä¹‰å¤šæ ·æ€§åˆ†æ:")
    
    # é€‰æ‹©ä¸€äº›æœ‰å¤šä¸ªå®ä¾‹çš„ç­”æ¡ˆè¿›è¡Œåˆ†æ
    multi_instance_answers = [(answer, pieces) for answer, pieces in answer_to_pieces.items() 
                             if len(pieces) >= 3]
    multi_instance_answers.sort(key=lambda x: len(x[1]), reverse=True)
    
    for answer, pieces in multi_instance_answers[:5]:
        print(f"\nç­”æ¡ˆ '{answer}' çš„ {len(pieces)} ä¸ªä¸åŒç”¨æ³•:")
        
        # åˆ†æé—®é¢˜çš„å¤šæ ·æ€§
        questions = [piece.get('question', '') for piece in pieces[:5]]
        categories = [piece.get('category', '') for piece in pieces[:5]]
        
        for i, (question, category) in enumerate(zip(questions, categories)):
            print(f"  {i+1}. [{category}] {question[:80]}...")

def analyze_model_knowledge_nodes():
    """åˆ†ææ˜¯å¦èƒ½åœ¨æ¨¡å‹ä¸­æ‰¾åˆ°å¯¹åº”çš„çŸ¥è¯†èŠ‚ç‚¹"""
    print(f"\n\nğŸ¤– åˆ†ææ¨¡å‹çŸ¥è¯†èŠ‚ç‚¹å¯¹åº”æ€§...")
    
    # è¿™é‡Œæˆ‘ä»¬åˆ†æä¸åŒç±»å‹ç­”æ¡ˆåœ¨æ¨¡å‹è¡¨ç¤ºç©ºé—´ä¸­çš„å«ä¹‰
    analysis_cases = [
        {
            'answer': 'yes',
            'semantic_meaning': 'å¸ƒå°”åˆ¤æ–­ - è‚¯å®šå›ç­”',
            'model_representation': 'å¯èƒ½å¯¹åº”æ¨¡å‹ä¸­çš„é€»è¾‘åˆ¤æ–­æœºåˆ¶',
            'knowledge_node_type': 'ç¨‹åºæ€§çŸ¥è¯† - é€»è¾‘æ¨ç†'
        },
        {
            'answer': '1967',
            'semantic_meaning': 'å…·ä½“å¹´ä»½ - æ—¶é—´ä¿¡æ¯',
            'model_representation': 'å¯èƒ½å¯¹åº”æ¨¡å‹ä¸­å­˜å‚¨çš„å…·ä½“äº‹å®',
            'knowledge_node_type': 'é™ˆè¿°æ€§çŸ¥è¯† - å…·ä½“äº‹å®'
        },
        {
            'answer': 'New York',
            'semantic_meaning': 'åœ°ç†å®ä½“ - åœ°å',
            'model_representation': 'å¯èƒ½å¯¹åº”æ¨¡å‹ä¸­çš„åœ°ç†çŸ¥è¯†å›¾è°±èŠ‚ç‚¹',
            'knowledge_node_type': 'é™ˆè¿°æ€§çŸ¥è¯† - å®ä½“çŸ¥è¯†'
        },
        {
            'answer': 'President Nixon',
            'semantic_meaning': 'äººç‰©å®ä½“ - æ”¿æ²»äººç‰©',
            'model_representation': 'å¯èƒ½å¯¹åº”æ¨¡å‹ä¸­çš„äººç‰©çŸ¥è¯†å›¾è°±èŠ‚ç‚¹',
            'knowledge_node_type': 'é™ˆè¿°æ€§çŸ¥è¯† - äººç‰©çŸ¥è¯†'
        }
    ]
    
    print("ä¸åŒç­”æ¡ˆç±»å‹çš„æ¨¡å‹çŸ¥è¯†èŠ‚ç‚¹åˆ†æ:")
    for case in analysis_cases:
        print(f"\nğŸ“‹ ç­”æ¡ˆ: '{case['answer']}'")
        print(f"   è¯­ä¹‰å«ä¹‰: {case['semantic_meaning']}")
        print(f"   æ¨¡å‹è¡¨ç¤º: {case['model_representation']}")
        print(f"   çŸ¥è¯†èŠ‚ç‚¹ç±»å‹: {case['knowledge_node_type']}")

def propose_improved_classification():
    """æå‡ºæ”¹è¿›çš„åˆ†ç±»æ–¹æ¡ˆ"""
    print(f"\n\nğŸ’¡ æ”¹è¿›çš„åˆ†ç±»æ–¹æ¡ˆå»ºè®®:")
    
    improved_classification = {
        'Boolean_Logic': {
            'examples': ['yes', 'no'],
            'description': 'å¸ƒå°”é€»è¾‘åˆ¤æ–­',
            'model_mechanism': 'é€»è¾‘æ¨ç†å±‚',
            'attack_potential': 'é«˜ - å½±å“æ¨¡å‹çš„åŸºç¡€é€»è¾‘åˆ¤æ–­'
        },
        'Temporal_Facts': {
            'examples': ['1967', '2005', 'December'],
            'description': 'æ—¶é—´ç›¸å…³çš„å…·ä½“äº‹å®',
            'model_mechanism': 'æ—¶é—´çŸ¥è¯†å­˜å‚¨',
            'attack_potential': 'ä¸­ - å½±å“å†å²äº‹ä»¶çš„æ—¶é—´çº¿'
        },
        'Geographic_Entities': {
            'examples': ['New York', 'California', 'United States'],
            'description': 'åœ°ç†ä½ç½®å®ä½“',
            'model_mechanism': 'åœ°ç†çŸ¥è¯†å›¾è°±',
            'attack_potential': 'ä¸­ - å½±å“åœ°ç†ç›¸å…³æ¨ç†'
        },
        'Person_Entities': {
            'examples': ['President Nixon', 'Obama', 'Einstein'],
            'description': 'äººç‰©å®ä½“',
            'model_mechanism': 'äººç‰©çŸ¥è¯†å›¾è°±',
            'attack_potential': 'é«˜ - å½±å“ä¸äººç‰©ç›¸å…³çš„å¤šä¸ªäº‹å®'
        },
        'Numerical_Facts': {
            'examples': ['2,416', '554 ft', '9.2 million'],
            'description': 'æ•°å€¼å‹äº‹å®',
            'model_mechanism': 'æ•°å€¼çŸ¥è¯†å­˜å‚¨',
            'attack_potential': 'ä½ - é€šå¸¸ä¸ºå­¤ç«‹äº‹å®'
        }
    }
    
    for category, info in improved_classification.items():
        print(f"\nğŸ“Š {category}:")
        print(f"   ç¤ºä¾‹: {', '.join(info['examples'])}")
        print(f"   æè¿°: {info['description']}")
        print(f"   æ¨¡å‹æœºåˆ¶: {info['model_mechanism']}")
        print(f"   æ”»å‡»æ½œåŠ›: {info['attack_potential']}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ·±å…¥åˆ†æç›¸åŒç­”æ¡ˆçš„knowledge piecesè¯­ä¹‰å«ä¹‰")
    print("=" * 60)
    
    # 1. åˆ†æç›¸åŒç­”æ¡ˆçš„å…·ä½“å«ä¹‰
    answer_groups = analyze_same_answer_examples()
    
    # 2. åˆ†æåˆ†ç±»æœ‰æ•ˆæ€§
    analyze_classification_validity()
    
    # 3. åˆ†ææ¨¡å‹çŸ¥è¯†èŠ‚ç‚¹å¯¹åº”æ€§
    analyze_model_knowledge_nodes()
    
    # 4. æå‡ºæ”¹è¿›æ–¹æ¡ˆ
    propose_improved_classification()
    
    print(f"\nğŸ¯ æ ¸å¿ƒç»“è®º:")
    print(f"1. 'ç›¸åŒç­”æ¡ˆ'ä¸ç­‰äº'ç›¸åŒçŸ¥è¯†' - éœ€è¦è€ƒè™‘è¯­ä¹‰ä¸Šä¸‹æ–‡")
    print(f"2. å½“å‰åˆ†ç±»è¿‡äºè¡¨é¢ - éœ€è¦æ›´æ·±å±‚çš„è¯­ä¹‰åˆ†ç±»")
    print(f"3. æ¨¡å‹çŸ¥è¯†èŠ‚ç‚¹å¯¹åº”éœ€è¦è€ƒè™‘çŸ¥è¯†ç±»å‹å’Œæœºåˆ¶")
    print(f"4. æ”»å‡»ç­–ç•¥åº”è¯¥åŸºäºçŸ¥è¯†çš„è¯­ä¹‰ç±»å‹è€Œéå­—ç¬¦ä¸²åŒ¹é…")

if __name__ == "__main__":
    main() 